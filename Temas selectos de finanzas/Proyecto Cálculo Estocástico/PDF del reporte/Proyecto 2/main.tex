\documentclass[11pt,notitlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage[spanish]{babel}
\usepackage[letterpaper]{geometry}
\graphicspath{ {images/} }
\usepackage{mathrsfs}
\newenvironment{solucion}
  {\begin{proof}[Solución]}
  {\end{proof}}
\renewcommand{\qedsymbol}{\footnotesize{\ensuremath{\blacksquare}}}
\pagenumbering{gobble}
\decimalpoint
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\pagestyle{fancy}
\pagenumbering{arabic}
\begin{document}
\begin{titlepage}
\centering
{ \includegraphics[scale=0.8]{FESAc.png}\par}
\vspace{1cm}
{\bfseries\LARGE Universidad Nacional Autónoma de México \par}
\vspace{0.8cm}
{\scshape\Large Facultad de Estudios Superiores Acatlán \par}
\vspace{2cm}
{\scshape\Huge Temas Selectos de Finanzas \par}
\vspace{2cm}
{\itshape\Large Proyecto 2 \par}
\vfill
{\Large García Flores Luis Edgar\par}
{\Large García Sánchez Cecilia Daniela \par}
{\Large Escalante López Julio César \par}
{\Large Ramírez Saldaña Valery Pamela \par}
{\Large Rojas Gutiérrez Rodolfo Emmanuel \par}

\vfill
{\Large Mayo 2020 \par}
\end{titlepage}
\newtheorem*{remark}{Observación}
\newtheorem{teor}{Teorema}[subsection]
\newtheorem{defi}{\textbf{\it\textbf{ Definición}}}[subsection]
\newtheorem{teo}{\textbf{\it\textbf{Teorema}}}[subsection]
\newtheorem{lema}{\textbf{\it\textbf{Lema}}} [subsection]
\tableofcontents
\clearpage
\part{Desarollo del tema}
\section{Movimiento Browniano}
\subsection{Caminatas Aleatorias Simétricas.}
Para la creación de un movimiento browniano, es necesario partir de una caminata aleatoria simétrica. Por otra parte, para construir una caminata aleatoria simétrica, repetidamente se lanza una moneda justa, es decir, con \(p\), la probabilidad de obtener cara en cada lanzamiento y \(q=1-p\), la probabilidad de obtener cruz en cada lanzamiento, iguales.
\\
\\
Los resultados sucesivos de los lanzamientos, se denotan por \(\omega=\omega_{1},\omega_{2},\omega_{3},\hdots\), donde \(\omega\) reprensenta la secuencia infinita de lanzamientos y \(\omega_{n}\) es el resultado del n-ésimo lanzamiento. Sea

\begin{equation*}
\label{eq:diana.xj}
    X_{j} = \begin{cases}
                1,& \mbox{si}\ \ \omega_{j}=Cara \\
                -1,& \mbox{si}\ \ \omega_{j}=Cruz
                \end{cases}
\end{equation*}
Se definirá \(M_k\) para \(k=0,1,2,...\), de la siguiente manera, \(M_{0}=0\), y
\begin{equation}
    M_{k}=\sum_{j=1}^k X_{j}, \ \ k=1,2,...
\end{equation}
El proceso \(M_{k}\), \(k=0,1,2,...\) es una caminata aleatoria simétrica. Con cada lanzamiento, sube una unidad o baja una unidad, y cada una de las posibilidades es igualmente probable.

\subsection{Incrementos de las caminatas aleatorias simétricas.}
Una caminata aleatoria tiene incrementos independientes, esto significa que si se eligen enteros no negativos, tales que \(0=k_{0}<k_{1}<...<k_{m}\), las variables aleatorias
\begin{equation}\label{MV.1}
\left (  M_{k_{1}}-M_{k_{0}}\right ) , \left ( M_{k_{2}}-M_{k_{1}}\right ),...,\left (M_{k_{m}}-M_{k_{m-1}}\right )
\end{equation}
\ son independientes, esto es sencillo de observar, debido a que las variables aleatorias presentadas previamente pueden expresarse como:
\begin{equation}\label{malditasea}
    M_{k_{i+1}}-M_{k_{i}}=\sum_{j=k_{i}+1}^{k_{i+1}} X_{j},
\end{equation}
 Es importante mencionar, que las variables aleatorias anteriores se conocen como los incrementos de la caminata aleatoria, y pueden interpretarse como el cambio en la posición de la misma entre los tiempos \(k_{i}\) y \(k_{i+1}\). Los incrementos en intervalos de tiempo no superpuestos son independientes, porque dependen de diferentes lanzamientos de monedas.\\ \\
Dado que, \(\mathbb{E}\left ( X_{j} \right )=0 \) y \(\mathbb{V}(X_{j})=\mathbb{E}(X_{j}^{2})=1\) \(\forall j\in \mathbb{N}\),  entonces, cada incremento \(M_{k_{i+1}}-M_{k_{i}}\) tendrá un valor esperado de 0 y varianza de \(k_{i+1}-k_{i}\). Esto significa que la varianza solo dependera del incremento que se tome. Es fácil ver que el valor esperado es cero, dado que el valor asociado a cada   \(M_{k_{i}}\)  es una suma de \(X_{j}\) entonces su valor esperado será una suma de ceros. Además dado que el proceso \(X_j\) tiene varianza unitaria y las \(X_{j}\) son independientes, por la ecuación (\ref{malditasea}), se tiene que
\begin{equation}\label{MV.2}
    \mathbb{V}(M_{k_{i+1}}-M_{k_{i}}) =\mathbb{V}(\sum_{j=k_{i}+1}^{k_{i+1}}X_{j})= \sum_{j=k_{i}+1}^{k_{i+1}} \mathbb{V}(X_{j})= \sum_{j=k_{i}+1}^{k_{i+1}}1=k_{i+1}-k_{i}
\end{equation}
%%\begincenter}
%% \includegraphics[scale=0.3]{RW_mario.png}    
%% \end{center}

 
Esto último prueba que la varianza de la caminata aleatoria simétrica se acumula con razón de uno por unidad de tiempo, de modo que la varianza del incremento sobre cualquier intervalo de tiempo \(k\) a \(l\) para enteros no negativos tales que \(k<l\) es \(l-k\).

\subsection{Propiedad de Martingala de las caminatas aleatorias simétricas.} 
Para verificar que una caminata aleatoria simétrica es martingala, se deben cumplir tres propiedades, la primera es que su esperanza debe ser finita, el proceso \(M_{k}\) cumple esta propiedad. La segunda propiedad es que debe ser adaptado a la filtración \(\mathcal{F}_{k}\) pero también es adaptada ya que el proceso, al momento \(k\), solo depende de los \(k\) primeros lanzamientos de moneda. Para la tercer propiedad se debe cumplir que \(\mathbb{E}[M_{l}|\mathcal{F}_{k}]=M_{k}\), procedamos entonces a calcular dicha esperanza condicional, sean \(k,l\) enteros no negativos tales que \(k<l\) observe que
\begin{align*}
    \mathbb{E}[M_{l}|\mathcal{F}_{k}]&=\mathbb{E}\left [ \left ( M_{l}-M_{k} \right )+M_{k}|\mathcal{F}_{k} \right ]\\
    &=\mathbb{E}[M_{l}-M_{k}|\mathcal{F}_{k}]+\mathbb{E}[M_{k}|\mathcal{F}_{k}]\\
    &=\mathbb{E}[M_{l}-M_{k}|\mathcal{F}_{k}]+M_{k}\\
    &=\mathbb{E}[M_{l}-M_{k}] + M_{k}\\
    &=M_{k}  
\end{align*}

 Se sabe que la notación \(\mathbb{E}[...|\mathcal{F}_{k}]\) denota la esperanza condicional respecto a la \(k\)-ésima sigma algebra de la filtración, que en este caso representa el conocimiento de los primeros \(k\) lanzamientos de moneda. 
La segunda ecuación, es resultado de la linealidad de la esperanza condicional, por otro lado, la tercera ecuación es debido a que  \(M_{k}\) depende únicamente de los primeros  \(k\) lanzamientos de moneda, el cual es un evento \(\mathcal{F}_{k}\)-medible. Finalmente, la cuarta igualdad se sigue de la propiedad de independencia de los incrementos brownianos.



\subsection{Variación cuadrática de una caminata aleatoria simétrica.}

Ahora, consideraremos la variación cuadrática de una caminata aleatoria simétrica. La variación cuadrática hasta el tiempo $k$, se define como
\begin{equation}\label{secc1.4_sumavc}
    [M,M]_{k}=\sum_{j=1}^{k}(M_{j}-M_{j-1})^{2}=k
\end{equation}
Tenga en cuenta que este cálculo se efectúa trayectoria a trayectoria. La variación cuadrática hasta el tiempo \(k\) a lo largo de una trayectoria, se calcula tomando todos los incrementos en un paso $M_{j}-M_{j-1}$ a lo largo de la trayectoria, los cuales son iguales a $X_{j}$, que toma el valor de 1 0 en caso contrario, el valor de -1, dependiendo de la trayectoria; una vez tomados los incrementos, éstos se elevan al cuadrado y luego se suman.\\

Nótese que $(M_{j}-M_{j-1})^2$ es igual a $1$, independientemente de si $M_{j}-M_{j-1}$ es 1 ó -1, así (\ref{secc1.4_sumavc}) resulta igual a
\begin{equation}
    \sum_{j=1}^{k}1=k
\end{equation}
 como habíamos mostrado. Nótese que fijar $k_{i+1}=k$ y $k_{i}=0$, resulta en que $[M,M]_{k}$ será igual que $\mathbb{V}(M_{k})$. Sin embargo, el cálculo de ambas cantidades difiere bastante.\\


Por un lado, $\mathbb{V}(M_{k})$ se calcula tomando un promedio sobre todas las trayectorias, teniendo en cuenta sus probabilidades. Si la caminata aleatoria no fuera simétrica, es decir, si la probabilidad $p$ es distinta a la probabilidad $q$, entonces eso afectaría el cálculo de $\mathbb{V}(M_{k})$.\\

Por el contrario, $[M,M]_{k}$ se calcula a lo largo de una simple trayectoria, y las probabilidades no afectan el cálculo de la misma. Uno puede calcular la varianza de una caminata aleatoria solo teóricamente, ya que requiere un promedio sobre todas las trayectorias, tanto realizadas como no realizadas. Sin embargo, a partir de los datos de precios, es posible calcular la variación cuadrática a lo largo de la trayectoria realizada, de manera bastante explícita.\\

Una caminata aleatoria, $[M,M]_{k}$ no depende de la trayectoria particular elegida, pero se analizará más adelante que la variación cuadrática para un proceso aleatorio generalmente depende de la trayectoria a lo largo de la cual se calcula.


\subsection{Caminata aleatoria simétrica escalada.} Para aproximar el Movimiento Browniano, podemos acelerar el tiempo y reducir proporcionalmente el tamaño de los incrementos o decrementos de una caminata aleatoria simétrica. Es decir, si fijamos un entero $n$, podemos  definir la \textit{caminata aleatoria simétrica escalada} como:
\begin{equation}\label{eq:marioWn}
    B^{(n)}_{t} = \frac{1}{\sqrt{n}}M_{nt}
\end{equation}
siempre que $nt$ sea entero. En el caso en que $nt$ no es entero, se escogen los puntos más cercanos, $s$ por la izquierda y $u$ por la derecha de $t$, de tal forma que $ns$ y $nu$ sean enteros. Así, defnimos  $ B^{(n)}_{t}$ como la interpolación lineal entre $ B^{(n)}_{s}$ y $ B^{(n)}_{u}$. Cuando $n \rightarrow \infty $ deberíamos obtener un Movimineto Browniano. En la figura \ref{fig: mario1} observamos una trayectoria de $B^{(100)}$ hasta el tiempo $4$, que fue generada por $400$ lanzaminetos de moneda, cada uno con decrementos o incrementos de tamaño $\frac{1}{10}$.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{RW_mario.png}
\caption{Una Trayectoria de $B^{(100)}$}
\label{fig: mario1}
\end{figure}
\(\linebreak\)
 Dado que la caminata aleatoria escalada tiene incrementos independientes. Si $0=t_0 <t_1 < \dots < t_m$ son de tal forma que $nt_j$, es entero, entonces

\[\left ( B^{(n)}_{t} - B^{(n)}_{t_0} \right ), \ \left ( B^{(n)}_{t_1} - B^{(n)}_{t_0} \right ), \ \dots, \ \left ( B^{(n)}_{t_1} - B^{(n)}_{t_0} \right )\]
son independientes, pues estas variables aleatorias dependen de diferentes lanzaminetos de moneda. Por ejemplo, $B^{(100)}_{0.20} - B^{(100)}_{0}$ depende de los primeros 20 lanzamientos, mientras que $B^{(100)}_{0.70} - B^{(100)}_{0.20}$ depende de los siguientes 50. Más aún, para $0\leq s\leq t$ tales que $ns$ y $nt$ son enteros, podemos calcular la esperanza y varianza de estos incrementos como sigue:
\begin{align*}
   \mathbb{E}[B^{(n)}_{t} - B^{(n)}_{s}] &= \mathbb{E}\left[\frac{1}{\sqrt{n}}\left(M_{nt}- M_{ns}\right)\right] \\
   &= \frac{1}{\sqrt{n}} \mathbb{E} \left[\sum_{j=ns+1}^{nt}X_j\right]\\
   &=\frac{1}{\sqrt{n}} \sum_{j=ns+1}^{nt} \mathbb{E} \left[X_j\right]\\
   &= 0
\end{align*}
\begin{align*}
   \mathbb{V}[B^{(n)}_{t} - B^{(n)}_{s}] &= \mathbb{V}\left[\frac{1}{\sqrt{n}}\left(M_{nt}- M_{ns}\right)\right] \\
   &= \frac{1}{n} \mathbb{V} \left[\sum_{j=ns+1}^{nt}X_j\right]\\
   \end{align*}
Como las $X_j$ son i.i.d., entonces: 
   \begin{align*}
         \mathbb{V}[B^{(n)}_{t} - B^{(n)}_{s}]&=\frac{1}{n} \sum_{j=ns+1}^{nt} \mathbb{V} \left[X_j\right]\\
   &= \frac{nt - ns}{n}\mathbb{V}[X_j]\\
   &= t-s
   \end{align*}

Retomando el ejemplo, $B^{(100)}_{0.70} - B^{(100)}_{0.20}$  es la suma de $50$ variables aleatorias independientes e identicamente distribuidas, cada una puede tomar el valor de $\frac{1}{10}$ o $-\frac{1}{10}$. Además, cada variable aleatoria tiene media $0$ y varianza $\frac{1}{100}$, entonces la varianza de $B^{(100)}_{0.70} - B^{(100)}_{0.20}$ es $50\cdot \frac{1}{100}= 0.50$.\\ \linebreak
Ahora exploremos la propiedad de martingala para la caminata aleatoria simétrica escalada. Sean $0\leq s\leq t$ dados, y reescribamos $B^{(n)}_{t}$ como:
\[B^{(n)}_{t} = (B^{(n)}_{t}-B^{(n)}_{s}) + B^{(n)}_{s}\]Si $s$ y $t$ son elegidos tal que $ns$ y $nt$ sean enteros, entonces el primer término de la derecha es independiente de $\mathcal{F}_{s}$, la $\sigma-$álgebra de información disponible al timepo $s$, y el segundo término, $B^{(n)}_{s}$ es $\mathcal{F}_{s}$-medible. Entonces podemos probar la propiedad de martingala para la caminata aleatoria escalada de la siguiente forma:
\begin{align*}
    \mathbb{E}[B^{(n)}_{t} | \mathcal{F}_{s}] = & \mathbb{E}[(B^{(n)}_{t}-B^{(n)}_{s}) + B^{(n)}_{s}| \mathcal{F}_{s}]\\
    = & \mathbb{E}[B^{(n)}_{t}-B^{(n)}_{s}| \mathcal{F}_{s}] + \mathbb{E}[ B^{(n)}_{s} | \mathcal{F}_{s}]\\
    = & \mathbb{E}[B^{(n)}_{t}-B^{(n)}_{s}] + \mathbb{E}[ B^{(n)}_{s} | \mathcal{F}_{s}]\\
    = &  B^{(n)}_{s}
\end{align*}
\linebreak
Por último, consideremos la \textit{vaciación cuadrática} de la caminata aleatoria simétrica escalada. Para $B^{(n)}$, la variación cuadrática al tiempo $t \geq 0$ tal que $nt$ es un entero, es definida como:
\begin{align*}
    [B^{(n)}, B^{(n)}](t) = & \sum _{j = 1}^{nt} \left[B^{(n)}\left(\frac{j}{n}\right) - B^{(n)}\left(\frac{j-1}{n}\right)\right]^2\\
     = & \sum_{j-1}^{nt}\left[\frac{1}{\sqrt{n}}X_j\right]^2 = \sum_{j-1}^{nt} \frac{1}{n} = t
\end{align*}
Si vamos del tiempo $0$ al tiempo $t$, a lo largo de la trayectoria de la caminata aleatoria escalada, evaluando el incremento de cada paso, elevando al cuadrado y sumando estos valores, resulta igual a $t$, la longitud del intervalo. Cabe mencionar que este es un cálculo para una trayectoria. De cualquier forma, notemos que $\mathbb{V}(B^{(n)}_{t})=t$, pero esta cantidad es el promedio sobre todas las posibles trayectorias.


\subsection{Distribución límite de la caminata aleatoria escalada.}

En la figura \ref{fig: mario1} se puede obsevar una sola trayectoria de la caminata aleatoria escalada, i.e., fijamos un valor de $\omega = \omega_1, \omega_2, ...$ y dibujamos el recorrido como función del tiempo $t$. Otra forma de pensar en una caminata aleatoria escalada es fijar el tiempo $t$ y considerar todas las posibles trayectorias en ese tiempo, dadas por los diferentes valores de $\omega$. Por ejemplo, sea $t=0.25$ y consideremos el conjunto de los posibles valores de $B^{(100)}_{0.25} = \frac{1}{10}M_{25}$, esta variable aleatoria es generada por 25 lanzamientos de moneda, y como $M_{25}$ puede tomar valores entre $-25$ y $25$, la caminata aleatoria escalada $B^{(100)}_{0.25}$ puede tomar cualquiera de los siguientes valores: \[ Ran\left ( B^{(100)}_{0.25} \right ) = \{ -2.5,\ -2.3, \ \dots, \ -0.1, \ 0.1, \ \dots, \ 2.3,  \ 2.5 \} \]
Para obtener la probabilidad de que $B^{(100)}_{0.25}$ tomamos algún valor $w \in Ran(B^{(100)}_{0.25}) $ debemos recordar que:
\begin{align*}
B^{(100)}_{0.25} = & \frac{1}{10}M_{25} = \frac{1}{10}\sum_{j=1}^{25}X_j
\end{align*}
donde $(X_j + 1)/2 \sim Ber(1/2)$ para cada $j\in \{1, \dots, 25\}$ y si definimos $Z_{25} = (M_{25} + 1)/2$ , entonces:
\[ Z_{25} = \frac{M_{25} + 25}{2} =\frac{1}{2} \left(\sum_{j=1}^{25}(X_j)+ 25 \right)  =\sum_{j=1}^{25}\frac{X_j + 1}{2} \sim Binom(25, 1/2)  \]
Así, podemos calcular la probabilidad puntual de que  $B^{(100)}_{0.25}$ tome algún valor $B \in Ran(B^{(100)}_0.25) $:
\begin{align*}
    P(B^{(100)}_{0.25} = w) = & P \left( \frac{1}{10}M_{25} = w\right)\\
    = & P \left( \frac{1}{10} ( 2\cdot Z_{25} - 25) = w \right)\\
    = & P \left(Z_{25} = \frac{10w + 25}{2} \right)\\
    = & \binom{25}{(10w + 25)/2} \left(\frac{1}{2}\right)^{25}
\end{align*}
donde el término $(10w + 25)/2$ siempre resulta ser un entero menor que $25$. Con esto, podemos graficar el histograma de la figura \ref{secc1.6_fig1}, representando con el área la probabilidad de que $B^{(100)}_{0.25}$ tome un valor entre $-1.5$ y $1.5$, donde cada barra tiene de ancho $0.2$, por lo que la altura de la barra centrada en $B \in Ran(B^{(100)}_{0.25}) $ será de $\frac{P(B^{(100)}_{0.25} = w)}{0.2}$. Por ejemplo, si tomamos $w= 0.1$, entonces:
\begin{equation}\label{eq:mariobino}
  P (B^{(100)}_{0.25} = 0.1) = \binom{25}{13} \left(\frac{1}{2}\right)^{25} = \frac{25!}{13!12!} \left(\frac{1}{2}\right)^{25} = 0.1555  
\end{equation}

que es equivalente a tener 13 lanzaminetos donde $X_j = 1$ y 12 donde $X_j = -1$. Además, la altura para la barra del histograma centrada en $0.1$ es de $\frac{0.1555}{0.2} = 0.7775$. \\
\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{hist_mario.png}
\caption{Distribución de $B^{(100)}_{0.25}$ y la curva $y = \frac{2}{\sqrt{2\pi}}e^{-2x^2}$}
\label{secc1.6_fig1}
\end{figure}
\linebreak
Por otra parte, la variable $B^{(100)}_{0.25}$ tiene esperanza igual a $0$ y varianza $0.25$. Superponiendo en la figura \ref{secc1.6_fig1} la función de densidad de una variable aleatoria que se distribuye como normal con esos parametros, es decir, la curva $f(x) = \frac{2}{\sqrt{2\pi}}e^{-2x^2}$ , entonces podemos ver que la distribución de $B^{(100)}_{0.25}$ es casi normal. Si tuvieramos  una función continua y acotada $g(x)$ y quisieramos calcular $\mathbb{E}\left[g\left(B^{(100)}_{0.25}\right)\right]$, una buena aproximación sería:
\[\mathbb{E}\left[g\left(B^{(100)}_{0.25}\right)\right] \approx \frac{2}{\sqrt{2\pi}}\int_{-\infty}^{\infty} g(x)e^{-2x^2}dx\]
El siguente teorema, es una versión del Teorema del Límite Central aplicable a nuestro contexto, que afirma que la aproximación anterior es válida.
\begin{teor}\label{secc1.6_teo1}
\textbf{(Límite Central).} Sea $t \geq 0 $. Si $n \rightarrow \infty$, la distribución de la caminata aleatoria escalada $B^{(n)}_{t}$ evaluada en el tiempo t, converge a una distribución normal con media $0$ y varianza $t$.
\end{teor}
\begin{proof}
Podemos utilizar la función generadora de momentos para identificar la distribución de una variable aleatoria. Para la densidad normal
 \[f(x)= \frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}\]
 con media cero y varianza $t$, la función generadora de momentos es
 \begin{align}
 \nonumber
     \varphi(u) = & \int_{-\infty}^{\infty}e^{ux}f(x)dx  \\ \nonumber
     = & \frac{1}{\sqrt{2\pi t}}\int_{-\infty}^{\infty} \exp\left\{ ux -\frac{x^2}{2t} \right\} dx\\ \nonumber
     = & e^{\frac{1}{2}u^2t}\cdot  \frac{1}{\sqrt{2\pi t}}\int_{-\infty}^{\infty} \exp\left\{ -\frac{(x - ut)^2}{2t} \right\} dx\\
     \label{mario1}
     = & e^{\frac{1}{2}u^2t}
 \end{align}
 pues $\frac{1}{\sqrt{2\pi t}}exp\left\{ -\frac{(x - ut)^2}{2t} \right\}$ es la función de densidad de una v.a. normal con media $ut$ y varianza $t$ y por lo tanto, integra a $1$.\\
 Por otro lado, si $t$ es de tal suerte que $nt$ es un entero, la función generadora de momentos para $B^{(n)}_{t}$ es
 \begin{align*}
     \varphi_n(u) = & E[e^{uB^{(n)}_{t}}] = E\left[\exp\left\{ \frac{u}{\sqrt{n}}M_{nt}\right\}\right]\\
     = & E\left[\exp\left\{ \frac{u}{\sqrt{n}}\sum_{j=1}^{nt}X_j\right\}\right] = E\left[\prod_{j=1}^{nt}\exp\left\{ \frac{u}{\sqrt{n}}X_j\right\}\right]
 \end{align*}
 Luego, como las variables aleatorias de la última expresión son independientes, puede ser escrita como:
 \[\prod_{j=1}^{nt}E\left[\exp\left\{ \frac{u}{\sqrt{n}}X_j\right\}\right] = \prod_{j=1}^{nt} \left(\frac{1}{2}e^{\frac{u}{\sqrt{n}}} + \frac{1}{2}e^{-\frac{u}{\sqrt{n}}}\right) = \left(\frac{1}{2}e^{\frac{u}{\sqrt{n}}} + \frac{1}{2}e^{-\frac{u}{\sqrt{n}}}\right)^{nt} \]
 Necesitamos probar que si $n \rightarrow \infty$,
 \[\varphi_n(u) = \left(\frac{1}{2}e^{\frac{u}{\sqrt{n}}} + \frac{1}{2}e^{-\frac{u}{\sqrt{n}}}\right)^{nt} \]
 converge a la función generadora de momentos $ \varphi(u) = e^{\frac{1}{2}u^2t}$ calculada en  \eqref{mario1}. Es suficiente probar que
 \[\log \varphi_n(u) = nt\log \left(\frac{1}{2}e^{\frac{u}{\sqrt{n}}} + \frac{1}{2}e^{-\frac{u}{\sqrt{n}}}\right) \]
 converge a $\log \varphi(u) = \frac{1}{2}u^2t$.
 Para esto, primero aplicamos el cambio de variable $ x = \frac{1}{\sqrt{n}}$ de tal forma que
 \[\lim_{n \rightarrow\infty} \log \varphi_n(u) = t\lim_{x\downarrow0}\dfrac{\log(\frac{1}{2}e^{ux} + \frac{1}{2}e^{-ux} )}{x^2}\]
 Si tratamos de evaluar $x = 0$ en la expresión de la derecha, obtenemos una indeterminación del tipo $\frac{0}{0}$, por lo que podemos proceder con la regla de L'H\^opital, donde la derivada respecto a $x$ del numerador es:
 \[\frac{\partial}{\partial x}\log \left(\frac{1}{2}e^{ux} + \frac{1}{2}e^{-ux} \right) = \dfrac{\frac{u}{2}e^{ux} - \frac{u}{2}e^{-ux}}{\frac{1}{2}e^{ux} + \frac{1}{2}e^{-ux}} \]
 y la derivada del denominador es
 \[\frac{\partial}{\partial x} x^2 = 2x\]
 Por lo tanto,
 \[ \lim_{n \rightarrow\infty} \log \varphi_n(u) = \lim_{x\downarrow0} \dfrac{\frac{u}{2}e^{ux} - \frac{u}{2}e^{-ux}}{2x\left(\frac{1}{2}e^{ux} + \frac{1}{2}e^{-ux}\right)} = \frac{t}{2} \lim_{x\downarrow0} \dfrac{\frac{u}{2}e^{ux} - \frac{u}{2}e^{-ux}}{x} \]
 donde hemos usado el hecho de que $\lim_{x\downarrow0} \left(\frac{1}{2}e^{ux} + \frac{1}{2}e^{-ux}\right) = 1 $. Ahora podemos aplicar la regla de L'H\^opital una vez más, donde la derivada del numerador es
 \[\frac{\partial}{\partial x}\left( \frac{u}{2}e^{ux} - \frac{u}{2}e^{-ux} \right)  = \frac{u^2}{2}e^{ux} + \frac{u^2}{2}e^{-ux}\]
 y la derivada del denominador es $\frac{\partial}{\partial x}x = 1$. Por lo tanto,
 \[\lim_{n \rightarrow\infty} \log \varphi_n(u) = \frac{t}{2} \lim_{x\downarrow0} \left( \frac{u^2}{2}e^{ux} + \frac{u^2}{2}e^{-ux} \right) = \frac{1}{2}u^2t \]
 como deseabamos.
\end{proof}


\subsection{Distribución log-normal como límite del modelo binomial.}
El Teorema del Límite Central puede ser usado para mostrar que el límite del modelo para valuación de activos binomial escalado, induce una distribución log-normal para el precio de las acciones. Supongamos que la tasa de interés $r$, es cero. Este resultado muestra que el modelo binomial es una versión discreta del modelo de movimiento Browniano geométrico, el cual es la base de la fórmula de Black- Scholes-Merton, para la valuación de opciones.\\
Al construir un modelo para el precio de las acciones sobre el intervalo de $0$ a $t$, primero escogemos un entero $n$ y construimos un modelo binomial que tome $n$ pasos por unidad de tiempo. Asumimos que $n$ y $t$ son escogidos de tal forma que $nt$ es entero. Tomamos el factor de subida como $u_n = 1 + \frac{\sigma}{\sqrt{n}}$ y el factor de bajada como  $d_n = 1 - \frac{\sigma}{\sqrt{n}}$. Aquí, $\sigma$ es una constante positiva que resultará ser la volitilidad del proceso límite del precio de la acción. Estas probabilidades neutrales al riesgo son:
\[\tilde{p} = \frac{1+ r - d_n}{u_n - d_n} = \frac{\sigma/\sqrt{n}}{2\sigma/\sqrt{n}} = \frac{1}{2}, \hspace{1cm} \tilde{q} = \frac{u_n - 1- r}{u_n - d_n} = \frac{\sigma/\sqrt{n}}{2\sigma/\sqrt{n}} = \frac{1}{2}\]
El precio de la acción al timepo $t$ está determinado por el valor inicial $S_0$ y el resultado de los primeros $nt$ lanzamientos de moneda. La suma del número de \textit{heads} $H_{nt}$  más el número de \textit{tails} $T_{nt}$ en los primeros $nt$ lanzamientos de moneda, es $nt$, un hecho que escribimos como:
\[nt = H_{nt} + T_{nt}\]
La caminata aleatoria $M_{nt}$ es el número de \textit{heads} menos el número de \textit{tails} en estos $nt$ lanzamientos:
 \[M_{nt} = H_{nt} -T_{nt}\]
 Sumando las dos ecuaciones y dividiendo entre $2$, vemos que:
 \[H_{nt} = \frac{1}{2}(nt + M_{nt})\]
 Restándolas y dividiendo entre $2$, vemos además que:
 \[T_{nt} = \frac{1}{2}(nt - M_{nt})\]
 En el modelo con factores $u_n$ y $d_n$, el precio de la acción al timepo $t$ es:
 \begin{equation} \label{mario2}
     S_n(t) = S_0\cdot u_n^{H_{nt}}\cdot d_n^{T_{nt}} = S_0\left(1 + \frac{\sigma}{\sqrt{n}}\right)^{\frac{1}{2}(nt + M_{nt})}\left(1 - \frac{\sigma}{\sqrt{n}}\right)^{\frac{1}{2}(nt - M_{nt})}
 \end{equation}
Deseamos conocer la distribución de esta variable aleatoria cuando $n$ tienda a infinito.

\begin{teo}
Si $n \rightarrow \infty$, la distribución de $S_n(t)$ en \eqref{mario2}, converge a la distribución de
\begin{equation} \label{mario3}
    S_t = S_0 \exp \left\{ \sigma B_t - \frac{1}{2}\sigma^2t\right\}
\end{equation}
donde $B_t$ es una variable aleatoria normal con media cero y varianza $t$. \\ \linebreak
\textnormal{La distribución de $S_t$ en \eqref{mario3} es llamada \textit{log-normal}. De manera general, cualquier variable aleatoria de la forma $ce^X$, con $c$ constante y $X$ con distribución normal, tendrá un distribución log-normal. En este caso, $X = \sigma B_t - \frac{1}{2}\sigma^2t$ es una normal con media $-\frac{1}{2}\sigma^2t$ y varianza $\sigma^2t$. }
\end{teo}

\begin{proof}
 Es suficiente probar que la distribución de
 \[\log S_n(t) = \log S_0 + \frac{1}{2}(nt + M_{nt}) \log\left(1 + \frac{\sigma}{\sqrt{n}}\right) + \frac{1}{2}(nt - M_{nt}) \log\left(1 - \frac{\sigma}{\sqrt{n}}\right) \]
 converge a la distribución de
 \[\log S_t = \log S_0 + \sigma B_t - \frac{1}{2}\sigma^2t\]
 Para hacer esto, necesitamos la expansión de la serie de Taylor para $f(x) = \log (1+ x)$. Calculamos $f'(x)=(1+x)^{-1}$ y $f''(x) = - (1+ x) ^{-2}$, luego evaluamos en $0$ para obtener $f'(0) = 1$ y $f''(0) = -1$. De acuerdo al Teorema de Taylor,
 \[\log (1+x) = f(0) + f'(0)x + \frac{1}{2}f''(0)x^2 + O(x^3) = x - \frac{1}{2}x^2 + O(x^3)\]
 donde $O(x^3)$ representa el término de orden $x^3$. Aplicamos esto a \eqref{mario3}, primero con $x_1= \frac{\sigma}{\sqrt{n}}$ y luego con $x_2 = -\frac{\sigma}{\sqrt{n}}$, donde $O(x_1) = O_1(\frac{\sigma^3}{n^{3/2}})$ y $O(x_2) = O_2(-\frac{\sigma^3}{n^{3/2}})$, que podemos redefinir de tal forma que el argumento de la función $O$ quede solo en términos de $n$, como sigue: $O(x_1) = O_1(n^{-\frac{3}{2}}) $ y $O(x_2) = O_2(-n^{-\frac{3}{2}}) $, además se cumple que $O(x_2) = - O(x_1)$, entonces tenemos que:
 \begin{align*}
     &\log S_t \\
     & =  \log S_0 + \frac{1}{2}(nt + M_{nt}) \left( \frac{\sigma}{\sqrt{n}} - \frac{\sigma^2}{2n} + O_1(n^{-\frac{3}{2}})\right) + \frac{1}{2}(nt - M_{nt}) \left( -\frac{\sigma}{\sqrt{n}} - \frac{\sigma^2}{2n} - O_1(n^{-\frac{3}{2}})\right)\\
     &= \log S_0 + nt \left(- \frac{\sigma^2}{2n}\right) + M_{nt} \left( \frac{\sigma}{\sqrt{n}}  + O_1(n^{-\frac{3}{2}})\right)\\
     &=\log S_0  - \frac{1}{2}\sigma^2t + \sigma B^{(n)}_t + O_1(n^{-1})B^{(n)}_t
 \end{align*}
 El término $B^{(n)}_t$ aparece en dos lugares en la última linea, por el Teorema del Límite Central, su distribución converge a la distribución de una variable aleatoria con media cero y varianza $t$, a la que llamaremos $B_t$, entonces si aplicamos límite cuando $n \rightarrow \infty$:
 \begin{align*}
    \lim_{n \rightarrow \infty} \log S_t = & \lim_{n \rightarrow \infty} \left[\log S_0  - \frac{1}{2}\sigma^2t + \sigma B^{(n)}_t + O_1(n^{-1})B^{(n)}_t\right]\\
    = & \log S_0 + \sigma B_t - \frac{1}{2}\sigma^2t\\
     = &\log S_t
 \end{align*}
 que es lo que queríamos probar.
\end{proof}

\subsection{Definición de Movimiento Browniano}
Se obtuvo el movimiento Browniano como el límite de la caminata aleatoria escalada $B^{(n)}_t$ de $\eqref{eq:marioWn}$ cuando $n \rightarrow \infty$. El movimiento Browniano hereda propiedades de estas caminatas aleatorias, las cuales se expresan en la siguiente definición.

\begin{defi}
 Sea $(\Omega, \mathcal{F}, \mathbb{P})$ un espacio de probabilidad. Para cada $\omega \in \Omega$, supóngase que hay una función continua $B_t$ con $t \geq 0$, que satisface $B_0=0$ y depende de $\omega$. Luego,  $B_t, \ t \geq 0$, es un movimiento Browniano si $\forall \ 0=t_0<t_1<\cdots<t_m$ los incrementos
\begin{equation}
    B_{t_1}=B_{t_1}-B_{t_0}, B_{t_2}-B_{t_1}, \ldots, B_{t_m}-B_{t_{m-1}}
\end{equation}
son independientes y cada de estos incrementos tiene distribución normal con
\begin{eqnarray}
\mathbb{E}[B_{t_{i+1}}-B_{t_i}]&=&0, \\
\mathbb{V}[B_{t_{i+1}}-B_{t_i}]&=& t_{i+1}-t_i
\end{eqnarray}
\end{defi}
\(\linebreak\)
Algunas diferencias entre el movimiento browniano \(B_t\) y una caminata aleatoria escalada, \(B_{t}^{n}\), es que dichas caminatas aleatorias tienen un cambio de \(\frac{1}{n}\) por unidad de tiempo, y son constantes entre dichos saltos en el tiempo, por otro lado, el movimiento browniano no es una función escalonada a trozos.
Otra diferencia, es que mientras que la caminata aleatoria escalada \(B_{t}^{n}\), es aproximadamente una variable aleatoria normal para cada tiempo \(t\), el movimiento browniano es exactamente una normal.\\

Hay dos formas de pensar en $w$ en la definición (1.8.1). Una es pensar en $\omega$ como la trayectoria del movimiento browniano. Se realiza un experimento aleatorio, y su resultado es la trayectoria del movimiento Browniano. Luego, $B_t$ es el valor de la trayectoria al tiempo $t$, el cual depende de qué trayectoria resultó del experimento aleatorio. Alternativamente, se puede pensar en $w$ como algo más
primitivo que la trayectoria en sí, de manera similar con el resultado obtenido del experimento de lanzar monedas, pero en este caso la moneda se lanza 'infinitamente rápido'. Una vez que se lanzaron las monedas y se obtuvo el resultado, se puede trazar la trayectoria del movimient Browniano. Si el lanzamiento se realiza nuevamente y sale una $w$ diferente, entonces se obtendrá una trayectoria diferente.

\(\linebreak\)
En cualquier caso, el espacio muestral $\Omega$ es el conjunto de todos los posibles resultados  que se puedan obtener de un experimento aleatorio, $\mathcal{F}$ es el $\sigma$-álgebra de subconjuntos de $\Omega$ cuyas probalilidades están definidas, y $\mathbb{P}$ es una medida de probabilidad. Para cada $A \in \mathcal{F}$, la probabilidad de $A$ , $\mathbb{P}(A)$ , es un número entre cero y uno.

\(\linebreak\)
Por ejemplo, al querer determinar la probabilidad del conjunto $A$
que contiene todos los $\omega \ \in \Omega$ que dan como resultado una trayectoria de Movimiento Browniano que satisface $0 \leq B_{0.25} \leq 0.2$. Consideremos primero este escenario para la caminata aleatoria escalada $B^{(100)}$.
Si queremos determinar el conjunto $\{ \omega : 0 \leq B^{(100)}_{0.25} \leq 0.2 \}$, notaríamos que la caminata aleatoria escalada $B^{(100)}$ cae entre 0 y 0.2 para el tiempo 0.25, la caminata aleatoria sin escala $M_{25}=10B^{(100)}_{0.25}$ cae entre 0 y 2 después de 25 lanzamientos. Dado que $M_{25}$ solo puede ser un número impar, cae entre 0 y 2 si y solo si es igual a 1 ó, equivalentemente, si y solo si $B^{(100)}_{0.25}=0.1$
Para lograr esto, los 25 lanzamientos de moneda deben resultar en 13 caras y 25 cruces. Por lo tanto, $A$ es el conjunto de las infinitas secuencias de lanzamientos de monedas con la propiedad de que en los 25 primeros lanzamientos hay 13 caras y 12 cruces. La probabilidad de que esto suceda es, por $\eqref{eq:mariobino}$, $\mathbb{P}(A)=0.1555$  

\(\linebreak\)
Para el movimiento Browniano $B_t$, también hay un conjunto de resultados $w$ para el
experimento aleatorio que da como resultado una trayectoria de un movimiento Browniano que satisface $0 \leq B_{0.25} \leq 0.2$. Además, existe un conjunto de $\omega \in \Omega$, cuya probabilidad es:
\begin{equation*}
    \mathbb{P}\{ 0 \leq B_{0.25} \leq 0.2 \} = \dfrac{2}{\sqrt{2\pi}} \int \limits_0^ {0.2} e^{-2x^2} dx
\end{equation*}
En lugar del área en la barra del histograma centrada en 0.1 en la Figura \ref{secc1.6_fig1}, ahora tenemos el área bajo la curva normal entre 0 y 0.2 en
esa figura. Áreas que son casi iguales.

\subsection{Distribución del Movimiento Browniano.}
Dado que los incrementos
 
\begin{equation*}
 B_{t_1}-B_{t_0}, \ B_{t_2}-B_{t_1}, \cdots, B_{t_m}-B_{t_{m-1}} 
\end{equation*}
son independientes y normalmente distribuidos, las variables aleatorias $B_{t_1},B_{t_2},...,B_{t_m}$  se distribuyen como una normal multivariada. Dicha distribución queda totalmente determinada por su vector de medias y matriz covarianzas. Cada variable aleatoria $B_{t_i}$ tiene media cero. Para $0 \leq s < t$, la covarianza de $B_s$ y $B_t$ es
\begin{eqnarray*}
    \mathbb{E} \left[ B_sB_t\right] &=& \mathbb{E} \left[ B_s(B_t - B_s) + B_{s}^2\right] \\
    && \text{ donde $B_s$ y $B_t-B_s$ son independientes} \\
    &=& \mathbb{E} \left[B_s\right] \mathbb{E} \left[B_t- B_s\right] + \mathbb{E} \left[B_{s}^2\right] \\
    &=& 0 + \mathbb{V}[B_s] \\
    &=& s
\end{eqnarray*}
Concluimos entonces que, si \(B_t\) es un Movimiento Browniano:
\[Cov(B_t,B_s)=\mathbb{E}[B_tB_s]=min\left \{ s,t \right \}\]
Por lo tanto la matriz de covarianzas del movimiento Browniano (i.e., para el vector aleatorio m-dimensional ($B_{t_1}, B_{t_2}, \cdots, B_{t_m} $) es
\begin{eqnarray*}
C &=& \left( \begin{array}{cccc}
\mathbb{E}\left[ B^2_{t_1} \right] & \mathbb{E}\left[ B_{t_1} B_{t_2} \right] & \cdots &  \mathbb{E}\left[ B_{t_1} B_{t_n} \right] \\ \\
\mathbb{E}\left[ B_{t_2} B_{t_1} \right] & \mathbb{E}\left[ B^2_{t_2} \right] & \dots & \mathbb{E}\left[ B_{t_2} B_{t_n} \right] \\ \\
\vdots & \vdots & \vdots & \vdots \\ \\
 \mathbb{E}\left[ B_{t_n} B_{t_1} \right]  &  \mathbb{E}\left[ B_{t_n} B_{t_2} \right]  & \cdots & \mathbb{E}\left[ B^2_{t_n} \right] \end{array} \right) \\ \\ \\
 &=&  \left(  
\begin{array}{cccc}
    t_1 & t_1 & \cdots & t_1  \\ \\
    t_1 & t_2 & \cdots & t_2 \\ \\
    \vdots & \vdots & \vdots & \vdots \\ \\
    t_1 & t_2 & \cdots & t_m
\end{array}
\right)
\end{eqnarray*}
Recuerde ahora que la función generadora de momentos de una v.a. normal con media cero y varianza \(t\),  está dada por
\begin{equation}
    \label{eq:keniagen}
    \varphi(u)= e^{\frac{1}{2}u^2 t}
\end{equation}
La función generadora de momentos de este vector aleatorio puede ser calculada utilizando la función generadora de momentos $\eqref{eq:keniagen}$ para una variable aleatoria normal con media cero, varianza \(t\) e independencia en los incrementos. El procedimientos es el siguiente.
\begin{align*}
   u_3 B_{t_3} + u_2 B_{t_2} + u_1 B_{t_1} &= u_3 ( B_{t_3}-B_{t_2})+ (u_2+u_3) (B_{t_2}- B_{t_1}) + (u_1+u_2+u_3)B_{t_1} 
\end{align*}
y de manera general
\begin{align*}
 u_m B_{t_m} + u_{m-1} B_{t_{m-1}} + \cdots + u_1 B_{t_1} &= u_m ( B_{t_{m}}-B_{t_{m-1}})+ (u_{m-1}+u_m) (B_{t_{m-1}}- B_{t_{m-2}}) \\
 &+ (u_{m-2}+u_{m-1}+u_m)(B_{t_{m-2}}- B_{t_{m-3}}) + \\
 &\hdots+ (u_1+u_2+ \cdots + u_m)B_{t_1} 
\end{align*}
 Con base a estos hechos se puede calcular la función generadora de momentos del vector aleatorio $(B_{t_1}, B_{t_2}, \cdots, B_{t_m}): $
 \begin{align*}
     \varphi(u_1, u_2, \cdots, u_m) &= \mathbb{E}\left[ e^{u_mB_{t_m} + u_{m-1} B_{t_{m-1}}  + \cdots + u_1 B_{t_{1}}} \right] \\
                         &= \mathbb{E}\left[e^{u_m \left(B_{t_m} - B_{t_{m-1}}\right) + \cdots + (u_1+ u_2+ \cdots + u_m)B_{t_1}}\right] \\
                          &= \mathbb{E}\left[e^{u_m\left(B_{t_m} - B_{t_{m-1}}\right)}\right]\cdot \mathbb{E}\left[e^{(u_{m-1}+u_m)\left(B_{t_{m-1}} - B_{t_{m-2}}\right)}\right]\cdots \mathbb{E}\left[e^{(u_1+ u_2+ \cdots + u_m)B_{t_1}}\right]\\
                          &= \exp{\left\{\dfrac{u^2_m(t_m - t_{m-1})}{2} \right\}}\cdot\exp{\left\{\dfrac{(u_{m-1}+u_m)^2(t_{m-1}-t_{m-2})}{2}\right\}}\\
                          &\cdots \exp{\left\{\dfrac{(u_1+u_2 + \cdots + u_m)^2t_1}{2} \right\}}
 \end{align*}
 En conclusión, la función generadora de momentos para el movimiento Browniano es
 \begin{align*}
  \varphi(u_1, u_2, \cdots, u_m) 
  &= \exp{\left\{ \dfrac{(u_1+u_2+\cdots + u_m)^2t_1}{2} + \dfrac{(u_2+u_3 + \cdots + u_m)^2(t_2-t_1)}{2}   \right.}\\
  &\left.+ \cdots + \dfrac{(u_{m-1}+u_m)^2(t_{m-1}+t_{m-2})}{2}+\dfrac{u^2_m(t_m-t_{m-1})}{2}\right\}
 \end{align*}

 La distribución de los incrementos Brownianos puede ser especificada bajo la función de densidad o la función generadora de momentos de las variables aleatorias $B_{t_1}, B_{t_2}, \cdots, B_{t_m}.$
 
%%DANIELA
\subsection{Propiedades de la filtración del Movimiento Browniano.}

\begin{defi} \label{Secc1.10_Def1}
Sea \((\Omega, \mathcal{F}, \mathbb{P})\) un espacio de probabilidad en el cual se define un movimiento Browniano \(B_t\), con \(t \geq 0\). Una filtración para el movimiento Browniano es una colección de $\sigma$-álgebras $\mathcal{F}_t$, con \(t \geq 0\), que satisface lo siguiente:

    \begin{enumerate}[(i)]
        \item \textbf{(La información se acumula)} Para $0 \leq s <t $, cada conjunto en $\mathcal{F}_s$ también está  en $\mathcal{F}_t$. En otras palabras, hay al menos tanta información disponible en el tiempo posterior $\mathcal{F}_t$, como la que hay en el tiempo anterior $\mathcal{F}_s$.
        \item \textbf{(Adaptabilidad)}
        Para cada $t\geq0$, el movimiento Browniano \(B_t\) al tiempo \(t\) es $\mathcal{F}_t$-medible. En otras palabras, la información disponible al tiempo \(t\) es suficiente para evaluar al movimento Browniano \(B_t\) en ese tiempo.
        \item \textbf{(Independencia de los incrementos futuros)}
        Para $0 \leq t < u $, el incremento $B_u-B_t$ es independiente de $\mathcal{F}_t$. En otras palabras, cualquier incremento del movimiento Browniano después del tiempo $t$ es independiente de la información disponible al tiempo \(t\).
     \end{enumerate}
Sea $\Delta_{t}$, con $t \geq0$, un proceso estocástico. Decimos que $\Delta_{t}$ se adapta a la filtración $\mathcal{F}_t$ si para cada $t \geq0$, la variable aleatoria $\Delta_{t}$ es $\mathcal{F}_t$-medible.
\end{defi}

Las propiedades (I) y (II), mencionadas en la definición anterior, garantizan que la información disponible para cada tiempo $t$ es al menos la misma que conoceríamos al observar el movimiento Browniano al tiempo $t$. La propiedad (III) dice que esta información no sirve para predecir movimientos futuros del Movimiento Browniano.\\

Existen dos posibilidades para la filtración $\mathcal{F}_t$ para un movimiento Browniano. La primera es dejar que $\mathcal{F}_t$ contenga solo la información obtenida al observar al mismo
Movimiento Browniano hasta el tiempo \(t\). La otra opción es incluir en $\mathcal{F}_t$ la información
obtenida al observar el movimiento Browniano y algunos otros procesos. Sin embargo, si la información en $\mathcal{F}_t$ incluye observaciones  de procesos distintos al Movimiento Browniano $B$, por la propiedad (III),esta información adicional no nos brinda pista alguna sobre los incrementos futuros de $B$.

\subsection{Propiedad de martingala del Movimiento Browniano.}
\begin{teor} \label{Secc1.11_Teorema1}
El Movimiento Browniano es una martingala
\end{teor}
\begin{proof}
Sea \(0 \leq s \leq t\), entonces
\begin{equation*}
    \mathbb{E} \left[ B_t| \mathcal{F}_s \right] = \mathbb{E}\left[ (B_t-B_s)+ B_s|\mathcal{F}_s \right]
\end{equation*}
como la esperanza condicional posee la propiedad de linealidad, considerando independencia y que \(B_s\) es \(\mathcal{F}_s\)-medible, tenemos que 
\begin{align*}
    \mathbb{E} \left[ B_t| \mathcal{F}_s \right]=&\mathbb{E} \left[ B_t- B_s|\mathcal{F}_s \right] + \mathbb{E} \left[B_s|\mathcal{F}_s \right] \\
    =& \mathbb{E}[ B_t- B_s] + B_s\\
    =& B_s
\end{align*}
por lo tanto, el Movimiento Browniano es una martingala.
\end{proof}
\subsection{Variación de primer orden}
Antes de probar que el Movimiento Browniano acumula \(T\) unidades de variación cuadrática entre los tiempos \(0\) y \(T\). Consideremos la función \(f(t)\) con la siguiente gráfica
\begin{center}
  \includegraphics[scale=0.6]{Im3.png}  
\end{center}
Queremos calcular la variación en la oscilación que hay entre los tiempos \(0\) y \(T\). Definamos la variación de primer orden \(FV_T(f)\) para la función \(f\)
\begin{equation} \label{3.4.2}
    \begin{split}
      FV_T(f)=& [f(t_1)-f(0)]  - [f(t_2)-f(t_1)] + [f(T)-f(t_2)] \\
      =& \int_0^{t_1} f'(t)dt+ \int_{t_1}^{t_2}(-f'(t))dt+ \int_{t_2}^T  f'(t)dt\\
      =& \int_{0}^T |f'(t)|dt
    \end{split}
\end{equation}
el término 
\[-[f(t_2)-f(t_1)]= f(t_1)-f(t_2) \]
se incluye de una manera tal que garantiza que la magnitud del movimiento descendente de la función \(f(t)\), entre los tiempos \(t_1\) y \(t_2\), se agregue, en lugar de restarla del total.\\

En general, para calcular la variación de primer orden, de una función al tiempo \(T\), elegimos una partición \(\Pi=\{t_0, t_1,...,t_n\}\) de \([0,T]\). No se necesita que los elementos de la partición guarden la misma distancia entre ellos, pero pueden tenerla. La máxima distancia entre los intervalos de la partición estará denotada como \(||\Pi||=\max_{j=0,...n-1}(t_{j+1}-t_j)\). Entonces, definimos 
\begin{equation} \label{3.4.3}
    FV_T(f)=\lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}|f(t_{j+1})-f(t_j)|
\end{equation}
El límite anterior se toma mientras el número \(n\) de puntos de la partición van a infinito, y la longitud del subintervalo más largo \(t_{j+1}-t_j\) tiende a cero.\\

Primero hay que verificar que la definición de (\ref{3.4.3}) sea consistente con la fórmula (\ref{3.4.2}). Para esto, usamos el teorema del valor medio, el cual dice que en cada subintervalo \([t_j,t_{j+1}]\) existe un punto \(t_j^*\) tal que 
\begin{equation} \label{3.4.4}
\frac{f(t_{j+1})-f(t_j)}{t_{j+1}-t_j}=f'(t_j^*)
\end{equation}
lo anterior se representa en la siguiente gráfica
\begin{center}
  \includegraphics[scale=0.6]{Im4.png}  
\end{center}
Ahora, si multiplicamos (\ref{3.4.4}) por \(t_{j+1}-t_j\) tenemos que
\begin{equation*}
    f(t_{j+1})-f(t_j)=f'(t_j^*)(t_{j+1}-t_j)
\end{equation*}
y la suma del lado derecho de (\ref{3.4.3}) estaría escrita como
\begin{equation*}
    \sum_{j=0}^{n-1}|f'(t_j^*)|(t_{j+1}-t_j)
\end{equation*}
la cual es una suma de Riemann para la integral de la función \(|f'(t_j^*)|\). Por lo tanto, 
\begin{equation*}
    FV_T(f)= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}|f'(t_j^*)|(t_{j+1}-t_j)= \int_0^T |f'(t)|dt
\end{equation*}
\subsection{Variación Cuadrática.}
\begin{defi} \label{Secc1.13_Def1}
Sea  \(f(t)\) una función definida para \(0 \leq t \leq T\). La variación cuadrática de \(f\) hasta el tiempo \(T\) está dada por:
\begin{equation}
    [f,f](T)= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}\left[f(t_{j+1})-f(t_j)\right]^2
\end{equation}
donde \(\Pi=\{t_0,t_1,...,t_n\}\) y \(0=t_0<t_1<...<t_n=T\)
\end{defi}

\textbf{Observación} Supongamos que la función \(f\) tiene derivada continua, entonces:
\begin{align*}
    \sum_{j=0}^{n-1}\left[f(t_{j+1})-f(t_j)\right]^2 &= \sum_{j=0}^{n-1} \left|f'(t_j^*)\right|^2(t_{j+1}-t_j)^2 \leq ||\Pi|| \sum_{j=0}^{n-1} \left|f'(t_j^*)\right|^2(t_{j+1}-t_j)
\end{align*}
entonces, si obtenemos el límite cuando \(||\Pi|| \to 0\)
\begin{align*}
    [f,f](T) &\leq \lim_{||\Pi|| \to 0} \left[||\Pi|| \sum_{j=0}^{n-1} \left|f'(t_j^*)\right|^2(t_{j+1}-t_j)\right]\\
    &= \lim_{||\Pi|| \to 0} ||\Pi||  \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} \left|f'(t_j^*)\right|^2\\
    &= \lim_{||\Pi|| \to 0} ||\Pi|| \int_0^T |f'(t)|^2 ~ dt\\
    &=0
\end{align*}
Lo anterior es válido, solo sí la derivada de \(f\) es continua, ya que esto implica que \(\int_0^T |f'(t)|^2 ~dt\) es finita, Sí \(\int_0^T |f'(t)|^2 ~dt\) es infinita, entonces la expresión
\[\lim_{||\Pi|| \to 0} \left[||\Pi|| \sum_{j=0}^{n-1} \left|f'(t_j^*)\right|^2(t_{j+1}-t_j)\right]\]
nos conducirá a situaciones de tipo \(0 \cdot \infty\), lo cual puede ser cualquier cosa entre \(0\) e \(\infty\).\\
La mayoría de las funciones tienen derivadas continuas y, por lo tanto, sus variaciones cuadráticas son \(0\), es por eso que no se suele utilizar la variación cuadrática en el cálculo ordinario. Por otra parte, las trayectorias del movimiento Browniano no son diferenciables con respecto al tiempo. Para funciones que no poseen derivadas, el Teorema del Valor Medio puede fallar, y la observación anterior no sería válida. \\

Consideremos el ejemplo siguiente, para la función de valor absoluto \(f(t)=|t|\), cuya gráfica es la siguiente
\begin{center}
  \includegraphics[scale=0.65]{Im1.png}  
\end{center}
La recta que une a los puntos \(\left(t_1, f(t_1)\right)\) y \(\left(t_2, f(t_2)\right)\) tiene pendiente \(\frac{1}{5}\), sin embargo, en ningún punto, entre \(t_1\) y \(t_2\), la derivada de \(f(t)=|t|\) es igual a \(\frac{1}{5}\). Por lo que tenemos que

\begin{equation*}
     f'(t) = \left\{
	       \begin{array}{ll}
		 -1 & \mathrm{si\ } t<0 \\
		  1 & \mathrm{si\ } t>0 \\
		 \mathrm{Indefinida} & \mathrm{si\ } t=0
	       \end{array}
	     \right.
   \end{equation*}
Nótese que la derivada no está definida para \(t=0\), que es donde la gráfica tiene un pico. La gráfica siguiente sugiere que las trayectorias del Movimiento Browniano tienen demasiados picos. 
\begin{center}
    \includegraphics[scale=0.45]{Im2.png}  
\end{center}
De hecho, para un Movimiento Browniano \(B_t\), no existe un valor \(t\) para el cual \(\frac{d}{dt}B_t\) esté definida.\\

\begin{teor} \label{Secc1.13_Teorema1}
Sea \(B\) un Movimiento Browniano. Entonces \([B,B](T)=T\) para toda \(T \geq 0 \) casi seguramente.\\
\end{teor}
Recordemos que  \textit{casi seguramente}, en este caso, implica que pueden haber trayectorias del Movimiento Browniano para las cuales la afirmación \([B,B](T)=T\) no se cumple.
Sin embargo, el conjunto de todas estas trayectorias tiene probabilidad \(0\). El conjunto de trayectorias
para el cual la afirmación del teorema es verdadera tiene probabilidad \(1\).
\begin{proof}
Sea \(\Pi=\{t_0,t_1,...,t_n\}\) una partición de \([0,T]\), definimos la variación cuadrática correspondiente a esta partición como
\[Q_\Pi= \sum_{j=0}^{n-1} \left(B_{t_{j+1}}-B_{t_j}\right)^2\]
Debemos demostrar que esta variación cuadrática, la cual es una variable aleatoria, es decir, que depende de la trayectoria del Movimiento Browniano con la cual es
calculada, converge a \(T\) cuando \(||\Pi||\) converge a \(0\). Probaremos que su valor esperado es \(T\), y que su varianza converge a \(0\). Y así,  converge al valor esperado \(T\), independientemente de la trayectoria al rededor de la cual se están realzizando los cálculos.\\

La variación cuadrática es una suma de variables aleatorias independientes, por lo tanto, su media y su varianza serán la suma de las medias y varianzas, respectivamente, de esas variables aleatorias. Así, tenemos que
\begin{equation} \label{3.4.6}
    \mathbb{E}\left[\left(B_{t_{j+1}}-B_{t_j}\right)^2\right]= \mathbb{V}\left[B_{t_{j+1}}-B_{t_j}\right]= t_{j+1}-t_j
\end{equation}
lo que implica que
\begin{equation*}
    \mathbb{E}(Q_\Pi)=\sum_{j=0}^{n-1}\mathbb{E}\left[(B_{t_{j+1}}-B_{t_j})^2\right]=\sum_{j=0}^{n-1}(t_{j+1}-t_j)=T
\end{equation*}
tal y como se esperaba, más aún
\begin{align*}
    \mathbb{V}\left[(B_{t_{j+1}}-B_{t_j})^2\right]=&\mathbb{E}\left[\left((B_{t_{j+1}}-B_{t_j})^2-(t_{j+1}-t_j)\right)^2\right]\\
    =& \mathbb{E}\left[(B_{t_{j+1}}-B_{t_j})^4\right]-2(t_{j+1}-t_j)\mathbb{E}\left[(B_{t_{j+1}}-B_{t_j})^2\right]\\+&(t_{j+1}-t_j)^2
\end{align*}
%%VER EJERCICIO 3.3
Recordemos que el cuarto momento de una variable aleatoria normal, con media cero, es tres veces su varianza al cuadrado. Por lo tanto,
\[\mathbb{E}\left[(B_{t_{j+1}}-B_{t_j})^4\right]=3(t_{j+1}-t_j)^2\]
y además
\begin{align*}
    \mathbb{V}\left[(B_{t_{j+1}}-B_{t_j})^2\right]= & 3(t_{j+1}-t_j)^2-2(t_{j+1}-t_j)^2+(t_{j+1}-t_j)^2
\end{align*}
por lo tanto, 
\begin{equation} \label{3.4.7}
     \mathbb{V}\left[(B_{t_{j+1}}-B_{t_j})^2\right]=2(t_{j+1}-t_j)^2
\end{equation}
entonces, tenemos que 
\begin{align*}
    \mathbb{V}(Q_\Pi)=&\sum_{j=0}^{n-1}\mathbb{V}\left[(B_{t_{j+1}}-B_{t_j})^2\right]\\
    =& \sum_{j=0}^{n-1}2 (t_{j+1}-t_j)^2\\
    \leq & \sum_{j=0}^{n-1} 2~||\Pi||~(t_{j+1}-t_j)\\
    =&2~||\Pi||~T
\end{align*}
En particular, \(\lim_{||\Pi|| \to 0} \mathbb{V}(Q_\Pi)\) es igual a \(0\), por lo tanto, \(Q_{\Pi}=\mathbb{E}(Q_{\Pi})=T\)
\end{proof}

\textbf{Observación} De la prueba anterior, recordemos las ecuaciones (\ref{3.4.6}) y (\ref{3.4.7}). Podríamos argumentar que \(t_{j+1}-t_j\) es pequeño, así, \((t_{j+1}-t_j)^2\) es todavía más pequeño, y por lo tanto, \((B_{t_{j+1}}-B_{t_j})^2\), aunque es aleatorio, se encuentra cercano a su media, \(t_{j+1}-t_j\), con alta probabilidad. De ese modo, podemos afirmar que 
\begin{equation} \label{3.4.8}
   (B_{t_{j+1}}-B_{t_j})^2 \approx t_{j+1}-t_j 
\end{equation}

%%LO DEL CONTENIDO xd
Esta afirmación es verdadera, pues, cuando \(t_{j+1}-t_j\) es pequeño, ambos lados son cercanos a \(0\). También sería cierta si eleváramos al cuadrado el lado derecho, y lo multiplicáramos por \(2\), o si hiciéramos más modificaciones significativas del lado derecho. En otras palabras, la ecuación (\ref{3.4.8}) no tiene mucho contenido. Otra forma de abordar esta idea es escribiendo
\begin{equation}\label{3.4.9}
    \frac{ (B_{t_{j+1}}-B_{t_j})^2}{t_{j+1}-t_j } \approx 1
\end{equation}
en lugar de (\ref{3.4.8}). Sin embargo, 
\[\frac{ (B_{t_{j+1}}-B_{t_j})^2}{t_{j+1}-t_j }\]
no es cercana a \(1\), independientemente de qué tan pequeño hagamos a \(t_{j+1}-t_j\). Notemos que es el cuadrado de la variable aleatoria normal estándar
\[Y_{j+1}=\frac{B_{t_{j+1}}-B_{t_j}}{\sqrt{t_{j+1}-t_j}}\]
y su distribución es la misma, sin importar qué tan pequeño hagamos a \(t_{j+1}-t_j\).\\

Para entender mejor la idea detrás del Teorema \ref{Secc1.13_Teorema1}, elegimos un valor grande para \(n\) y consideramos \(t_j=\frac{jT}{n}\), con \(j=0,1,...,n\). Entonces \(t_{j+1}-t_j=\frac{T}{n}\) para toda \(j\), y 
\[ (B_{t_{j+1}}-B_{t_j})^2=T~\frac{Y^2_{j+1}}{n}\]
Como las variables aleatorias \(Y_1, Y_2,...,Y_n\) son independientes e idénticamente distribuidas, siguiendo la Ley de los Grandes Números tenemos que si \(n \to \infty\), entonces \(\sum_{j=0}^{n-1}\frac{Y^2_{j+1}}{n}\) converge a \(\mathbb{E}\left[Y^2_{j+1}\right]\). Esta esperanza es igual a \(1\), y por lo tanto, \(\sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_j})^2\) converge a T. Cada uno de los términos \((B_{t_{j+1}}-B_{t_j})^2\) dentro de la suma puede ser distinto a su media \(t_{j+1}-t_j=\frac{T}{n}\), pero cuando sumamos muchos
términos como este, el promedio de las diferencias es \(0\).\\

Escribamos, de manera informal 
\begin{equation} \label{3.4.10}
    dB_t~dB_t= dt
\end{equation}
Esto solo aplica cuando sumamos ambos lados de (\ref{3.4.9}) y hacemos uso de la Ley de los Grandes Números para cancelar los errores de una afirmación cierta. La afirmación es que para un intervalo \([0,T]\), el Movimiento Browniano acumula \(T\) unidades de variación cuadrática.Si calculamos la variación cuadrática para el intervalo \([0,T_1]\), obtenemos \([B,B](T_1)=T_1\). Si calculamos la variación cuadrática para el intervalo \([0,T_2]\), cuando \(0 <T_1<T_2\), obtenemos \([B,B](T_2)=T_2\). Entonces, si partimos el intervalo \([T_1,T_2]\), elevando al cuadrado los incrementos del Movimiento Browniano para cada subintervalo de la partición, sumamos los incrementos al cuadrado, y tomamos el límite a medida que la máxima distancia entre los elementos del intervalo se acerca a \(0\), obtenemos el siguiente límite
\[[B,B](T_2)-[B,B](T_1)= T_2-T_1\]
El Movimiento Browniano acumula \(T_2-T_1\) unidades de variación cuadrática en el intervalo \([T_1,T_2]\), y como esto es cierto para cada intervalo de tiempo, podemos concluir que el Movimiento Browniano acumula variación cuadrática con razón de uno por unidad de tiempo.\\

En particular, el \(dt\) del lado derecho de (\ref{3.4.10}), se multiplica por un \(1\).\\

Cabe resaltar que la variación cuadrática del movimiento browniano es un factor clave para la volatilidad en los precios de los activos impulsados por el movimiento browniano.\\

%observación 3.4.5
\textbf{Observación 1.13.1} Sea \(\Pi=\{t_0,t_1,...,t_n\}\) una partición de \([0,T]\), es decir \(t_0<t_1<...<t_n\) con \(t_0=0\) y \(t_n=T\), para calcular la variación cuadrática del Movimiento Browniano 
\begin{equation} \label{3.4.11}
    \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_j})^2=T
\end{equation}
podemos calcular la variación cruzada de \(B_t\) con \(t\), y la variación cuadrática de \(t\), las cuales son:
\begin{equation} \label{3.4.12}
    \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_j})(t_{j+1}-t_j)=0
\end{equation}

\begin{equation} \label{3.4.13}
     \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}(t_{j+1}-t_j)^2=0
\end{equation}
Para ver que \(0\) es el límite en (\ref{3.4.12}) observemos que
\begin{equation*}
  \left|(B_{t_{j+1}}-B_{t_j})(t_{j+1}-t_j)\right|\leq \max_{0 \leq k \leq n-1} |B_{t_{k+1}}-B_{t_k}| (t_{j+1}-t_j)
\end{equation*}
y entonces
\begin{equation*}
    \left|\sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_j})(t_{j+1}-t_j)\right| \leq \max_{0 \leq k \leq n-1}|B_{t_{k+1}}-B_{t_k}| T
\end{equation*}
Como \(B\) es continuo, \(\max_{0 \leq k \leq n-1}|B_{t_{k+1}}-B_{t_k}|\) tiene límite cero, siempre que \(||\Pi|| \to 0\).\\
Ahora, para ver que \(0\) es el límite en (\ref{3.4.13}) observemos lo siguiente
\begin{equation*}
    \sum_{j=0}^{n-1}(t_{j+1}-t_j)^2 \leq \max_{0 \leq k \leq n-1} (t_{k+1}-t_k) \sum_{j=0}^{n-1}(t_{j+1}-t_j)= ||\Pi||T
\end{equation*}
el cual tiene por límite cero, siempre que \(||\Pi|| \to 0\).

\subsection{Volatilidad del Movimiento Browniano Geométrico.}
Sean \(\alpha\) y \(\sigma>0\) constantes, definamos el Movimiento Browniano  %XD
\[S_t=S_0\exp{\left\{\sigma B_t+\left(\alpha - \frac{1}{2}\sigma^2\right)t\right\}}\]
Este es el modelo de precio de activos utilizado en la fórmula de Black-Scholes-Merton. Lo que haremos ahora, será entender cómo usar la variación cuadrática del Movimiento Browniano para identificar la volatilidad \(\sigma\) de una trayectoria en este proceso.\\

Sean \(0 \leq T_1 < T_2\), dados, y supongamos que observamos el Movimiento Browniano Geométrico \(S_t\) para \(T_1\leq t \leq T_2\). Entonces podemos elegir una partición para dicho intervalo, \(T_1=t_0<t_2<...<t_m=T_2\). Observemos los log-rendimientos
\[\log\frac{S_{t_{j+1}}}{S_{t_j}}=\sigma(B_{t_{j+1}}-B_{t_j})+\left(\alpha - \frac{1}{2}\sigma^2\right)(t_{j+1}-t_j)\]
sobre cada uno de los subintervalos \([t_j, t_{j+1}]\). La suma de los cuadrados de los log-rendimientos es:

\begin{equation} \label{3.4.15}
    \begin{split}
        \sum_{j=0}^{m-1}\left(\log\frac{S_{t_{j+1}}}{S_{t_j}}\right)^2=&\sigma^2 \sum_{j=0}^{m-1}(B_{t_{j+1}}-B_{t_j})^2+\left(\alpha - \frac{1}{2}\sigma^2\right)^2~\sum_{j=0}^{m-1}(t_{j+1}-t_j)^2\\
    &+2\sigma \left(\alpha - \frac{1}{2}\sigma^2\right) \sum_{j=0}^{m-1}(B_{t_{j+1}}-B_{t_j})(t_{j+1}-t_j)
    \end{split}
\end{equation}

%%%REVISAR OBSERVACIÓN 3.4.5 
En donde la máxima distancia entre los elementos del intevalo  \(||\Pi||=\max_{j=0,...,m}(t_{j+1}-t_j)\) es pequeña, así, el primer término del lado derecho de (\ref{3.4.15}) es aproximadamente igual a su límite, el cual es \(\sigma^2\) multiplicado por la variación cuadrática acumulada por el Movimiento Browniano en el intervalo \([T_1,T_2]\), es decir, \(T_2-T_1\). El segundo término del lado derecho de (\ref{3.4.15}) es \(\left(\alpha - \frac{1}{2}\sigma^2\right)^2\) multiplicado por la variación cuadrática de \(t\), la cual es igual a \(0\), de acuerdo a la observación 1.13.1. El tercer término del lado derecho de (\ref{3.4.15}) es \(2\sigma \left(\alpha - \frac{1}{2}\sigma^2\right)\) multiplicado por la variación cruzada de \(B_t\) y \(t\), la cual, por la observación 1.13.1, es \(0\). Por lo tanto, concluimos que cuando la máxima distancia entre elementos del intevalo \(||\Pi||\) es pequeña, el lado derecho de (\ref{3.4.15}) es aproximadamente igual a \(\sigma^2(T_2-T_1)\). Así,
\begin{equation}\label{3.4.16}
    \frac{1}{T_2-T_1}\sum_{j=0}^{m-1}\left(\log\frac{S_{t_{j+1}}}{S_{t_j}}\right)^2 \approx \sigma^2
\end{equation}

Si el precio del activo \(S_{t}\) es realmente un Movimiento Browniano Geométrico, con volatilidad constante \(\sigma\), entonces \(\sigma\) puede identificarse a partir de observaciones de precios, calculando el
lado izquierdo de (\ref{3.4.16}), y obteniendo su raíz cuadrada.En teoría, podemos hacer esta aproximación tan precisa como queramos, disminuyendo la distancia entre los elementos de la partición. En la práctica, existe un límite respecto a qué tan pequeño puede ser la distancia en esta partición.


\subsection{Propiedad de Markov.}
\begin{teor} \label{Secc1.15_Teorema1}
Sea \(B_{t}\), con \(t \geq 0\), un Movimiento Browniano, y sea \(\mathcal{F}_{t}\) con \(t \geq 0\) una filtración para este Movimiento Browniano, entonces \(B_{t}\) es un proceso de Markov.
\end{teor}
\begin{proof}
Debemos probar que siempre que \(0 \leq s \leq t\), y que \(f\) sea una función de Borel, entonces existe otra función de Borel \(g\) tal que
\begin{equation} \label{3.5.1}
    \mathbb{E}\left[f\left(B_{t}\right)|\mathcal{F}_s\right]=g\left(B_s\right)
\end{equation}
Para llegar a esto, escribamos lo siguiente
\begin{equation}\label{3.5.2}
    \mathbb{E}\left[f\left(B_{t}\right)|\mathcal{F}_{s}\right]=\mathbb{E}\left[f\left((B_{t}-B_{s})+B_{s}\right)|\mathcal{F}_{s}\right]
\end{equation}
La variable aleatoria \(B_{t}-B_{s}\) es independiente de \(\mathcal{F}_{s}\), y la variable aleatoria \(B_{s}\) es \(\mathcal{F}_{s}\)-medible. Lo anterior nos permite aplicar el Lema de Independencia.Con el fin de calcular la esperanza del lado derecho de (\ref{3.5.2}), reemplazamos a \(B_{s}\) por una variable 'muda' \(x\), para mantenerlo constante, y después, obtenemos la esperanza no condicionada de la variable aleatoria restante, es decir, definimos \(g(x)=\mathbb{E}[f(B_{t}-B_{s}+x)]\), pero \(B_{t}-B_{s}\) se distribuye de forma normal, con media \(0\) y varianza \(t-s\). Por lo tanto,

\begin{equation} \label{3.5.3}
    g(x)=\frac{1}{\sqrt{2\pi(t-s)}} \int_{-\infty}^\infty f(w+x) e^{-\frac{w^2}{2(t-s)}} ~dw
\end{equation}
El Lema de Independencia establece que si tomamos la función \(g(x)\) definida como (\ref{3.5.3}) y reemplazamos la variable 'muda' por la variable aleatoria \(B_{s}\), entonces se cumple la ecuación (\ref{3.5.1}), y por lo tanto \(B_{t}\) es un proceso de Markov.
\end{proof}

Podemos hacer el cambio de variable \(\tau=t-s\) y \(y=w+x\) en la ecuación (\ref{3.5.3}) para obtener

\begin{equation*} 
    g(x)=\frac{1}{\sqrt{2\pi \tau}} \int_{-\infty}^\infty f(y) e^{-\frac{(y-x)^2}{2\tau}} ~dy
\end{equation*}
Definimos la densidad de transición \(p(\tau, x,y)\) para un Movimiento Browniano como
\begin{equation*}
  p(\tau, x,y)=  \frac{1}{\sqrt{2\pi \tau}} e^{-\frac{(y-x)^2}{2\tau}}
\end{equation*}
así, podemos escribir a la ecuación (\ref{3.5.3}) como
\begin{equation} \label{3.5.4}
    g(x)= \int_{-\infty}^\infty f(y) p(\tau, x,y)~dy
\end{equation}
y a la ecuación (\ref{3.5.1}) como

\begin{equation} \label{3.5.5}
    \mathbb{E}\left[f\left(B_{t}\right)|\mathcal{F}_{s}\right]=\int_{-\infty}^\infty f(y) p(\tau, B_{s},y)~dy
\end{equation}
Dada la información de \(\mathcal{F}_{s}\), la densidad condicional de \(B_{t}\) es \( p(\tau, B_{s},y)\), esta densidad es normal con media \(B_{s}\) y varianza \(\tau=t-s\). De hecho, la única información relevante que aporta \(\mathcal{F}_{s}\) es \(B_{s}\). El hecho de que solo \(B_{s}\) es relevante es la esencia de la Propiedad de Markov.


\subsection{Primera aproximación de la distribución del tiempo.}
Empecemos definiendo una martingala que contenga al Movimiento Browniano en la función exponencial, y también una constante fija \(\sigma\). A la siguiente martingala se le conoce como \textit{martingala exponencial}, correspondiente a \(\sigma\)

 \begin{equation}\label{3.6.1}
     Z_t= \exp\left\{\sigma B_t-\frac{1}{2}\sigma^2t\right\}
 \end{equation}
la cual tendrá un papel muy importante a lo largo de esta sección.\\

\begin{teor} \label{Secc1.16_Teorema1}
\textbf{(Martingala exponencial)} Sea \(B_t\) con \(t \geq 0\) un Movimiento Browniano, con una filtración \(\mathcal{F}_t\), con \(t \geq 0\), y sea \(\sigma\) una constante. El proceso \(Z_t\) definido en (\ref{3.6.1}), con \(t \geq 0\), es una martingala.
\end{teor}

%%arregla corchetes
\begin{proof}
Para \(0 \leq s \leq t\) tenemos que
\begin{equation} \label{3.6.2}
    \begin{split}
        \mathbb{E}\left[Z_t|\mathcal{F}_s\right] &=  \mathbb{E}\left[\exp\left.\left\{\sigma B_t-\frac{1}{2}\sigma^2t\right\}\right|\mathcal{F}_s\right]\\
         &=  \mathbb{E}\left[\exp\{\sigma(B_t-B_s)\}\exp\left.\left\{\sigma B_s-\frac{1}{2}\sigma^2t\right\}\right|\mathcal{F}_s\right]\\
         &= \exp\left\{\sigma B_s-\frac{1}{2}\sigma^2t\right\} \mathbb{E}[\exp\{\sigma(B_t-B_s)\}|\mathcal{F}_s]
    \end{split}
\end{equation}
Luego, por independencia, tenemos 
\begin{equation*}
    \mathbb{E}[\exp \{\sigma(B_t-B_s)|\mathcal{F}_s\}]=\mathbb{E}[\exp \{\sigma(B_t-B_s)\}]
\end{equation*}

pues \(B_t-B_s\) se distribuye como normal con varianza \(0\) y varianza \(t-s\), y su valor esperado es \(\frac{1}{2}\sigma^2(t-s)\). Sustituyendo en (\ref{3.6.2}) obtenemos 
\begin{equation}
    \mathbb{E}[Z_t|\mathcal{F}_s]=\exp\left\{\sigma B_s-\frac{1}{2}\sigma^2s\right\}= Z_s
\end{equation}
por lo tanto, \(Z_t\) es una martingala. 
\end{proof}

Sea \(m\) un número real, definamos el primer tiempo de llegada del proceso a \(m\) como
\begin{equation} \label{3.6.3}
    \tau_m=\min\{t \geq 0; B_t=m\}
\end{equation}
Si el Movimiento Browniano nunca alcanza el nivel \(m\), entonces establecemos \(\tau=\infty\). Una martingala que se encuentra detenida en un tiempo de paro, sigue siendo una martingala y debería tener esperanza constante debido a lo siguiente

\begin{equation} \label{3.6.4}
    1= Z_0= \mathbb{E}\left[Z_{t\wedge \tau_m}\right]= \mathbb{E}\left[\exp \left\{\sigma B_{t\wedge \tau_m}-\frac{1}{2}\sigma^2(t\wedge \tau_m)\right\}\right]
\end{equation}

donde \((t\wedge \tau_m)\) denota el mínimo entre \(t\) y \(\tau_m\). Para el siguiente paso, asumimos que \(\sigma >0\) y \(m>0\). En este caso, el Movimiento Browniano siempre es igual o menor al nivel \(m\) para \(t\leq \tau_m\), entonces
\begin{equation} \label{3.6.5}
    0 \leq \exp\{\sigma B_{(t\wedge \tau_m)}\} \leq e^{\sigma m}
\end{equation}
Si \(\tau_m < \infty\), el término \(\exp\left\{-\frac{1}{2}\sigma^2(t\wedge \tau_m)\right\}\) es igual a  \(\exp\left\{-\frac{1}{2}\sigma^2 \tau_m\right\}\) para \(t\) suficientemente grande. Por otro lado, si \(\tau_m=\infty\), entonces el término \(\exp\left\{-\frac{1}{2}\sigma^2(t\wedge \tau_m)\right\}\) es igual a \(\exp\left\{-\frac{1}{2}\sigma^2 t\right\}\), y mientras \(t \to \infty\), este converge a \(0\). Lo anterior podemos escribirlo de la siguiente manera
\begin{equation*}
    \lim_{t \to \infty} \exp\left\{-\frac{1}{2}\sigma^2(t\wedge \tau_m)\right\}= \mathbbm{1}_{\tau_m < \infty} \exp\left\{-\frac{1}{2}\sigma^2 \tau_m\right\}
\end{equation*}

Así, \(\exp\{\sigma B_{(t\wedge \tau_m)}\}=\exp\{\sigma B_{\tau_m}\}= e^{\sigma m}\), para \(t\) suficientemente grande. Si \(\tau_m =\infty\), entonces no sabemos qué pasa con \(\exp\{\sigma B_{(t\wedge \tau_m)}\})\) mientras \(t \to \infty\), pero al menos sabemos que este término está acotado, por (\ref{3.6.5}), esto es suficiente para asegurar que el producto de \(\exp\{\sigma B_{(t\wedge \tau_m)}\})\) y \(\exp\left\{-\frac{1}{2}\sigma^2 \tau_m\right\}\) tiene límite \(0\), en este caso. Por lo cual, tenemos
\begin{equation*}
    \lim_{t \to \infty} \exp\left\{\sigma B_{(t\wedge \tau_m)}-\frac{1}{2}\sigma^2 (t\wedge \tau_m)\right\}=\mathbbm{1}_{\tau_m < \infty}  \exp\left\{\sigma m-\frac{1}{2}\sigma^2 \tau_m\right\}
\end{equation*}
y si sacamos el límite de (\ref{3.6.4}) obtenemos que 
\begin{equation*}
    1=\mathbb{E}\left[\mathbbm{1}_{\tau_m < \infty}\exp\left\{\sigma m-\frac{1}{2}\sigma^2 \tau_m\right\} \right]
\end{equation*}
o, de forma equivalente
\begin{equation} \label{3.6.6}
    \mathbb{E}\left[\mathbbm{1}_{\tau_m < \infty}\exp\left\{-\frac{1}{2}\sigma^2 \tau_m\right\} \right]= e^{-\sigma m}
\end{equation}

La ecuación anterior es cierta mientras que \(m\) y \(\sigma\) sean positivas. No deberíamos considerar \(\sigma=0\), pero dado que es válida para todos los positivos, debemos tomar el límite de ambos lados, considerando \(\sigma \downarrow 0\). Esto implica que \(\mathbb{E}\left[\mathbbm{1}_{\tau_m<\infty}\right]=1\), o de forma equivalente:
\begin{equation} \label{3.6.7}
    \mathbb{P}(\tau_m<\infty)=1
\end{equation}
pues \(\tau_m\) es finito, casi seguramente. Considerando esto en (\ref{3.6.6}) obtenemos

\begin{equation} \label{3.6.8}
    \mathbb{E}\left[\exp\left\{-\frac{1}{2}\sigma^2 \tau_m\right\} \right]= e^{-\sigma m}
\end{equation}
lo cual ahorrará trabajo para la demostración del siguiente teorema.

\begin{teor} \label{Secc1.16_Teorema2}
Para \(m \in \mathbb{R}\) el primer tiempo de llegada del Movimiento Browniano al nivel \(m\) es finito, casi seguramente, y la Transformada de Laplace de esa distribución está dada por 
\begin{equation} \label{3.6.9}
    \mathbb{E}[e^{-\alpha \tau_m}] = e^{-|m|\sqrt{2\alpha}}
\end{equation}
para toda \(\alpha>0\)
\end{teor}
\begin{proof}
Consideremos el primer caso, cuando \(m\) es positiva, sea \(\alpha\) una constante positiva, y definamos \(\sigma=\sqrt{2\alpha} \) tal que \(\frac{1}{2}\sigma^2=\alpha\). Entonces, la ecuación (\ref{3.6.8}) pasa a ser igual a la ecuación (\ref{3.6.9}). En caso de que \(m\) se negativa, dado que el Movimiento Browniano es simétrico, el primer tiempo de llegada \(\tau_m\) y \(\tau_{|m|}\) tienen la misma distribución, por lo que la ecuación (\ref{3.6.9}) también es válida para este caso.
\end{proof}

\newpage    
\section{Introducción al cálculo estocástico}


\subsection{ La integral de Itô para integrandos simples }
La integral de Itô se utiliza para modelar el valor de un portafolio como el resultado del trading de los activos en tiempo continuo. \\

Sea \(\left \{ B_{t} \right \}_{t\geq 0}\) un movimiento Browniano y \(\mathcal{F}_t\) la filtración canónica inducida por este , esto es ,\(\mathcal{F}_t=\sigma (B_{s} : o\leq t\leq s)\).
Sea \(T>0\) a lo largo de esta sección se buscara dar sentido a la siguiente expresión
\[\int_{0}^{T}\Delta_{t}dB_t\]
donde \(0 \leq t\), y \(\Delta_{t}\) un proceso estocástico adaptado a la filtración canónica del Browniano \(\left \{ B_{t} \right \}_{t\geq 0}\). Recordemos que un proceso estocástico \(\left \{ X_{t} \right \}_{t\geq 0}\) en un espacio  de probabilidad se dice adaptado a una filtración \(\left\{\mathcal{F}_t\right\}_{t\geq 0}\) si \(X_{t}\) es \(\mathcal{F}_t\)-medible. Así pues \(\Delta_{t}\) será \(\mathcal{F}_t\)-medible para cualquier \(t \geq 0\).
Así, la información obtenida a tiempo $t$ es suficiente para evaluar \(\Delta_{t}\) en ese tiempo.
Esto puede interpretarse como que \(\Delta_{t}\) es una variable aleatoria, pero al tiempo \(t\) existe suficiente información para evaluarla y deja de ser aleatoria. \\ \\
Recordemos también que el Movimiento Browniano por definición tiene incrementos independientes, por lo que
las posiciones que tomamos sobre los activos pueden depender de su histórico, pero deben ser independientes de los incrementos futuros del Movimiento Browniano que impulsa esos precios. \\

El problema que se enfrentará para conseguir el objetivo planteado, es que las trayectorias del movimiento Browniano no son diferenciables, por lo que la integral de Lebesgue no funcionara para este proceso, esto nos obligara a iniciar una construcción desde cero para definir está integral. 

\textbf{Observación} En algunas de las gráficas utilizadas a lo largo de esta sección, se utiliza la notación \(\delta(t)\), en vez de \(\delta_t\).

\subsubsection{Construcción de la integral}
Sea \(\Pi=\left \{ t_0,t_1,...,t_n \right \}\) una partición de \([0,T]\), es decir, \[0=t_0\leq t_1\leq ...\leq t_n=T\]
Supongamos que \(\Delta_{t}\) es constante en \(t\) en cada subintervalo \([t_j,t_{j+1})\), un proceso con estás caracteristicas se denominará \textit{simple}. Este comportamiento se ilustra con la siguiente gráfica: 
\begin{center}
 \includegraphics[scale=.4]{Camino de un proceso simple.jpg}     
\end{center}
La figura nos enseña una trayectoria de un proceso simple \(\Delta_{t}\). Nosotros siempre debemos elegir procesos simples para tomar un valor en la partición al tiempo \(t_j\) y luego mantenerlo constante pero sin incluir la siguiente partición al tiempo
 \(t_{j+1}\). La trayectoria que se muestra depende de la misma \(\omega\) de la cual depende la trayectoria del movimiento Browniano \(B_t\) correspondiente. Si uno eligiera, por ejemplo, una \(\omega\) diferente, se tendría una trayectoria del movimiento Browniano distinta, y posiblemente una trayectoria diferente de \(\Delta_{t}\). Sin embargo, el valor de \(\Delta_{t}\) depende solo de la información disponible al tiempo t.
Como no hay información en el momento 0, el valor de \(\Delta_{0}\) debe ser el mismo para todas las trayectorias, y por lo tanto, el primer trozo de \(\Delta_{t}\), para \(0  \leq t < t_1\), realmente no depende de \(\omega\). El valor de \(\Delta_{t}\) en el segundo intervalo, \([t_1, t_2)\), puede depender de observaciones realizadas durante el primer intervalo de tiempo \([0, t_1)\).\\

La interección entre \(\Delta_{t}\) y  el movimiento Browniano \(B_t\) vista desde un modelo financiero es la siguiente. Pensemos en \(B_t\) como el precio de una acción de un activo al tiempo $t$, los tiempos de la partición \(t_0,t_1,..., t_{n-1}\) como fechas de trading para el activo y las \(\Delta_{t_i}\) como el número de acciones a comprar de ese activo a cierta fecha \(t_i\), y que serán retenidas hasta la siguiente fecha, de esta forma la ganancia se ve de la siguiente manera (nuestra posición por la diferencia de los precios):
\[I_{t}=\Delta_{t_0}\left [ B_t-B_{t_{0}} \right ]=\Delta_{t_0}B_t, \;\;\; 0\leq t\leq t_1\]
\[I_{t}=\Delta_{0} B_{t_{1}}+ \Delta_{t_1}\left [ B_{t}-B_{t_{1}} \right ], \;\;\; t_1\leq t\leq t_2\]
\[I_{t}=\Delta_{0} B_{t_{1}}+ \Delta (t_1)\left [ B_{t_{2}}-B_{t_{1}} \right ]+ \Delta_{t_2}\left [ B_t-B_{t_{2}} \right ], \;\;\; t_2\leq t\leq t_3\]
Así en general, para \(t_k\leq t\leq t_{k+1}\)
\begin{equation} \label{2.1}
I_{t}=\sum_{j=0}^{k-1}\Delta_{t_j}\left [ B_{t_{j+1}}-B_{t_{j}} \right ]+\Delta_{t_k}\left [ B_t-B_{t_{k}} \right ]
\end{equation}
El proceso \(I_{t}\) de la ecuación (\ref{2.1}) es la integral de Itô para procesos simples \(\Delta_{t}\), tambíen se puede escribir  como
\[I_{t}=\int_{0}^{t}\Delta_{u}dB_u\]
y además definimos esta integral para cada límite superior de integración entre $0$  y \(T\)



\subsubsection{Propiedades de la Integral} 
La integral de Itô está definida como la ganancia del traiding en la martingala \(B_t\). \\

\begin{teor}
La integral de Itô definida de la siguiente manera
\[ I_{t}=\sum_{j=0}^{k-1}\Delta_{t_j}\left [ B_{t_{j+1}}-B_{t_{j}} \right ]+\Delta_{t_k}\left [B_t-B_{t_{k}} \right ]\]
es una martingala. 
\end{teor}

\begin{proof}
Demostraremos las tres condiciones para que sea una martingala. \\
\begin{enumerate}
    \item Note que \[\mathbb{E}\left[ | I_{t} ]\right |=\mathbb{E}\left [  \left | \sum_{j=0}^{k-1}\Delta_{t_j}\left [ B_{t_{j+1}}-B_{t_{j}} \right ]+\Delta_{t_k}\left [ B_t-B_{t_{k}} \right ] \right |\right ]< \infty \]
    Ya que \(\Delta_{t} < \infty\) y positiva y \(\mathbb{E}[\left | B_t \right |]  =0\) al ser MB, por lo tanto la esperanza de su suma finita es finita, y así, se cumple la primera propiedad. 
    \item Notemos que \(I_{t}\) es un función que depende de \(B_t\) que es \(\mathcal{F}_t\)-medible y \(\Delta_{t}\) al ser adaptado es \(\mathcal{F}_t\)-medible también. Por lo tanto \(I_{t}\) es \(\mathcal{F}_t\)-medible
  
    \item Sea \(0\leq s\leq t\leq T\) (\(s\) y \(t\) subintervalos en la partición \(\Pi\)).  Pd \(\mathbb{E}[I_t| \mathcal{F}_s ]=I_s\)\\
     La ecuación de Ito se puede escribir de la siguiente manera:
\end{enumerate}
\begin{align}
I_{t}=&\sum_{j=0}^{l-1}\Delta_{t_j} [ B_{t_{j+1}}-B_{t_{j}}]+\Delta_{t_l}  [ B_{t_{l+1}}-B_{t_{l}}]\label{2.2}  \\ & +\sum_{j=l+1}^{k-1}\Delta_{t_j} [ B_{t_{j+1}}-B_{t_{j}}] +\Delta_{t_k} [ B_{t}-B_{t_{k}}] \label{2.3}\; \end{align} 

Ahora como \(t_l \leq s \), el primer sumando de (\ref{2.2})
\[\sum_{j=0}^{l-1}\Delta_{t_j} [ B_{t_{j+1}}-B_{t_{j}}]\; es \; \mathcal{F}_s-medible\]
\[\Rightarrow \mathbb{E}[\sum_{j=0}^{l-1}\Delta_{t_j}[ B_{t_{j+1}}-B_{t_{j}}] | F_s ] =\sum_{j=0}^{l-1}\Delta_{t_j}[ B_{t_{j+1}}-B_{t_{j}}]\]
Para la segunda parte, como el Movimiento Browniano es una martingala
\begin{align*}
 \mathbb{E}[\Delta_{t_l} [ B_{t_{l+1}}-B_{t_{l}}  | \mathcal{F}_s ] = & \Delta_{t_l}(E[B_{t_{l+1}}| \mathcal{F}_s ] -B_{t_{l}} )\\ 
 =& \Delta_{t_l}(B_s-B_{t_{l}})
 \end{align*}
 Sumando estos últimos resultados, obtenemos lo que buscamos
\begin{align*}
\mathbb{E}[I_{t}| \mathcal{F}_s ]=& \sum_{j=0}^{l-1}\Delta_{t_j}[ B_{t_{j+1}}-B_{t_{j}}]+ \Delta_{t_l}(B_s-B_{t_{l}})\\
                                 =& I_{s}
 \end{align*}
Solo queda por demostrar que los últimos 2 sumandos de (3) son cero.\\
Notemos que \(t_j \geq t_{l+1} > s \), entonces para  \(\sum_{j=l+1}^{k-1}\Delta_{t_j}[ B_{t_{j+1}}-B_{t_{j}}] \) tomamos cada sumando \(\Delta_{t_j} [ B_{t_{j+1}}-B_{t_{j}}]\) y sacamos su esperanza condicional:
\[\mathbb{E}\left [ \Delta_{t_j} ( B_{t_{j+1}}-B_{t_{j}}) \mid \mathcal{F}_s \right ]\]
Por la propiedad de la torre de la esperanza condicional podemos volver a condicionar en una \(F_{t_{j}}\) siempre que \(t_j > s \), entonces
\begin{align*}
&\mathbb{E}\left [ \Delta_{t_j} (B_{t_{j+1}}-B_{t_{j}}) \mid \mathcal{F}_s \right ]    \\
&=\mathbb{E}\left [\mathbb{E}[ \Delta_{t_{j}} (B_{t_{j+1}}-B_{t_{j}}) \mid \mathcal{F}_{t_{j}}] \mid \mathcal{F}_s\right ]\\
&=\mathbb{E}\left [ \Delta_{t_{j}}(\mathbb{E}[ (B_{t_{j+1}}) \mid \mathcal{F}_{t_{j}}] -B_{t_{j}})\mid \mathcal{F}_s\right ]\\
&= \mathbb{E}\left [ \Delta_{t_{j}}(B_{t_{j}}-B_{t_{j}})\mid \mathcal{F}_s\right ]\\
&=0
\end{align*}
Recordando que \(\Delta_{t_{j}}\) y \(B_{t_{j}}\) son \(\mathcal{F}_{t_{j}}\)-medibles y que $B_t$ es una martingala.\\
Así, la esperanza condicional de toda la suma es cero:
\[\mathbb{E}\left [ \sum_{j=l+1}^{k-1}\Delta (t_j)[ B_{t_{j+1}}-B_{t_{j}}] \left\mid\right. \mathcal{F}_s\right ]= 0\]
ya que cada termino de la suma es cero.\\
Para el cuarto y último termino de la suma tenemos:
\begin{align*}
& \mathbb{E}\left \{  \Delta_{t_k}[ B_t-B_{t_{k}}]  \mid \mathcal{F}_s\right \}\\
&= \mathbb{E}\left \{ \mathbb{E}[\Delta_{t_k}[B_t-B_{t_{k}}] \mid \mathcal{F}_{t_{k}}] \mid \mathcal{F}_s\right \}\\
&= \mathbb{E}\left \{\Delta_{t_k} (E[ B_t \mid F_{t_{k}}]-B_{t_{k}}) \mid \mathcal{F}_s\right \}\\
&= \mathbb{E}\left \{\Delta_{t_k}  [B_{t_{k}}-B_{t_{k}}] \mid \mathcal{F}_s\right \}\\
&=0
\end{align*}
\[\therefore \mathbb{E}[I_{t}| \mathcal{F}_{s}]=I_{s}\]
\[\therefore I_{t} \; es\; una \; martingala\]
\end{proof}


\begin{teor} (Isometría de Itô)
La integral de Itô \(I_{t}=\int_{0}^{t}\;\Delta_{u}\;dB_u\) satisface
\[\mathbb{E}\;[I^{2}_{t}]=\mathbb{E}\left[\int_{0}^{t}\Delta^{2}_{u}\;du\right]\]
\end{teor}

\begin{proof}
Por notación escribimos \(D_j=B_{t_{j+1}}-B_{t_{j}}\) para \(j=0,1,...,k-1\) y  \(D_k= B_t-B_{t_{k}}\). Así, podemos escribir la ecuación de Itó de la siguiente forma \(I_{t}=\sum_{j=0}^{k} \Delta_{t_j} D_j\) y
\[I^{2}_{t}=\sum_{j=0}^{k} \Delta^{2}_{t_j} D^2_j\;+2\sum_{0\leq 1<j\leq k} \Delta_{t_i}  \Delta_{t_j} D_iD_j\]

Notemos que para $i<j$ la variable \(\Delta_{t_i}\Delta_{t_j} D_i\) es \(\mathcal{F}_{t_{j}}-medible\), \(D_j\) es independiente de \(\mathcal{F}_{t_{j}}\) y \(\mathbb{E}[D_j]=0\) por lo tanto
\[\mathbb{E}[ \Delta_{t_i} \Delta_{t_j} D_iD_j ]=\mathbb{E}[ \Delta_{t_i} \Delta_{t_j} D_i]\cdot \mathbb{E}[D_j]=0\]

Entonces
\[\mathbb{E}\left[2\sum_{0\leq 1<j\leq k} \Delta_{t_i}  \Delta_{t_j} D_iD_j \right]=0\]
Para los términos \(\Delta^{2}_{t_j} D^2_j\)  la variable \(\Delta^{2}_{t_j}\) es \(\mathcal{F}_{t_{j}}-medible\)
y \(D^2_j\) es independiente de \(\mathcal{F}_{t_{j}}\). Además, como \(B_j\) es Movimiento Browniano para \(j=0,....,k-1\)
\[\mathbb{E}[D^2_j]=\mathbb{E}[(B_{t_{j+1}}-B_{t_{j}})^2]=min\left \{ t_{j+1}-t_j,t_{j+1}-t_j \right \}=t_{j+1}-t_j\]
\[\mathbb{E}[D^2_k]=\mathbb{E}[(B_{t}-B_{t_{k}})^2]=min\left \{ t-t_k,t-t_k \right \}=t-t_k\]
Por lo tanto
\begin{align*}
\mathbb{E}[I^{2}_{t}]&=\sum_{j=0}^{k} \mathbb{E}[\Delta^{2}_{t_j} D^2_j]\\
&= \sum_{j=0}^{k} \mathbb{E}[\Delta^{2}_{t_j}]\cdot \mathbb{E}[D^2_j] \\
&=\sum_{j=0}^{k-1} \mathbb{E}[\Delta^{2}_{t_j}](t_{j+1}-t_j)+
\mathbb{E}[\Delta^{2}_{t_k}](t-t_k)
\end{align*}

Ahora como \(\Delta_{t_j}\) es constante en el intervalo \([t_j,t_{j+1})\) y por lo tanto \(\Delta^{2}_{t_j}(t_{j+1}-t_j)=\int_{t_j}^{t_{j+1}}\Delta^{2}_{u}du\) y de igual forma  \(\Delta^{2}_{t_k}(t-t_k)=\int_{t_k}^{t}\Delta^{2}_{u}du\), con esto, obtenemos para la ecuación anterior:

\begin{align*}
\mathbb{E}[I^{2}_{t}]&=\sum_{j=0}^{k-1} \mathbb{E}\left[\int_{t_j}^{t_{j+1}}\Delta^{2}_{u}du\right]+\mathbb{E}\left[\int_{t_k}^{t}\Delta^{2}_{u}du\right]\\
&=\mathbb{E} \left [ \sum_{j=0}^{k-1} \int_{t_j}^{t_{j+1}}\Delta^{2}_{u}du+ \int_{t_k}^{t}\Delta^{2}_{u}du \right ] \\
&=\mathbb{E}\left[\int_{0}^{t}\Delta^{2}_{u}du\right]    
\end{align*} 
Lo que concluye la demostración. 
\end{proof}


\begin{teor}
La variación cuadrática acumulada hasta el tiempo t por la integral de Itô es
\[[I,I](t)= \int_{0}^{t}\Delta^{2}_{u}du\]
\end{teor}

\begin{proof} \\ \\
Se iniciará analizando la variación cuadrática acumulada en los intervalos \([t_j,t_{j+1}]\), en donde \(\Delta_{u}\) es constante, por ende se elegirá un partición de puntos: 
\[t_j=s_0<s_1<.....<s_m=t_{j+1}\]
y sea 
\begin{align}
\sum_{i=0}^{m-1}[I_{s_{i+1}}-I_{s_i}]^{2} &=\sum_{i=0}^{m-1}[\Delta_{t_j}(B_{s_{i+1}}-B_{s})]^{2}  \\ 
 &= \Delta^{2}_{t_j}\sum_{i=0}^{m-1}(B_{s_{i+1}}-B_{s})^{2}
\end{align}
 Conforme \(m \to \infty \) el \(max_{i=0,..,m-1}(s_{i+1}-s_{i}) \to 0\), y la suma
 \(\sum_{i=0}^{m-1}(B_{s_{i+1}}-B_{s})^{2}\) converge a la variación cuadrática acumulada del Movimiento Browniano entre los tiempos \(t_{j+1}\) y \(t_j\), por lo tanto el límite de la expresión anterior es
 \[\Delta^{2}_{t_j}(t_{j+1}-t_j)=\int_{t_j}^{t_{j+1}}\Delta^{2}_{u}du\]
La igualdad con la integral es debida a que \(\Delta_{u}\) es constante para \(t_j \leq u < t_{j+1}\). De forma análoga, la variación cuadrática acumulada por la integral de Itô entre los tiempos \(t_k\) y \(t\) es
\[\Delta^2(t_j)(t-t_k)=\int_{t_k}^{t}\Delta^{2}_{u}du\]
Sumando las ecuaciones anteriores tenemos que la variación cuadrática acumulada hasta \(t\) es:
\[[I,I](t)= \int_{0}^{t}\Delta^{2}_{u}du\]
\end{proof} 

\textbf{Observación(Notación)}
Las expresiones
\[I_{t}=\int_{0}^{t}\Delta_{u}dB_u\] y
\[dI_{t}=\Delta_{t}dB_t\]
Significan casi lo mismo. La segunda expresión nos indica el cambio en la Integral de Itô es \(\Delta_{t}\) por el cambio en el Movimiento Browniano y si integramos ambos lados obtenemos:
\[I_{t}=I_{0}+\int_{0}^{t}\Delta_{u}dB_u\]
Por lo tanto podemos decir que la segunda es la forma diferencial de la primera, con la diferencia solo de la condición inicial \(I_{0}\).


\subsection{Integral de Itô para integrandos generales}
En esta sección definimos la integral de Itô \( \int_{0}^{T}\Delta_{t}dB_t\) para integrandos \(\Delta_{t}\) que pueden variar continuamente en el tiempo y tener saltos, es decir, ya no se asume que \(\Delta_{t}\) es un proceso simple, seguimos asumiendo que  \(\Delta_{t}\) es adaptado a la filtración \(\mathcal{F}_t\) y también 
\[\mathbb{E}\left[\int_{0}^{T}\Delta^{2}_{t}dt\right] < \infty\]
Como se puede ver en la figura aproximamos \(\Delta_{t}\) mediante procesos simples
\begin{center}
  \includegraphics[scale=.4]{Approximating a continuously varying integrand..png}    
\end{center}

Puede probarse que siempre es posible elegir una sucesion \(\Delta_n(t)\) de procesos simples, los cuales conforme \(n \to \infty \) convergen a la funcion de variación continua  \(\Delta_{t}\), es decir

\[\lim_{n\rightarrow \infty }\mathbb{E}\left[\int_{0}^{T}\left | \Delta _n(t)-\Delta_{t} \right |^{2} dt\right] =0\]
Y en la sección anterior, ya se ha definido la integral de Itó para cada uno de los procesos simples \(\Delta_n(t)\), y para cada \(0 \leq t \leq T\), por ende será posible extender la integral Itó para el integrando continuamente variable  \(\Delta_{t}\) de la siguiente manera
\begin{equation}
    \int_{0}^{t}\Delta_{u}\;dB_u=\lim_{n \to \infty} \int_{0}^{t}\Delta_n(u)\;dB_u,\;\;\; 0\leq t\leq T
\end{equation}
\begin{teor}

Sea \(T\) una constante postiva y \(\Delta_{t}\) un proceso estocástico adaptado con \(0 \leq t\leq T\).Entonces, 
\[\int_{0}^{t}\Delta_{u}\;dB_u\] tiene la siguientes propiedades.
\begin{enumerate}
    \item \textbf{Continuidad} Como función del límite superior de integración de t, los caminos de \(I_{t}\) son continuos.
    \item \textbf{Adaptabilidad} Para todo \(t\), \(I_{t}\) es \(\mathcal{F}_s\)-medible
    \item \textbf{Linealidad} Si \(I_{t}= \int_{0}^{t}\Delta_{u}dB_u\) y \(J_{t}= \int_{0}^{t}\Gamma_{u}dB_u\), entonces
    \[I_{t} \pm  J_t= \int_{0}^{t}(\Delta_{u} \pm \Gamma_{u} )dB_u\] 
    Y además, para toda constante $c$
    \[cI_t= \int_{0}^{t}c\Delta_{u}dB_u\]
    \item \textbf{Martingala} \(I_{t}\) es una martingala
    \item \textbf{Isometría de Itô} \(\mathbb{E}[I^{2}_t]= \mathbb{E}\left[\int_{0}^{t}\Delta^{2}_{u}du\right]\)
    \item \textbf{Variación cuadrática} \([I,I](t)= \int_{0}^{t}\Delta^{2}_{u}du\)
\end{enumerate}
\end{teor}

\textbf{Ejemplo}\\
Calcularemos \(I_{t}= \int_{0}^{t} B_t \;dB_t\), con una \(n\) lo suficientemente grande y aproximamos \(\Delta_{t}=B_t\) por procesos simples
\[\Delta_n(t)=\left\{\begin{matrix}
B_0=0 &  si\; 0\leq t < \frac{T}{n} \\ 
B_{T/n} & si\; \frac{T}{n} \leq t < \frac{2T}{n}\\ 
... & \\ 
B_{(n-1)T/n} & si\; \frac{(n-1)T}{n} \leq t < T & 
\end{matrix}\right.\]
Y por lo tanto \(\lim_{n\rightarrow \infty }\mathbb{E}\left[\int_{0}^{T}\left | \Delta _{n}(t)-B_{t} \right |^{2} dt\right] =0\). Por definición,
\begin{align}
I_{t}= \int_{0}^{t} B_{t} \;dB_t &= \lim_{n \to \infty} \int_{0}^{t} \Delta_n(t) \;dB_{t} \label{2.7}\\ 
 &= \lim_{n \to \infty}\sum_{j=0}^{n-1} B_{jT/n}[B_{(j+1)T/n}-B_{jT/n}] \label{2.8}
\end{align}
 Por notación definimos \(B_j=B_{jT/N}\), y recordemos que \(B_0=0\), entonces tenemos que
 \begin{align*}
 \frac{1}{2}\sum_{j=0}^{n-1}(B_{j+1}-B_{j})^{2}&= \frac{1}{2}\sum_{j=0}^{n-1}B_{j+1}^{2}-\sum_{j=0}^{n-1}B_jB_{j+1}+ \frac{1}{2}\sum_{j=0}^{n-1}B_{j}^{2}\\
 &=\frac{1}{2}\sum_{k=1}^{n}B_{k}^{2}-\sum_{j=0}^{n-1}B_jB_{j+1}+ \frac{1}{2}\sum_{j=0}^{n-1}B_{j}^{2}\\
 &=\frac{1}{2} B_n^2  + \frac{1}{2}\sum_{k=0}^{n-1}B_{k}^{2}-\sum_{j=0}^{n-1}B_jB_{j+1}+ \frac{1}{2}\sum_{j=0}^{n-1}B_{j}^{2}\\
 &=\frac{1}{2} B_n^2  + \sum_{j=0}^{n-1}B_{j}^{2}-\sum_{j=0}^{n-1}B_jB_{j+1} \\
  &=\frac{1}{2} B_n^2  + \sum_{j=0}^{n-1}B_{j}(B_j-B_{j+1})
 \end{align*}  
Por lo tanto concluimos 
\[\sum_{j=0}^{n-1}B_{j}(B_{j+1}-B_{j})=\frac{1}{2} B_n^2-\frac{1}{2}\sum_{j=0}^{n-1}(B_{j+1}-B_{j})^{2}\]
Regresando a la notación original
\[\sum_{j=0}^{n-1}B_{jT/N}(B_{(j+1)T/N}-B_{jT/N})=\frac{1}{2} B_T^2-\frac{1}{2}\sum_{j=0}^{n-1}(B_{(j+1)T/N}-B_{jT/N})^{2}\]
Si en (\ref{2.8}) hacemos que \(n \to \infty\)
\[\int_{0}^{T}B_tdB_t=\frac{1}{2}B_T^2-\frac{1}{2}[B,B](T)=\frac{1}{2}B_T^2-\frac{1}{2}(T)\]
El término extra \(-\frac{1}{2}(T)\) proviene de la variación cuadrática distinta de cero del movimiento browniano y la forma en que construimos la integral Ito, siempre evaluando el integrando en el extremo izquierdo del subintervalo.
Ahora, sea \(g\) una función diferenciable con \(g(0)=0\), entonces
\[\int_{0}^{T}g(t)\;dg(t)=\int_{0}^{T}g(t)\;g'(t)\;dt=\frac{1}{2}g^{2}(t)\mid_0^T=\frac{1}{2}g^{2}(T)\]
Si evaluamos la ecuación desde el punto medio
\[\lim_{n \to \infty}\sum_{j=0}^{n-1} B_{(j+1/2)T/n}[B_{(j+1)T/n}-B_{jT/n}]\]
No tendríamos el término extra \(-\frac{1}{2}(T)\). La integral obtenida al hacer este reemplazo se llama integral de Stratonovich, dicha integral se resuelve en los ejercicios adicionales, y la se le aplican reglas ordinarias de cálculo. Para las funciones \(g(t)\) que tienen una derivada, integrales como \(\int g (t) dg (t) \)
no son sensibles a esta distinción (es decir,las aproximaciones integrales de la integral Itô y Stratonovich tienen el mismo límite, que es \(\frac{1}{2}g^{2}(T)\). Para funciones que tienen una variación cuadrática distinta de cero, las integrales son sensibles a donde en
los subintervalos se evalúan los integrandos aproximados. \\
El límite superior de integración \(T\) puede ser reemplazado para cualquier \( t>0\)
\[\int_{0}^{t}B_udB_u=\frac{1}{2}B_t^2-\frac{1}{2}(t)\]
El teorema anterior garantiza que \(\int_{0}^{t}B_udB_u\) es una martingala y por lo tanto tiene esperanza constante. En  \(t = 0\), esta martingala es 0 y, por lo tanto, su esperanza siempre debe ser cero. Este es el caso porque \(\mathbb{E}(B^2_t)= t\). Si el término \(-\frac{1}{2}(t)\) no esta presente, no tendríamos una martingala
 




\subsection{Fórmula de Itô - Doeblin }
El autor de la bibliografía le da merito también a \textit{Doeblin} sobre la fórmula de Itô al agregarle su nombre, ya que a mediados del año 2000, la Academia Francesa de Ciencias abrió una carta de un soldado francés que tenían guardada desde febrero de 1940, ese soldado era W. Doeblin. En dicha carta, encontraron una construcción de la integral estocástica ligeramente distinta a la de Itô, por ello también se le atribuye el mérito del desarrollo de esta fórmula.


\subsubsection{Fórmula del Movimiento Browniano Geométrico}
Buscamos una regla que nos permita hacer expresiones \textit{diferenciales} de la forma $f(B_{t})$, donde $f(x)$ es una función diferenciable y $B_{t}$ es un Movimiento Browniano. Si $B_{t}$ fuese diferenciable, entonces por regla de la cadena del cálculo diferencial tenemos
\begin{equation*}
    \frac{d}{dt}f(B_{t})=f'(B_{t}) B_{t}^{'}
\end{equation*}
que en notación de diferenciales se puede escribir como
\begin{equation*}
    df(B_{t})=f'(B_{t})B_{t}^{'}dt=f'(B_{t})dB_{t}
\end{equation*}
Dada la variación cuadrática del Movimiento Browniano, la fórmula correcta tiene un término adicional que es 
\begin{equation}\label{secc2.3_forma diferencial}
    df(B_{t})=f'(B_{t})dB_{t}+\frac{1}{2}f^{''}(B_{t})dt
\end{equation}
Esta es la \textit{Fórmula de Itô - Doeblin} en su forma diferencial. Si integramos (\ref{secc2.3_forma diferencial}) obtenemos lo siguiente:
\begin{equation}\label{secc2.3_forma integral}
    f(B_{t})-f(B_{0})=\underbrace{\int_{0}^{t}f'(B_{u})dB_{u}}_{\textit{Integral estocástica}}+\frac{1}{2}\underbrace{\int_{0}^{t}f^{''}(B_{u})du}_{\textit{Integral de Lebesgue}}
\end{equation}
Es importante comentar que (\ref{secc2.3_forma integral}) es una forma muy útil para resolver integrales estocásticas. Por otro lado, para llevar a cabo cálculos a mano, resulta más conveniente utilizar la forma (\ref{secc2.3_forma diferencial}), donde podemos \textit{interpretar} a $df(B_{t})$ como el cambio en $f(B_{t})$ dado un cambio \text{\textquotedblleft infinitesimal \textquotedblright} en \(t\). Sin embargo, (\ref{secc2.3_forma integral}) nos permite darle un significado preciso a (\ref{secc2.3_forma diferencial}).\\

Además, (\ref{secc2.3_forma diferencial}) y (\ref{secc2.3_forma integral}) tienen una relación similar a lo que se desarrolla en el cálculo ordinario para hacer cambios de variable en una integral.\\

\begin{teor}
\textbf{\label{secc2.3_teo1}}\textbf{ (Fórmula de Itô - Doeblin para el Movimiento Browniano)}.\\
Sea $f(t,x)$ una función con derivadas parciales $\frac{\partial }{\partial t}f(t,x)$, $\frac{\partial }{\partial x}f(t,x)$ y $\frac{\partial^{2}}{\partial x}f(t,x)$ definidas y continuas. Y sea $B_{t}$ un Movimiento Browniano. Entonces, para todo $T\geq0$,
\end{teor}

\begin{align*}
    f(T,B_{T})=f&(0,B_{t})+\int_{0}^{T}\frac{\partial }{\partial t}f(t,B_{t})dt\\
                &+\int_{0}^{T}\frac{\partial }{\partial x}f(t,B_{t})dB_{t}+\frac{1}{2}\int_{0}^{T}\frac{\partial^{2}}{\partial x}f(t,x)f(t,B_{t})dt
\end{align*}
\\
\textit{Bosquejo de la demostración.}\\
Lo primero que haremos será ver la heurística de la prueba de (\ref{secc2.3_forma integral}). Para ello, sean $x_{j+1}$, $x_{j}$ números reales. Entonces, al realizar una expansión de Taylor se obtiene:
\begin{equation}\label{secc2.3_taylor1}
    f(x_{j+1})-f(x_{j}) = f'(x_{j})(x_{j+1}-x_{j})+\frac{1}{2}f''(x_{j})(x_{j+1}-x_{j})^{2}+ \cdots
\end{equation}
Sea, $\Pi=\left\{ t_{0},t_{1},...,t_{n} \right\}$ una partición de $[0,T]$ (es decir, $0=t_{0}<t_{1}<\cdots<t_{n}=T$) con $T>0$. Este cambio en $f(B_{t})$ entre los tiempos $t=0$ y $t=T$ se pueden escribir como la suma de los cambios en $f(B_{t})$ sobre cada uno de los intervalos $[t_{j},t_{j+1}]$. Si hacemos esto y usamos (\ref{secc2.3_taylor1}) con $x_{j}=B_{t_{j}}$ y $x_{j+1}=B_{t_{j+1}}$, obtenemos lo siguiente

\begin{align*}
    f(B_{t})-f(B_{0})&=\sum_{j=0}^{n-1}\left [ f(B_{t_{j+1}})-f(B_{t_{j}}) \right ]\\
    &=\sum_{j=0}^{n-1} f'(B_{t_{j}})\left [ B_{t_{j+1}}-B_{t_{j}} \right ]\\ &+ \frac{1}{2}\sum_{j=0}^{n-1} f''(B_{t_{j}})\left [ B_{t_{j+1}}-B_{t_{j}} \right ]^{2}+\frac{1}{6}\sum_{j=0}^{n-1} f'''(B_{t_{j}})\left [ B_{t_{j+1}}-B_{t_{j}} \right ]^{3}+\cdots
\end{align*}
Cuando $\left \| \Pi \right \| \rightarrow 0$, los primeros dos términos convergen a una integral de Lebesgue y a una integral de Itô respectivamente. Todos los limites después de la variación cuadrática convergen a cero, es decir

\begin{align*}
    f(B_{t})-f(B_{0})&= \lim_{\left \| \Pi \right \| \rightarrow 0}\sum_{j=0}^{n-1} f'(B_{t_{j}})\left [ B_{t_{j+1}}-B_{t_{j}} \right ] +\lim_{\left \| \Pi \right \| \rightarrow 0}\frac{1}{2}\sum_{j=0}^{n-1} f''(B_{t_{j}})\left [ B_{t_{j+1}}-B_{t_{j}} \right ]^{2}\\ & \  \ \ \ +\underbrace{\lim_{\left \| \Pi \right \| \rightarrow 0}\frac{1}{6}\sum_{j=0}^{n-1} f'''(B_{t_{j}})\left [ B_{t_{j+1}}-B_{t_{j}} \right ]^{3}}_{0}+\cdots\\
    &=\int_{0}^{T}f'(B_{t})dB_{t}+\frac{1}{2}\int_{0}^{T}f''(B_{t})dt
\end{align*}
Esta es la fórmula de \textit{Itô - Doeblin} en su forma de integral para funciones univariadas $f(x)$, es decir, la fórmula (\ref{secc2.3_forma integral}). Análogamente, para probar (\ref{secc2.3_teo1}) para una función $f(t,x)$, el teorema de Taylor nos dice 
\begin{align*}
    f(t_{j+1},x_{j+1})-f(t_{j},x_{j})&= \frac{\partial}{\partial t}f(t_{j},x_{j})(t_{j+1}-t_{j})+\frac{\partial}{\partial x}f(t_{j},x_{j})(x_{j+1}-x_{j})\\
    & \  \ \ \ + \frac{1}{2} \frac{\partial^2}{\partial x^2}f(t_{j},x_{j})(x_{j+1}-x_{j})^2 + \frac{\partial}{\partial x\partial t}f(t_{j},x_{j})(x_{j+1}-x_{j})(t_{j+1}-t_{j})\\
    & \ \ \ \ + \frac{1}{2} \frac{\partial^2}{\partial t^2}f(t_{j},x_{j})(t_{j+1}-t_{j})^2+\cdots
\end{align*}
Reemplazamos $x_{j}$ por $B_{t_{j}}$, $x_{j+1}$ por $B_{t_{j+1}}$ y sumamos:
\begin{align*}
    f(T,B_{T})-f(0,B_{0})&= \sum_{j=0}^{n-1}\left [ f(t_{j+1},B_{t_{j+1}})-f(t_{j},B_{t_{j}}) \right ]\\
    &=\sum_{j=0}^{n-1}\frac{\partial}{\partial t}f(t_{j},B_{t_{j}})(t_{j+1}-t_{j})+\sum_{j=0}^{n-1}\frac{\partial}{\partial x}f(t_{j},B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})\\
    & \ \ \ \ \ +\frac{1}{2} \sum_{j=0}^{n-1} \frac{\partial^2}{\partial x^2}f(t_{j},B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})^2\\
    & \ \ \ \ \ +  \sum_{j=0}^{n-1} \frac{\partial^2}{\partial t \partial x}f(t_{j},B_{t_{j}})(t_{j+1}-t_{j})(B_{t_{j+1}}-B_{t_{j}})\\
    & \ \ \ \ \ +\frac{1}{2} \sum_{j=0}^{n-1} \frac{\partial^2}{\partial t^2}f(t_{j},B_{t_{j}})(t_{j+1}-t_{j})^2+\cdots
\end{align*}

Analizaremos qué pasa con cada término de la anterior suma si tomamos el límite cuando $\left \| \Pi \right \| \rightarrow 0$. El \textit{primer término} es una integral de Lebesgue:

\begin{equation}\label{sec2.3_término1}
    \lim_{\left \| \Pi \right \| \rightarrow 0} \sum_{j=0}^{n-1}\frac{\partial}{\partial t}f(t_{j},B_{t_{j}})(t_{j+1}-t_{j})=\int_{0}^{T}\frac{\partial}{\partial t}f(t,B_{t})dt
\end{equation}
El límite del \textit{segundo término} es una integral de Itô:
\begin{equation}\label{sec2.3_término2}
    \lim_{\left \| \Pi \right \| \rightarrow 0}\sum_{j=0}^{n-1}\frac{\partial}{\partial x}f(t_{j},B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})=\int_{0}^{T}\frac{\partial}{\partial x}f(t,B_{t})dt
\end{equation}
El límite del \textit{tercer término} es una integral ordinaria (de Lebesgue) de nuevo:

\begin{equation}\label{sec2.3_término3}
    \lim_{\left \| \Pi \right \| \rightarrow 0}\frac{1}{2} \sum_{j=0}^{n-1} \frac{\partial^2}{\partial x^2}f(t_{j},B_{t_{j}})(B_{t_{j+1}}-B_{t_{j}})^2=\frac{1}{2}\int_{0}^{T} \frac{\partial^2}{\partial x^2}f(t,B_{t})dt
\end{equation}
A partir del cuarto término para adelante, los límites son 0. Veremos en específico que pasa con el \textit{cuarto término}:
\begin{align*}
    \lim&_{\left \| \Pi \right \| \rightarrow 0} \left | \sum_{j=0}^{n-1} \frac{\partial^2}{\partial x \partial t}f(t_{j},B_{t_{j}})(t_{j+1}-t_{j})(B_{t_{j+1}}-B_{t_{j}}) \right |\\
    &\leq lim_{\left \| \Pi \right \| \rightarrow 0} \sum_{j=0}^{n-1} \left |  \frac{\partial^2}{\partial x \partial t}f(t_{j},B_{t_{j}}) \right | \cdot (t_{j+1}-t_{j}) \cdot \left |  B_{t_{j+1}}-B_{t_{j}}\right |\\
    &\leq lim_{\left \| \Pi \right \| \rightarrow 0} \max_{0\leqk\leq n-1} \left | B_{t_{k+1}}-B_{t_{k}} \right | \cdot lim_{\left \| \Pi \right \| \rightarrow 0}\sum_{j=0}^{n-1} \left |  \frac{\partial^2}{\partial x \partial t}f(t_{j},B_{t_{j}}) \right | \cdot (t_{j+1}-t_{j})\\
    &=0 \cdot \int_{0}^{T} \left | \frac{\partial^2}{\partial x \partial t}f(t,B_{t}) \right | dt=0
\end{align*}
Análogamente, para el \textit{quinto término}:
\begin{align*}
    \lim&_{\left \| \Pi \right \| \rightarrow 0}\left | \frac{1}{2} \sum_{j=0}^{n-1} \frac{\partial^2}{\partial t^2}f(t_{j},B_{t_{j}})(t_{j+1}-t_{j})^2 \right |\\
    &\leq \lim_{\left \| \Pi \right \| \rightarrow 0} \frac{1}{2}\sum_{j=0}^{n-1}\left | \frac{\partial^2}{\partial t^2}f(t_{j},B_{t_{j}}) \right | \cdot (t_{j+1}-t_{j})^2\\
    &\leq \frac{1}{2} lim_{\left \| \Pi \right \| \rightarrow 0} \max_{0\leqk\leq n-1} (t_{k+1}-t_{k}) \cdot lim_{\left \| \Pi \right \| \rightarrow 0} \sum_{j=0}^{n-1} \left | \frac{\partial^2}{\partial t^2}f(t_{j},B_{t_{j}}) \right | (t_{j+1}-t_{j})\\
    &=\frac{1}{2}\cdot 0 \cdot \int_{0}^{T} \frac{\partial^2}{\partial t^2}f(t,B_{t})dt=0
\end{align*}
 Los términos de orden mayor también resultarán cero. Así, sumando los límites cuando $\left \| \Pi \right \| \rightarrow 0$ de los primeros tres términos obtenemos:
 \begin{equation*}
     f(T,B_{T})=f&(0,B_{t})+\int_{0}^{T}\frac{\partial }{\partial t}f(t,B_{t})dt\\
                &+\int_{0}^{T}\frac{\partial }{\partial x}f(t,B_{t})dB_{t}+\frac{1}{2}\int_{0}^{T}\frac{\partial^{2}}{\partial x^2}f(t,x)f(t,B_{t})dt
 \end{equation*}

\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \blacksquare

\textbf{Observación}. El hecho de que el límite del término que contiene al producto $(t_{j+1})(B_{t_{j+1}}-B_{t_{j}})$, el cuarto término de la suma, se haga cero, se puede reflejar informalmente por la fórmula $dtdB_{t}=0$. Análogamente, la suma que contiene al producto $(t_{j+1}-t_{j})^2$, también tendrá límite cero, este hecho queda reflejado en la fórmula $dtdt=0$. Escribiendo la fórmula de Itô-Doeblin en forma diferencial tenemos

\begin{align*}
    df(t,B_{t})=&\frac{\partial}{\partial t}f(t,B_{t})dt+\frac{\partial}{\partial x}f(t,B_{t})dB_{t}+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,B_{t})dB_{t}dB_{t}\\&+\frac{\partial}{\partial t \partial x}f(t,B_{t})dtdB{t}+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,B_{t})dtdt
\end{align*}
Pero
\begin{align}\label{secc2.33_multdiff}
    dB_{t}dB_{t}=dt,&  &dtdB_{t}=dB_{t}dt=0& &dtdt=0,
\end{align}
Con ello la fórmula de \textit{Itô-Doeblin} en forma diferencial se simplifica a:
\begin{equation}
    df(t,B_{t})=&\frac{\partial}{\partial t}f(t,B_{t})dt+\frac{\partial}{\partial x}f(t,B_{t})dB_{t}+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,B_{t})dt
\end{equation}

\subsubsection{Fórmula de Itô para procesos de Itô}
Extenderemos la fórmula de \textit{Itô - Doeblin} para procesos estocásticos más generales que el Movimiento Browniano, como lo son los procesos de Itô. Casi todo los procesos estocásticos, exceptuando aquellos que tienen saltos, son procesos de Itô.\\

\begin{defi} \label{Secc2.3.2_Def}
 Sea $B_{t}$, $t\geq0$ un Movimiento Browniano con filtración $\mathcal{F}_t$, $t\geq0$. Un \textit{Proceso de Itô} es un proceso estocástico de la forma
\end{defi}

\begin{equation}\label{secc2.3_ProcesosdeIto}
    X_{t}=X_{0}+\int_{0}^{t}\Delta_{s}ds + \int_{0}^{t}\Theta_{s}dB_{s}
\end{equation}

Donde, $X_{0}$ no es aleatoria y $\Delta_{s}$ y $\Theta_{s}$ son procesos estocásticos adaptados. Para poder entender la volatilidad asociada con los procesos de Itô, debemos determinar la razón a la que estos acumulan la variación cuadrática. Con dicho fin en mente, se demostrará el siguiente lema\\ \\

\begin{lema}\label{secc2.3_lema1}
La variación cuadrática del proceso de Itô es:

\begin{equation}\label{secc2.3_variación-cuadrática-Ito}
    \left [ X,X \right ](t)=\int_{0}^{t}\Delta^{2}_{s} ds
\end{equation}
\end{lema} 
\begin{proof}
Introducimos la notación $I_t=\int_{0}^{t}\Delta_{s}dW_{s}$, $R_{t}=\int_{0}^{t}\Theta_{s}ds$. Ambos procesos son continuos en el limite superior de integración $t$. Para determinar la variación cuadrática de $X$ de $[0,t]$, escogemos una partición $\Pi={t_0,t_1,...,t_n}$ de $[0,t]$ y escribimos la variación cuadrática muestral

\begin{align*}
    \sum_{j=0}^{n-1}\left [ X_{t_{j+1}}-X_{t_{j}} \right ]^2=\sum_{j=0}^{n-1}&\left [ I_{t_{j+1}}-I_{t_{j}} \right ]^2+ \sum_{j=0}^{n-1}\left [ R_{t_{j+1}}-R_{t_{j}} \right ]^2\\
    &+2\sum_{j=0}^{n-1}\left [ I_{t_{j+1}}-I_{t_{j}} \right ]\left [ R_{t_{j+1}}-R_{t_{j}} \right ]
\end{align*}
Cuando $\left \| \Pi \right \| \rightarrow 0$, la primer suma del lado derecho de la igualdad, $\sum_{j=0}^{n-1}&\left [ I_{t_{j+1}}-I_{t_{j}} \right ]^2$, converge a la variación cuadrática de $I$ en $[0,t]$, que es $[I,I](t)=\int_{0}^{t}\Delta^{2}(s)$ds. El valor absoluto del segundo término está acotado superiormente por

\begin{align*}
    \max_{0\leq k\leq n-1} &\left | R_{t_{k+1}}-R_{t_{k}} \right | \cdot \sum_{j=0}^{n-1} \left | R_{t_{j+1}}-R_{t_{j}}\right |\\
    &=  \max_{0\leq k\leq n-1} \left | R_{t_{k+1}}-R_{t_{k}} \right | \cdot \sum_{j=0}^{n-1} \left | \int_{t_{j}}^{t_{j+1}} \Theta_{s} ds \right |\\
    &\leq  \max_{0\leq k\leq n-1} \left | R_{t_{k+1}}-R_{t_{k}} \right | \cdot \sum_{j=0}^{n-1} \int_{t_{j}}^{t_{j+1}} \left | \Theta_{s} \right | ds\\
    &=\max_{0\leq k\leq n-1} \left | R_{t_{k+1}}-R_{t_{k}} \right | \cdot \int_{t_{j}}^{t_{j+1}} \left | \Theta_{s} \right | ds,
\end{align*}
Y así, cuando $||\Pi|| \rightarrow 0$, su límite es $0 \cdot \int_{0}^{t} \left | \Theta_{s} \right | ds=0$ ya que $R_{t}$ es un proceso continuo, de este modo el límite del segundo termino también sera cero. Del mismo modo el valor absoluto del tercer término está acotado superiormente por:

\begin{align*}
    2 &\max_{0\leq k\leq n-1}  \left | I_{t_{k+1}}-I_{t_{k}} \right | \cdot \sum_{j=0}^{n-1} \left | R_{t_{j+1}}-R_{t_{j}} \right |\\
    &\leq  2 \max_{0\leq k\leq n-1} \left | I_{t_{k+1}}-I_{t_{k}} \right | \cdot \int_{0}^{t} \left | \Theta_{s} \right | ds,
\end{align*}
y esto tiene límite $0 \cdot \int_{0}^{t} \left | \Theta_{s} \right |^2 ds=0$ sí $||\Pi|| \rightarrow 0$ ya que $I_{t}$ es un proceso estocástico continuo.
\begin{equation*}
    \therefore [X,X](t)=[I,I](t)=\int_{0}^{t}\Delta^{2}_{s}ds
\end{equation*}
\end{proof}
La conclusión de este Lema, se comprende mejor si primero escribimos al \textit{proceso de Itô} (\ref{secc2.3_ProcesosdeIto}) en su forma diferencial

\begin{equation}\label{secc2.3_procesodeItodiff}
    dX_{t}=\Delta_{t}dB_{t}+\Theta_{t}dt
\end{equation}
Y después utilizando las igualdades de multiplicación diferencial (\ref{secc2.33_multdiff}) , calculamos

\begin{equation}\label{secc2.3_procesodeItodiff2}
     dX_{t}dX_{t}&=\Delta^{2}_{t} dB_{t} dB_{t}+2\Delta_{t}\Theta_{t}dB_{t}dt + \Theta_{t}^{2}dt dt=\Delta^{2}_{t}dt
\end{equation}
   

Podemos interpretar esto como que para cada tiempo t, el proceso $X$ está acumulando una variación cuadrática a una razón $\Delta_{t}^{2}$ por unidad de tiempo. Por tanto la variación cuadrática total acumulada en el intervalo de tiempo $[0,t]$ es $[X,X](t)=\int_{0}^{t}\Delta^{2}_sds$. Esta variación cuadrática es consecuencia solamente de la variación cuadrática de la integral de Itô, ya que la integral (de Lebesgue) $R_{t}$ tiene variación cuadrática cero. \\ \\
Nótese que tener variación cuadratica igual a cero, no implica que $R_{t}$ no sea aleatoria, debido a que $\Theta_{s}$ puede ser aleatoria, \(R_t\) puede también serlo. Sin embargo, $R_{t}$ no es tan volatil como $I_{t}$. Para cada tiempo t, tenemos una buena estimación de el siguiente incremento de $R_{t}$. Para pequeños saltos de tiempo $h>0$,
\begin{align*}
    R_{t+h}\approx R_{t}+\Theta_{t}h& &\text{Con $R_{t},\Theta_{t}$ conocidos al tiempo \(t\)}
\end{align*}
Esto es como invertir en una cuenta bancaria con tasa de interés variable. Para cada tiempo, tenemos una buena estimación del rendimiento a corto plazo ya que conocemos la tasa de interés actual. Sin embargo, el rendimiento sigue siendo aleatorio ya que la tasa de interés puede cambiar. ($\Theta$ en esta analogía). En cambio, $I$ es más volatil ya que su estimación dado un cambio en el tiempo $h>0$ es
\begin{equation*}
    I_{t+h}\approx I_{t}+\Delta_{t}(B_{t+h}-B_{t})
\end{equation*}
Sin embargo, $B_{t+h}-B_{t}$ no depende de la información disponible al tiempo \(t\), es más como invertir en acciones.\\ \\

\textbf{Definición \label{secc2.3_def2}\ref{secc2.3_def2}} Sea $X_{t}$, $t\geq0$, un proceso de Itô tal como se define en (\ref{secc2.3_ProcesosdeIto}), y sea $\Gamma_{t},t\geq0$ un proceso adaptado. Definimos a la \textit{integral respecto a un proceso de Itô} como

\begin{equation}\label{secc2.3_IntegralRespectoIto}
    \int_{0}^{t}\Gamma_{s}dX_{s}=\int_{0}^{t}\Gamma_{s}\Delta_{s}dB_{s}+\int_{0}^{t}\Gamma_{s}\Theta_{s}ds
\end{equation}

Trabajaremos de nuevo sobre el bosquejo de la prueba del Teorema (\ref{secc2.3_teo1}) pero en general para procesos de Itô. Analogamente, antes de aplicarle límite cuando $||\Pi||$ tiende a cero, de este modo:
\begin{align*}
    f(T,X_{T})-f(0,X_{0})&= \sum_{j=0}^{n-1}\left [ f(t_{j+1},X_{t_{j+1}})-f(t_{j},X_{t_{j}}) \right ]\\
    &=\sum_{j=0}^{n-1}\frac{\partial}{\partial t}f(t_{j},X_{t_{j}})(t_{j+1}-t_{j})+\sum_{j=0}^{n-1}\frac{\partial}{\partial x}f(t_{j},X_{t_{j}})(X_{t_{j+1}}-X_{t_{j}})\\
    & \ \ \ \ \ +\frac{1}{2} \sum_{j=0}^{n-1} \frac{\partial^2}{\partial x^2}f(t_{j},X_{t_{j}})(X_{t_{j+1}}-X_{t_{j}})^2\\
    & \ \ \ \ \ +  \sum_{j=0}^{n-1} \frac{\partial^2}{\partial t \partial x}f(t_{j},X_{t_{j}})(t_{j+1}-t_{j})(X_{t_{j+1}}-X_{t_{j}})\\
    & \ \ \ \ \ +\frac{1}{2} \sum_{j=0}^{n-1} \frac{\partial^2}{\partial t^2}f(t_{j},X_{t_{j}})(t_{j+1}-t_{j})^2+\cdots
\end{align*}

De manéra análoga a el caso Browniano, los límites conforme $||\Pi|| \rightarrow 0$ después del tercer término también convergen a cero. El límite del \textit{primer término} es
\begin{equation*}
     \int_{0}^{T}\frac{\partial}{\partial t}f(t,X_{t})dt
\end{equation*}
El límite del \textit{segundo término} es
\begin{equation*}
    \int_{0}^{T}\frac{\partial}{\partial x}f(t,X_{t})dX_{t}=\int_{0}^{T}\frac{\partial}{\partial x}f(t,X_{t})\Delta_{t}dB_{t}+\int_{0}^{T}\frac{\partial}{\partial x}f(t,X_{t})\Theta_{t}dt
\end{equation*}
Finalmente, el límite del tercer término es
\begin{equation*}
   \frac{1}{2} \int_{0}^{T}\frac{\partial^2}{\partial x^2}f(t,X_{t})d[X,X](t)=\frac{1}{2} \int_{0}^{T}\frac{\partial^2}{\partial x^2}f(t,X_{t})\Delta_{t}^{2}dt
\end{equation*}
ya que el proceso de Itô $X_{t}$ acumula variación cuadrática a razón $\Delta_{t}^{2}$ por unidad de tiempo por el lema (\ref{secc2.3_lema1}). Estas consideraciones conducen a la siguiente generalización del teorema (\ref{secc2.3_teo1})

\begin{teor}
\textbf{\label{secc2.3_teo2} Fórmula de Itô - Doeblin para procesos de Itô.} Sea $X_{t}$, $t\geq0$, un proceso de Itô (\ref{secc2.3_ProcesosdeIto}) y sea f(t,x) una función con derivadas parciales $\frac{\partial}{\partial t}f(t,x)$,$\frac{\partial}{\partial x}f(t,x)$ y $\frac{\partial}{\partial x^2}f(t,x)$ definidas y continuas. Entonces, para toda $T\geq0$,
\end{teor}

\begin{align*}
    f(T,X_{T})&=f(0,X_{0})+\int_{0}^{T}\frac{\partial}{\partial t}f(t,X_{t})dt+ \int_{0}^{T}\frac{\partial}{\partial x}f(t,X_{t})dX_{t}+\frac{1}{2} \int_{0}^{T}\frac{\partial^2}{\partial x^2}f(t,X_{t})d[X,X](t)\\
    &=f(0,X_{0})+\int_{0}^{T}\frac{\partial}{\partial t}f(t,X_{t})dt+\int_{0}^{T}\frac{\partial}{\partial x}f(t,X_{t})\Delta_{t}dB_{t}+\int_{0}^{T}\frac{\partial}{\partial x}f(t,X_{t})\Theta_{t}dt\\
    &\ \ \ \ \ +\frac{1}{2} \int_{0}^{T}\frac{\partial^2}{\partial x^2}f(t,X_{t})\Delta_{t}^{2}dt
\end{align*}
 Y en notación diferencial:
\begin{equation*}
    df(t,X_{t})=\frac{\partial}{\partial t}f(t,X_{t})dt+\frac{\partial}{\partial x}f(t,X_{t})dX_{t}+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,X_{t})dX_{t}dX_{t}
\end{equation*}
Esta última expresión la podemos reducir a una que implica solamente $dt$ y $dB_{t}$ utilizando (\ref{secc2.3_procesodeItodiff}) y (\ref{secc2.3_procesodeItodiff2}), así obtenemos

\begin{equation}\label{secc2.3_ItoDoeblin.ProcesosIto}
      df(t,X_{t})=\frac{\partial}{\partial t}f(t,X_{t})dt+\frac{\partial}{\partial x}f(t,X_{t})\Delta_{t}dB_{t}+\frac{\partial}{\partial x}f(t,X_{t})\Theta_dt+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,X_{t})\Delta_{t}^{2}dt
\end{equation}
El cálculo de Itô es un poco más que el uso de (\ref{secc2.3_ItoDoeblin.ProcesosIto}) en una variedad de situaciones.


\subsubsection{Ejemplos}
Está sección se concluye con tres ejemplos que ilustran muy bien el uso de la fórmula de Ito-Doeblin en diversas aplicaciones.\\

\textbf{Ejemplo 1. (Movimiento Browniano Geométrico Generealizado.)}  Sea $B_{t}, t \geq 0$,
 un Movimiento Browniano, sea, $\mathcal{F}_{t}, t \geq 0$, su filtración asociada, y sean $\alpha_{t}$ y $\sigma_{t}$ procesos adaptados. Definimos:

\begin{equation*}
    X_t= \int_{0}^{t} \sigma_{s}dB_{s}+\int_{0}^{t}\left ( \alpha_{s}-\frac{1}{2}\sigma^{2}_{s} \right ) ds
\end{equation*}
O en su forma diferencial
\begin{equation*}
     dX_t=\sigma_{t}dB_{t}+\left ( \alpha_{t}-\frac{1}{2}\sigma^{2}_{t} \right )dt
\end{equation*}
y
\begin{equation*}
    dX_{t}dX_{t}=\sigma^{2}_{t}dB_{t}dB_{t}=\sigma^{2}_{t}dt
\end{equation*}
Considerando que el comportamiento del precio de un activo en el tiempo t viene dado por el siguiente proceso estocástico
\begin{equation*}
    S_{t}=S_{0}e^{X_{t}}=S_{0}\exp\left \{ \int_{0}^{t}\sigma_{s}dB_{s}+\int_{0}^{t}\left ( \sigma_{s}-\frac{1}{2}\alpha^{2}_{s} \right )ds \right \},
\end{equation*}
donde $S_{0}$ es el precio del subyacente al tiempo cero (i.e. no es aleatorio) y positivo. Podemos escribir $S_{t}=f(X_{t})$, con $f(x)=S_{0}e^{x}$, $f'(x)=S_{0}e^{x}$. Así, de acuerdo a la formula de Itô-Doeblin
\begin{align*}
    dS_{t}&=df(X_{t})\\
    &=f'(X_{t})dX_{t}+\frac{1}{2}f''(X_{t})dX_{t}dX_{t}\\
    &=S_{0}e^{X_{t}}dX_{t}+\frac{1}{2}S_{0}e^{X_{t}}dX_{t}dX_{t}\\
    &=S_{t}dX_{t}+\frac{1}{2}S_{t}dX_{t}dX_{t}\\
    &=\alpha_{t}S_{t}dt+\sigma_{t}S_{t}dB_{t}
\end{align*}
\begin{equation}\label{secc2.3_StDiff}
    dS_{t}=\alpha_{t}S_{t}dt+\sigma_{t}S_{t}dB_{t}
\end{equation}
El precio del activo $S_{t}$ tiene una media de tasa de rendimiento instantánea $\alpha_{t}$ y volatilidad $\sigma_{t}$, ambas pueden variar en el tiempo y hacerlo aleatoriamente.\\

Este ejemplo incluye a todos los posibles modelos del proceso del precio de un subyacente, que siempre es positivo, no tiene saltos y es influenciado por un solo Movimiento Browniano. A pesar de que el modelo está impulsado por un movimiento browniano, la distribución de $S_{t}$ no necesariamente será log-normal ya que $\alpha_{t}$ y $\sigma_{t}$ pueden variar en el tiempo y hacerlo aleatoriamente. Si hacemos el supuesto de que $\alpha$ y $\sigma$ son constantes, tenemos la expresión usual del Movimiento Browniano Geométrico y su distribución si será log-normal.
\begin{equation}\label{secc2.3_movbrwgeometrico}
    S_{t}=S_{0}exp \left \{\left (\alpha-\frac{1}{2}\sigma^{2}  \right )t+\sigma B_{t}\right \}
\end{equation} 
Este proceso no es una martingala a pesar de que $B_{t}$ sí lo es, esto ya que $S_{0}e^{\sigma B_{t}}$ no es martingala. Se puede comprobar que el proceso $S_{0}e^{\sigma \B_{t}-\frac{1}{2}\sigma^{2}t}$ si es una martingala; y si le agregamos $\alpha t$ en la exponencial, obtenemos $S_{t}$ un proceso con media de rendimiento $\alpha$.\\
La formula de \textit{Itô-Doeblin} refleja estos efectos, aún cuando $\alpha$ y $\sigma$ varían con el tiempo y son aleatorios. Si $\alpha=0$, entonces 
 \begin{equation}
     dS_{t}=\sigma_{t}S_{t}dB_{t}
 \end{equation}
Integrando de ambos lados
\begin{equation*}
    S_{t}=\underbrace{S_{0}}_{constante}+\underbrace{\int_{0}^{t}\sigma_{s}S_{s}dB_{s}}_{\textit{Integral de Itô}}
\end{equation*}
La integral de Itô es una martingala y así (en el caso que $\alpha=0$)

\begin{equation}\label{secc2.3_Stmartingala}
    S_{t}=S_{0}\exp\left \{ \int_{0}^{t}\sigma_{s}dB_{s}-\frac{1}{2}\int_{0}^{t}\sigma^{2}_{s}ds \right \}
\end{equation}
es una martingala. Es decir, $\sigma_{t}S_{t}dB_{t}$ en la parte derecha de la igualdad de (\ref{secc2.3_StDiff}) contribuye solamente la volatilidad, mas no una tendencia para el precio del activo. Cuando $\alpha_{t}$ es un proceso que no es cero, juega el rol de la tasa media de retorno. En el caso de que $\alpha_{t}$ varíe con el tiempo y sea aleatorio, vamos a llamarlo la tasa media \textit{instantánea} de retorno, ya que depende del tiempo en que está siendo evaluada.\\

Este ejemplo nos da la clave para la prueba del siguiente teorema.

\begin{teor}\label{secc2.3_IntItodetermin} \textbf{(Integral de Itô para un integrando determinista)}. Sea $B_{t}$, $s\geq0$,es un movimiento browniano y sea $\Delta_{s}$ es una función del tiempo no aleatoria.Definimos $I(t) = \int_{0}^{t}\Delta_{s}dB_{s}$. Para cada $t\geq0$, la variable aleatoria $I_{t}$ se distribuye normal media cero y varianza $\int_{0}^{t}\Delta^{2}_{s}ds$.
\end{teor}

\begin{proof}
La media y varianza de $I_{t}$ se determinan facilmente.  Como $I_{t}$ es
una martingala e $I_{0} = 0$, debemos tener $\mathbb{E}[I_{t}] =\mathbb{E}[I_{0}] = 0$. Por otro lado la isometría de Itô implica que:
\begin{equation*}
    \mathbb{V}[I_{t}]=\mathbb{E}[I_{t}^{2}]=\int_{0}^{t}\Delta^{2}_{s}ds
\end{equation*}
Como $\Delta_{s}$ no es aleatoria no es necesario también poner la esperanza de la integral del lado derecho de la igualdad. El reto es mostrar que $I_{t}$ se distribuye normal. Esto lo podemos hacer probando que $I_{t}$ tiene la función generadora de momentos de una variable aleatoria normal con media cero y varianza $\int_{0}^{t}\Delta^{2}_{s}ds$; es decir que para cada $u \in \mathbb{R}$ se tiene que:
\begin{equation}
    \mathbb{E}[e^{uI_{t}}]=exp\left \{ \frac{1}{2}u^2\int_{0}^{t}\Delta^{2}_{s}ds \right \} 
\end{equation}
Dado que $\Delta_{s}$ no es aleatoria, esto es equivalente a 
\begin{equation}\label{secc2.3_Iexpectedvalue}
    \mathbb{E}\left [ \exp\left \{ uI_{t}-\frac{1}{2}u^2 \int_{0}^{t}\Delta^{2}_{s}ds \right \} \right ]=\mathbb{E}\left [\exp\left \{\int_{0}^{t}u\Delta_{s}dB_{s}-\frac{1}{2}u^2 \int_{0}^{t}\left ( u\Delta_{s}\right )^2ds \right \} \right ]=1
\end{equation}
Note ahora que el proceso
\begin{equation*}
    exp\left \{\int_{0}^{t}u\Delta_{s}dB_{s}-\frac{1}{2}u^2 \int_{0}^{t}\left ( u\Delta_{s}\right )^2ds \right \} 
\end{equation*}
es una martingala. De hecho, es un Movimiento Browniano Generalizado con tasa media de retorno $\alpha=0$, y con desviación estandar $\sigma_{s}=u\Delta_{s}$, esto puede notarse fácilmente observando (\ref{secc2.3_Stmartingala}). Además, este proceso toma el valor de 1 en $t=0$, por tanto su esperanza siempre es 1. De lo cuál es posible concluir (\ref{secc2.3_Iexpectedvalue}).
\end{proof}

\textbf{Ejemplo 2 (Modelo de tasas de interés de Vasicek.)}  Sea $B_{t}, t \geq 0$, un Movimiento Browniano. El modelo de Vasicek para el proceso de tasa de interés $R_{t}$ es:
\begin{equation}\label{secc2.3_Vasicek}
    dR_{t}=(\alpha-\beta R_{t})dt+\sigma dB_{t}
\end{equation}
Donde $\alpha, \beta$ y $\sigma$ son constantes positivas. La ecuación (\ref{secc2.3_Vasicek}) es un ejemplo de un ecuación diferencial estocástica. Define un proceso aleatorio, $R_t$ en este caso, dando una fórmula para su diferencial, y la fórmula involucra al proceso estocástico en sí mismo y el diferencial de un movimiento browniano.\\
La solución a la ecuación diferencial estocástica (\ref{secc2.3_Vasicek}) se puede determinar en forma cerrada y es:
\begin{equation}\label{secc2.3_Rt}
    R(t)=e^{-\beta t}R_{0}+\frac{\alpha}{\beta}\left ( 1-e^{-\beta t} \right )+\sigma e^{-\beta t}\int_{0}^{t}e^{\beta s} dB_{s}
\end{equation}
En particular, calculamos el diferencial del lado derecho de (\ref{secc2.3_Rt}). Para hacer esto, utilizamos la fórmula de Itô con
\begin{equation*}
    f(t,x)=e^{-\beta t}R_{0}+\frac{\alpha}{\beta}\left ( 1-e^{-\beta t} \right ))+\sigma e^{-\beta t}x
\end{equation*}
y $X_{t}=\int_{0}^{t}e^{\beta s} dB_{s}$. La técnica que estamos utilizando es separar el lado derecho en dos partes: una función ordinaria de dos variables $t$ y $x$, que no tiene aleatoriedad, y un proceso de Itô $ X_{t}$, que contiene toda la aleatoriedad.Para la fórmula de Itô-Doeblin, necesitaremos las siguientes derivadas parciales de $f (t, x)$:

\begin{equation*}
    \frac{\partial}{\partial t}f(t,x)= - \beta e^{-\beta t}R_{0}\alpha e^{-\beta t}-\sigma\beta e^{-\beta t}x=\alpha -\beta f(t,x)
\end{equation*}
\begin{equation*}
     \frac{\partial}{\partial x}f(t,x)=\sigma e^{-\beta t}
\end{equation*}
\begin{equation*}
    \frac{\partial^2}{\partial x^2}f(t,x)=0
\end{equation*}

También necesitaremos el diferencial de $X_{t}$, que es $dX_{t} = e^{\beta t} dB_{t}$. Nosotros no necesitaremos $dX_{t}dX_{t}= e^{2 \beta t} dt $ porque $\frac{\partial^2}{\partial x^2}f(t,x)=0$. La fórmula de Itô establece que:

\begin{align*}
    df(t,X_{t})&=\frac{\partial}{\partial t}f(t,x)dt+\frac{\partial}{\partial x}f(t,x)dX_{t}+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,x)dX_{t}dX_{t}\\
    &=\left ( \alpha-\beta f(t,X_{t}) \right )dt+\sigma dB_{t}
\end{align*}
Esto muestra que $f(t,X_{t})$ satisface la ecuación diferencial (\ref{secc2.3_Vasicek}). Más aun $f(0,X_{0})=R_0$. Por otro lado, el teorema (\ref{secc2.3_IntItodetermin}) dice que la variable aleatoria $\int_{0}^{t}e^{\beta s} dB_{s}$ se distribuye normal con media cero y varianza
\begin{equation*}
    \int_{0}^{t}e^{2 \beta s} ds=\frac{1}{2\beta}(e^{2 \beta t}-1)
\end{equation*}
Por tanto $R_{t}$ se distribuye normal con media $e^{\beta t}R_0 +\frac{\alpha}{\beta}(1-e^{-\beta t})$ y varianza $\frac{\alpha^2}{2 \beta}(1-e^{-2 \beta t})$. En particular, no importa cómo se eligen los parámetros, hay una probabilidad de que $R_{t}$ sea negativo, una propiedad no deseable para un modelo de tasas de interés.\\
El modelo Vasicek tiene la propiedad deseable de que la tasa de interés sea una
reversión a la media. Cuando $R_{t} = \frac{\alpha}{\beta}$, el término  \(dt\) en (\ref{secc2.3_Vasicek}) es igual a cero. Cuando $ R_t>\frac{\alpha}{\beta} $ este término es negativo, lo que acerca a $R_t$ hacia  $\frac{\alpha}{\beta}$. Cuando  $R_t<\frac{\alpha}{\beta}$ este término es positivo, lo que nuevamente empuja a $R_t$ hacia  $\frac{\alpha}{\beta}$. Si $R_0 = \frac{\alpha}{\beta}$ entonces $\mathbb{E}[R_t]=\frac{\alpha}{\beta}$ para todo $t \geq 0$. Si $R_0 \neq \frac{\alpha}{\beta}$ entonces $\lim_{t \rightarrow \infty} \mathbb{E}[R_t]=\frac{\alpha}{\beta}$.\\ 

\textbf{Ejemplo 3. (Modelo de tasas de interés de Cox-Ingersoll-Ross (CIR).)}
Sea $B_t,t \geq 0$ , un movimiento browniano. El modelo Cox-Ingersoll-Ross para tasas de interés de proceso $R_t$es
\begin{equation}\label{CIR}
    dR_t=(\alpha- \beta R_t) dt+ \sigma \sqrt{R_t}dB_t
\end{equation}
donde $\alpha, \beta $ y $\sigma$ son constantes positivas. A diferencia de la ecuación de Vasicek (\ref{secc2.3_Vasicek}),la ecuación CIR  no tiene una solución de forma cerrada. La ventaja sobre el modelo Vasicek es que la tasa de interés en el modelo CIR no se vuelve negativo. Si $R_t$ llega a cero, el término que multiplica $dB_t$ desaparece y el término $ \alpha dt$ en la ecuación (\ref{CIR}) genera tasas de interés positivas de nuevo. Al igual que el modelo Vasicek, el modelo CIR es una reversión a la media.\\
Aunque no se puede obtener una solución de forma cerrada para (\ref{secc2.3_Vasicek}), se puede determinar la distribución de $R_t$ para cada $t$ positiva. Ese cálculo nos tomaría demasiado tiempo. En cambio, nos contentamos con la derivada del valor esperado y la varianza de $ R_t$. Para hacer esto, usamos la función $f (t, x) = e^{\beta t}x$ y la fórmula de Itô-Doeblin para calcular

\begin{align*}
    d(e^{\beta t}R_t)&=df(t,R_t)\\
    &=\frac{\partial}{\partial t}f(t,R_t)+\frac{\partial}{\partial x}f(t,R_t)dR_t+\frac{1}{2}\frac{\partial^2}{\partial x^2}f(t,R_t)dR_t dR_t\\
    &= \alpha e^{\beta t} dt + \sigma e^{\beta t}\sqrt{R_t}dB_t
\end{align*}
Integrando de ambos lados tenemos
\begin{equation*}
    e^{\beta t}R_t=R_{0}+ \frac{\alpha}{\beta} (e^{\beta t}-1)+ \sigma \int_{0}^{t} e^{\beta u}\sqrt{R_t}dB_{u}
\end{equation*}
Utilizando que la esperanza de la integral de Itô es cero, obtenemos
\begin{equation}
    \mathbb{E}[R_{t}]=e^{\beta t}R_0 + \frac{\alpha}{\beta} (e^{\beta t}-1)
\end{equation}
Dicha esperanza coincide con el modelo Vasicek. Para clacular la varianza de $R_t$, definimos $X_t= e^{\beta t} R_t$, para el que ya hemos calculado 
\begin{equation*}
    dX_t=\alpha e^{\beta t} dt + \sigma e^{\beta t}\sqrt{R_t}dB_{t}=\alpha e^{\beta t} dt + \sigma e^{\frac{\beta t}{2}} \sqrt{X_t}dB_t
\end{equation*}
y $\mathbb{E}X_t=R_0+\frac{\alpha}{\beta} (e^{\beta t}-1)$. De acuerdo con la fórmula de Itô (con $f(x)=x^2, f'(x)=2x$ y $f''(x)=2$, 
\begin{align*}
    d(X^2_{t})&=2X_{t}dX_t+dX_{t}dX_{t}\\
    &=2\alpha e^{\beta t}X_{t}dt+2\sigma e^{\frac{\beta t}{2}} X^{\frac{3}{2}}_{t}dB_{t}+ \sigma^2 e^{\beta t} X_{t}dt 
\end{align*}
Si lo integramos llegamos a
\begin{equation*}
    X^2_{t}= X_{0}^{2}+(2\alpha+\alpha^2) \int_{0}^{t}e^{\beta u}X_{u}du+2\sigma e^{\frac{\beta u}{2}} X^{\frac{3}{2}}_{u}dB_u
\end{equation*}
Tomando las esperanzas, y usando el hecho de que la esperanza de una integral de Itô es cero y la fórmula para $ \mathbb{E}[X_t]$, obtenemos

\begin{align*}
    \mathbb{E}[X_{t}^2]&= X_{0}^{2}+(2 \alpha + \sigma^2)\int_{0}^{t}e^{\beta u} \mathbb{E}X_{u}du \\
    &=R_{0}^{2}+(2 \alpha + \sigma^2)\int_{0}^{t}e^{\beta u}(R_0+\frac{\alpha}{\beta} (e^{\beta t}-1))du\\
    &=R_{0}^{2}+\left ( \frac{2 \alpha + \sigma^2}{\beta} \right )\left ( R_0-\frac{\alpha}{\beta} \right )\left ( e^{\beta t}-1 \right )+\frac{2 \alpha + \sigma^2}{2 \beta} \cdot \frac{\alpha}{\beta}(e^{2 \beta t}-1) 
\end{align*}
Entonces,
\begin{align*}
    \mathbb{E}[R_{t}^2]&=e^{-2\beta t}\mathbb{E}X_{t}^{2}\\
    &=e^{-2\beta t}R_{0}^{2}+\frac{2 \alpha + \sigma^2}{\beta}(R_{0}-\frac{\alpha}{\beta})(e^{-\beta t}-e^{-2\beta t})+\frac{ \alpha(2 \alpha + \sigma^2)}{2 \beta^2}(1-e^{-2\beta t})
\end{align*}
Finalmente,
\begin{align*}
    Var(R_t)&=\mathbb{E}[R_{t}^{2}]-(\mathbb{E}[R_t])^2 \\
    &=e^{-2\beta t}R_{0}^{2}+\frac{2 \alpha + \sigma^2}{\beta}\left ( R_{0}-\frac{\alpha}{\beta} \right )\left ( e^{-\beta t}-e^{-2\beta t} \right )\\
    &\ \ \ \ \ +\frac{ \alpha(2 \alpha + \sigma^2)}{2 \beta^2}\left ( 1-e^{-2\beta t} \right )\left ( 1- e^{-2\beta t} \right )-e^{-2\beta t}R_{0}^{2}\\
    &\ \ \ \ \ -\frac{2 \alpha}{\beta}R_0\left ( e^{-\beta t}-e^{-2\beta t} \right )-\frac{\alpha^2}{\beta^2}\left ( 1-e^{-2\beta t} \right )^2\\
    &= \frac{\alpha^2}{\beta}R_{0}\left ( e^{-\beta t}-e^{-2\beta t} \right )+ \frac{\alpha \sigma^2}{2\beta^2}\left ( 1-2e^{-\beta t}+e^{-2\beta t} \right )
\end{align*}
En particular,
\begin{equation*}
    \lim_{t \rightarrow\infty} Var(R_{t})= \frac{\alpha \sigma^2}{2\beta^2}.
\end{equation*}
Estos modelos serán analizados en mayor profundidad en la siguiente sección del trabajo. 

\part{Ejercicios} 
\section{Ecuaciones} 
    Resuelva la ecuación diferencial y calcule la esperanza/varianza y el estimador de máxima verosimilitud de las siguientes ecuaciones diferenciales estocásticas.\\
    Para la resolución de las ecuaciones diferenciales de Vasicek, CIR y Hull-White se utilizara en repetidas veces la siguiente fórmula, la cual se deriva de 
    la formula de Itô para procesos de Itô, sea \(R_t\) un proceso de ito, entonces: 
    \begin{equation}\label{SII.1}
         d\left(e^{\beta t}R_t \right) = e^{\beta t}dR_t + \beta e^{\beta t}R_tdt
    \end{equation}
    la prueba es sencilla y se deja en las siguientes lineas: 
    \begin{proof}
    Tomaremos \(f(t,x) = xe^{\beta t}\) notese que: 
    \begin{align*}
        f_{x}(t,x) &= e^{\beta t}\\
        f_{t}(t,x) &= \beta xe^{\beta t} \\
        f_{xx}(t,x) &= 0
    \end{align*}
    De acuerdo a la formula de \(Itô\) para procesos de \(Itô\) obtenemos la siguiente serie de igualdades: 
    \begin{align*}
         d\left(e^{\beta t}R_t \right) &= df(t,R_t)\\
                                       &= f_{t}(t,R_t)dt + f_{x}(t,R_t)dR_t + \frac{1}{2}f_{xx}(t,R_t)dR_tdR_t \\
                                       &= \beta R_te^{\beta t} + e^{\beta t}dR_t + 0\\
                                       &= \beta R_te^{\beta t} + e^{\beta t}dR_t
    \end{align*}
    Por lo cual es posible concluir (\ref{SII.1}) 
    \end{proof}
   
        %%Parte de rod mierda 
        \subsection{Vasicek} \\
        Sea \(\{B_{t}\}_{t\geq0}\), un movimiento browniano. En el modelo de Vasicek, la tasa de intéres \(R_t\) se supondrá un proceso adaptado a la filtración
        \(\{\mathcal{F}_{t}\}_{t\geq0}\), a la cual el browniano esta adaptado, que sigue la dinámica de mercado descrita por la ecuación diferencial estocástica 
        \begin{equation}\label{SII.V.I}
            dR_t = (\alpha - \beta R_t)dt + \sigma dB_t \ \ \ \alpha,\beta,\sigma >0 \ t \geq 0 
        \end{equation}
        con condicion inicial \(R_0 = r_0\), ya que el rendimiento en el tiempo cero está dado. La solución se puede encontrar de la siguiente manera: 
        \begin{solucion}
        \begin{align*}
            dR_t &= (\alpha - \beta R_t)dt + \sigma dB_t\\
            dR_t +\beta R_t dt &= \alpha dt + \sigma dB_t\\ 
            e^{\beta t}dR_t +\beta e^{\beta t}R_t dt &= \alpha e^{\beta t}dt + \sigma e^{\beta t}dB_t
        \end{align*} 
        En la ú+9+9ltima igualdad se ha multiplicado por la constante \(e^{\beta t}\), observe que por (\ref{SII.1}) se satisface 
        la siguiente igualdad: 
        \begin{align*}
            d\left(e^{\beta t}R_t \right)&=e^{\beta t}dR_t +\beta e^{\beta t}R_t dt = \alpha e^{\beta t}dt + \sigma e^{\beta t}dB_t\\
            d\left(e^{\beta t}R_t \right)&=\alpha e^{\beta t}dt + \sigma e^{\beta t}dB_t
        \end{align*}
        La expresión anterior describe a un proceso  de \(Itô\), y dada la condición inicial \(R_0 = r_0\) obtenemos la solución a dicha expresión como: 
        \begin{align*}
            e^{\beta s}R_s  &=  e^{\beta*(0)}r_0 + \alpha\int_{0}^{s}e^{\beta t}dt + \sigma\int_{0}^{s}e^{\beta t}dB_t \\
             e^{\beta s}R_s  &= r_0 + \frac{\alpha}{\beta}\left(e^{\beta s} - 1\right) + \sigma\int_{0}^{s}e^{\beta t}dB_t
        \end{align*}
         multiplicando ambos lados de la ecuación anterior por \(e^{\beta s}\), se tiene que la solución a (\ref{SII.V.I}) queda dada por:
         \begin{equation}\label{SII.V.2}
             R_s  = r_0e^{-\beta s} + \frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right) + \sigma e^{-\beta s}\int_{0}^{s}e^{\beta t}dB_t
         \end{equation}
     \end{solucion}
    Ahora obtengamos el cálculo de la esperanza y la varianza de nuestro proceso anterior:
    \begin{solucion}
    El teorema (\ref{secc2.3_IntItodetermin}) implica que la variable aleatoria \(\int_{0}^{s}e^{\beta t}dB_t\), se distribuye como \(N\left(0,\int_{0}^{s}e^{2\beta t}dt = \frac{1}{2\beta}
\left(e^{2\beta s} - 1 \right) \right)\), así por (\ref{SII.V.2}) se tiene que: \[R_s\sim N\left( r_0e^{-\beta s} + \frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right),\frac{\sigma^2 e^{-2\beta s}}{2\beta}
\left(e^{2\beta s} - 1 \right)\right)\]
Simplificando un poco los términos anteriores obtenemos: 
\begin{equation*}
    R_s\sim N\left( r_0e^{-\beta s} + \frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right),\frac{\sigma^2}{2\beta}
\left(1 - e^{-2\beta s} \right)\right)
\end{equation*}
es decir, se tiene que:
\begin{align*}
    \mathbb{E}(R_s) &= r_0e^{-\beta s} + \frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right)\\
    \mathbb{V}(R_s) &= \frac{\sigma^2}{2\beta}\left(1 - e^{-2\beta s} \right)
\end{align*}

\subsubsection{Estimadores de máxima verosimilitud para el modelo de Vasicek}

Cabe destacar que, entre los usuarios de este modelo suele ser normal, realizar la estimación de las siguientes funciones de los parámetros: 

\begin{align}\label{SII.VA.3}
    a &= e^{-\beta\delta}\\
    b &= \frac{\alpha}{\beta}\\
    V^2 &= \frac{\sigma^2}{2\beta}\left(1 - e^{-2\beta \delta}\right)
\end{align}

donde \(\delta\) denota el lapso de tiempo entre las obsevaciones \(R_1,R_2,\hdots,R_n\) de la v.a. \(R\), típicamente \(\delta = 1\) día. Los estimadores de máxima verosimilitud para \(a,b\) y \(V^2\) están dados por: 
\begin{align}\label{SII.VA.4}
    \hat{a} &= \frac{(n-1)\sum_{i=2}^{n}R_{i}R_{i-1} - \sum_{i=2}^{n}R_i\sum_{i=2}^{n}R_{i-1}}{(n-1)\sum_{i=2}^{n}R_{i}^2 - \left(\sum_{i=2}^{n}R_{i-1}\right)^2}\\
    \hat{b} &= \frac{\sum_{i=2}^{n}\left[R_i - \hat{a}R_{i-1}\right]}{(n-1)(1-\hat{a})}\\
    \hat{V^2} &= \frac{1}{n-1}\sum_{i=2}^{n}\left[R_i - \hat{a}R_{i-1} -\hat{b}(1-\hat{a}) \right]
\end{align}
La demostración de este hecho utiliza las densidades de transición del proceso, y por ende se encuentra fuera del alcance de los conocimientos actuales, pero puede ser consulata, con una parametrización distinta, en el siguiente artículo K. Fergusson and E. Platen (p. 18) 'Application of maximum likelihood estimation to stochastic short rate models'.
     \end{solucion}
     
        \subsection{Modelo Cox-Ingersoll-Ross (CIR) para tasas de interés}\\
        Sea \(\{B_{t}\}_{t\geq0}\), un movimiento browniano. En el modelo CIR, la tasa de intéres \(R_t\) se supondrá un proceso adaptado a la filtración
        \(\{\mathcal{F}_{t}\}_{t\geq0}\),a la cual el browniano esta adaptado, que sigue la dinámica de mercado descrita por la ecuación diferencial estocástica 
        \begin{equation}\label{SII.CIR.1}
            dR_t = (\alpha - \beta R_t)dt + \sigma\sqrt{R_t}dB_t \ \ \ \alpha,\beta,\sigma >0 \ t \geq 0 
        \end{equation}        
        Tristemente, y contrario a lo que ocurre en el caso del modelo de Vasicek, la ecuacion (\ref{SII.CIR.1}) del modelo CIR, no posee una solución explicita, aunque cuenta con otras ventajas ante dicha ecuación, entre ellas se tiene que la tasa de interes en el modelo CIR nunca se vuelve negativa.
        Por otro lado, aunque no es posible obtener una solución explicita para (\ref{SII.CIR.1}), si que es posible determinar la distribución de la variable aleatoria
        \(R_t\) la demostración de este resultado esta fuera del alcance de este trabajo, y puede ser consultada en Mark, Ioffe (p. 1) "Probability Distribution of Cox-Ingersoll-Ross Model", donde se obtiene que: 
        \[
        R_{t + s} | R_t = r_t \sim \frac{X}{2\gamma} \ \ \forall s \geq 0 
        \]
        donde \(\gamma = \frac{2\beta}{(1 - e^{-\beta s})\sigma^2}\), y \(X\) es una v.a. \(\chi^2\) no centrada con \(\frac{4\alpha}{\sigma^2}\) grados de libertad
        y parametro de descentralización \(2\gamma r_t e^{-\beta s}\). 
        Como dicha demostración quedo fuera del alcance de este trabajo, se dara una 'solución' a la ecuación diferencial estocástica (\ref{SII.CIR.1}), que servira 
        para derivar las formulas para la esperanza y la varianza del proceso \(R_t\).
        \begin{solucion}
             \begin{align*}
                 dR_t &= (\alpha - \beta R_t)dt + \sigma\sqrt{R_t}dB_t\\
                 dR_t + \beta R_tdt &= \alpha dt + \sigma\sqrt{R_t}dB_t\\
                 e^{\beta t}dR_t +\beta e^{\beta t}R_t dt &= \alpha e^{\beta t}dt + \sigma e^{\beta t}\sqrt{R_t}dB_t
             \end{align*}
        En la ultima igualdad se ha multiplicado por la constante \(e^{\beta t}\), observe que por (\ref{SII.1}) se satisface 
        la siguiente igualdad: 
        \begin{align*}
            d\left(e^{\beta t}R_t \right)&=e^{\beta t}dR_t +\beta e^{\beta t}R_t dt = \alpha e^{\beta t}dt + \sigma e^{\beta t}\sqrt{R_t}dB_t\\
        \end{align*}
        O equivalentemente 
        \begin{equation}\label{SII.CIR.2}
             d\left(e^{\beta t}R_t \right)=\alpha e^{\beta t}dt + \sigma e^{\beta t}\sqrt{R_t}dB_t
        \end{equation}
        De la ecuacion (\ref{SII.CIR.2}) obtenemos las siguientes ecuacione: 
        \begin{itemize}
            \item La esperanza de \(R_t\) es igual \(r_0e^{-\beta s} + \frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right)\).\\ En efecto, obteniendo 
            la forma integral de la ecuación (\ref{SII.CIR.2}), se tiene lo siguiente: 
            \begin{align*}
                e^{\beta s}R_s &= r_0 + \alpha\int_{0}^{s}e^{\beta t}dt + \sigma\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t\\
                               &= r_0 +\frac{\alpha}{\beta}\left(e^{\beta s} - 1 \right) + \sigma\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t
            \end{align*}
            Así, hemos obtenido que: 
            \begin{equation}\label{SII.CIR.3}
                e^{\beta s}R_s =  r_0 +\frac{\alpha}{\beta}\left(e^{\beta s} - 1 \right) + \sigma\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t
            \end{equation}
            O equivalentemente, que: 
            \begin{equation}\label{SII.CIR.4}
                R_s =  r_0e^{-\beta s} +\frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right) + \sigma\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t
            \end{equation}
            La ecuación anterior representa a la cuasisolución mencionada con anterioridad, la cual será de utilidad para el calculo de la esperanza
            de \(R_s\), note que lo anterior no es una solución explícita de la ecuación en diferencias del modelo CIR, debido a que el lado 
            derecho de la igualdad anterior depende del proceso \(\{R_t\}_{t\geq 0}\). Pero como se calcula a partir de la ecuación anterior la esperanza, primeramente
            observe que, \(\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t\) es una integral de Ito, y por lo tanto \(\mathbb{E}\left( \int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t\right) = 0\), así pues tomando esperanzas en (\ref{SII.CIR.4}) se obtiene:
            \begin{align*}
                \mathbb{E}( R_s) &= \mathbb{E}\left( r_0e^{-\beta s} +\frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right) + \sigma\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t\right)\\
                                 &= r_0e^{-\beta s} +\frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right)  + \sigma\mathbb{E}\left( \int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t\right)\\
                                 &= r_0e^{-\beta s} +\frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right) + 0
            \end{align*}
            Por tanto: 
            \[\mathbb{E}( R_s) = r_0e^{-\beta s} +\frac{\alpha}{\beta}\left(1 - e^{-\beta s} \right)\]
            Como dato curioso, observe que dicha esperanza es igual a la esperanza para la tasa de intéres en el modelo de Vasicek. 
            \item Sea \(X_s = e^{\beta s}R_s\), entonces 
            \[\mathbb{E}(X_{s}^2) = r_{0}^2 + \frac{2\alpha + \sigma^2}{\beta}\left(r_0 - \frac{\alpha}{\beta}\right)\left(e^{\beta t} -1\right) +
           \frac{2\alpha + \sigma^2}{2\beta}\frac{\alpha}{\beta}\left(e^{\beta t} -1\right)\]   
           Por (\ref{SII.CIR.2}) se tiene que: 
           \begin{equation}\label{SII.CIR.5}
               dX_t = \alpha e^{\beta t}dt + \sigma e^{\frac{\beta t}{2}}\sqrt{X_t}dB_t
           \end{equation}
           Y por (\ref{SII.CIR.3}) sabemos que 
           \begin{equation}\label{SII.CIR.6}
               X_s =  r_0 +\frac{\alpha}{\beta}\left(e^{\beta s} - 1 \right) + \sigma\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t
           \end{equation}
           Y ¿esto para qué? Note primero que gracias a (\ref{SII.CIR.6}) es posible obtener una fórmula para la esperanza del proceso, una vez mas
           gracias  a que \(\int_{0}^{s}e^{\beta t}\sqrt{R_t}dB_t\) es una integral de Itô, y por cálculos semejantes a los del inciso anterior obtenemos
           que: 
           \begin{equation}\label{SII.CIR.7}
               \mathbb{E}(X_s) =  r_0 +\frac{\alpha}{\beta}\left(e^{\beta s} - 1 \right)
           \end{equation}
           Por otro lado, sea \(g(t,x) = x^2\) entonces: 
           \begin{align*}
               g_{x}(t,x) &= 2x\\
               g_{t}(t,x) &= 0\\
               g_{xx}(t,x)&= 2 
           \end{align*}
           Y nuevamente por (\ref{SII.CIR.6}), es facil notar que el proceso \(\{X_t\}_{t\geq 0}\) es de Itô, así pues aplicando la formula de Itô, para este
           tipo de procesos se tiene que: 
           \begin{align*}
               dX_{t}^{2} &= dg(t,X_t)\\
                          &= g_{t}(t,X_t)dt + g_{x}(t,X_t)dX_t + \frac{1}{2}g_{xx}(t,X_t)\\
                          &= 2X_tdX_t + dX_tdX_t \\
                          &= *
           \end{align*}
           Por (\ref{SII.CIR.5}) y recordando que la variación cuadrática de un proceso de Itô, es igual al cuadrado de la función que multiplica al diferencial del 
           browniano por el diferencia del tiempo la cadena de igualdades anterior puede continuarse como: 
           \begin{align*}
                  * &=2X_t\left(\alpha e^{\beta t}dt + \sigma e^{\frac{\beta t}{2}}\sqrt{X_t}dB_t\right) + 2\sigma e^{\beta t}X_tdt\\
                    &=2\alpha e^{\beta t}X_tdt + 2\sigma e^{\frac{\beta t}{2}}X_{t}^{\frac{3}{2}}dB_t + \sigma^2e^{\beta t}X_tdt
           \end{align*}
           De este modo, obtenemos la siguiente ecuación diferencial estocástica: 
           \begin{equation*}
               dX_{t}^{2} = (2\alpha + \sigma^2 )e^{\beta t}X_tdt + 2\sigma e^{\frac{\beta t}{2}}X_{t}^{\frac{3}{2}}dB_t 
           \end{equation*}
           Una vez más dicha ecuación corresponde a un proceso de Itô, que puede ser obtenido integrando de ambos lados:
           \begin{equation*}
               X_{s}^2 = X_{0}^2 + (2\alpha + \sigma^2 )\int_{0}^{s}e^{\beta t}X_tdt +  2\sigma\int_{0}^{s}e^{\frac{\beta t}{2}}X_{t}^{\frac{3}{2}}dB_t 
           \end{equation*}
           Ahora es posible obtener la esperanza de \(X_{s}^2\), haciendo previamente la observación de que \(\int_{0}^{s}e^{\frac{\beta t}{2}}X_{t}^{\frac{3}{2}}dB_t \)
           es una integral de Itô, la cual tendrá esperanza nula: 
           \begin{align*}
               \mathbb{E}(X_{s}^2) &= \mathbb{E}\left( X_{0}^2 + (2\alpha + \sigma^2 )\int_{0}^{s}e^{\beta t}X_tdt +  2\sigma\int_{0}^{s}e^{\frac{\beta t}{2}}X_{t}^{\frac{3}{2}}dB_t  \right) \\
                                   &= r_{0}^2 + (2\alpha + \sigma^2 )\mathbb{E}\left(\int_{0}^{s}e^{\beta t}X_tdt\right) + 2\sigma\mathbb{E}\left(\int_{0}^{s}e^{\frac{\beta t}{2}}X_{t}^{\frac{3}{2}}dB_t\right)\\      
                                   &=r_{0}^2 + (2\alpha + \sigma^2 )\mathbb{E}\left(\int_{0}^{s}e^{\beta t}X_tdt\right) + 0\\
                                   &= *
           \end{align*}
           Por fubini se tiene que \(\mathbb{E}\left(\int_{0}^{s}e^{\beta t}X_tdt\right) = \int_{0}^{s}e^{\beta t}\mathbb{E}(X_t)dt\), y utilizando la fórmula que 
           fue obtenida en (\ref{SII.CIR.7}), es posible seguir la cadena de igualdades anterior de la siguiente manera: 
           \begin{align*}
               * &= r_{0}^2 + \int_{0}^{s}e^{\beta t}\mathbb{E}(X_t)dt\\
                 &= r_{0}^2 + \int_{0}^{s}e^{\beta t}\left(r_0 +\frac{\alpha}{\beta}\left(e^{\beta t} - 1 \right) \right)dt\\
                 &= r_{0}^2 + \frac{2\alpha + \sigma^2}{\beta}\left(r_0 - \frac{\alpha}{\beta}\right)\left(e^{\beta t} -1\right) +
           \frac{2\alpha + \sigma^2}{2\beta}\frac{\alpha}{\beta}\left(e^{\beta t} -1\right)
           \end{align*}
           Por lo tanto: 
           \begin{equation}\label{SII.CIR.8}
              \mathbb{E}(X_{s}^2) = r_{0}^2 + \frac{2\alpha + \sigma^2}{\beta}\left(r_0 - \frac{\alpha}{\beta}\right)\left(e^{\beta t} -1\right) +
           \frac{2\alpha + \sigma^2}{2\beta}\frac{\alpha}{\beta}\left(e^{\beta t} -1\right)
           \end{equation}
        
        \item Finalmente se tendra que \(\mathbb{V}(R_s) = \frac{\sigma^2}{\beta}r_0\left(e^{-\beta s} - e^{-2\beta s} \right) + \frac{\alpha\sigma^2}{2\beta^2}
             \left(1 - e^{-2\beta s}\right)\)\\
            Por (\ref{SII.CIR.8}) y dado que \(X_s = e^{\beta s}R_s\) se tiene que 
            \begin{align*}
            \mathbb{E}(R_{s}^2) &= e^{-2\beta s}\mathbb{E}(X_{s}^2)\\
                                &= r_{0}^2e^{-2\beta s} + e^{-2\beta s}\frac{2\alpha + \sigma^2}{\beta}\left(r_0 - \frac{\alpha}{\beta}\right)\left(e^{\beta t} -1\right)\\
                                &+ e^{-2\beta s} \frac{2\alpha + \sigma^2}{2\beta}\frac{\alpha}{\beta}\left(e^{\beta t} -1\right)
            \end{align*}
        Dado lo anterior y (\ref{SII.CIR.7}), es posible concluir que:
        \begin{align*}
        \mathbb{V}(R_s) &= \mathbb{E}(R_{s}^2) - (\mathbb{E}(R_{s}))^2 \\
                        &= r_{0}^2e^{-2\beta s} + e^{-2\beta s}\frac{2\alpha + \sigma^2}{\beta}\left(r_0 - \frac{\alpha}{\beta}\right)\left(e^{\beta s} -1\right)\\
                        &+ e^{-2\beta s} \frac{2\alpha + \sigma^2}{2\beta}\frac{\alpha}{\beta}\left(e^{\beta s} -1\right)\\
                        &-\left( r_0 +\frac{\alpha}{\beta}\left(e^{\beta s} - 1 \right)\right)^2\\
                        &=e^{-2\beta s}r_{0}^2+\frac{2\alpha + \sigma^2}{\beta}\left(r_0 - \frac{\alpha}{\beta}\right)\left(e^{-\beta s} - e^{-2 \beta s}\right)\\ 
                        &+\frac{\alpha(2\alpha + \sigma^2)}{2\beta^2}(1 - e^{-2\beta s})- e^{-2\beta s}r_{0}^2\\
                        &- \frac{2\alpha}{\beta}r_0\left(e^{-\beta s} - e^{-2\beta s}\right) - \frac{\alpha^2}{\beta^2}\left(1-e^{-2 \beta s}\right)\\
                        &= \frac{\sigma^2}{\beta}r_0\left(e^{-\beta s} - e^{-2\beta s} \right) + \frac{\alpha\sigma^2}{2\beta^2}
        \end{align*}
        \end{itemize}
        \end{solucion}
        Por último para obtener los estimadores de máxima verosimilitud primeramente se dará una metodología para obtener los estimadores por mínimos cuadrados ordinarios, para los parámetros, la idea tras este método es obtener aproximaciones de los parámetros \(\alpha,\beta\) y \(\sigma\), considerando una versión discretizada del modelo \(CIR\), la cual es similar a la fórmula de discretización de euler utilizada para la simulación de ecuaciones diferenciales estocásticas:
        \begin{align*}
             R_{t_{i+1}} - R_{t_i} &= (\alpha - \beta R_{t_i})\delta_{t_i}  + \sigma\sqrt{R_{t_i}}\delta B_i\\
                                   &= \beta\left(\frac{\alpha}{\beta} -   R_{t_i}\right)\delta_{t_i}  + \sigma\sqrt{R_{t_i}}\delta B_i
        \end{align*}
        Sea \(\mu = \frac{\alpha}{\beta}\), entonces: 
        \begin{equation*}
              R_{t_{i+1}} - R_{t_i} = \beta\left(\mu -   R_{t_i}\right)\delta_{t_i} + \sigma\sqrt{R_{t_i}}\delta B_i
        \end{equation*}
        Donde \(i = 1,2,3,...,n-1\), \(\delta_{t_i}  = t_{i+1} - t_{i}\) y \(\delta B_i = B_{t_{i+1}} - B_{t_i}\). Por otro lado, \(R_{t_1},R_{t_2},\hdots,R_{t_n}\) serán observaciones de la variable aleatoria \(R\), la tasa de interés a modelar, para obtener los estimadores por mínimos cuadrados ordinarios, primeramente, dividiremos la ecuación anterior por \(\sqrt{R_{t_i}}\), para obtener: 
        \begin{align*}
            \frac{ R_{t_{i+1}} - R_{t_i}}{\sqrt{R_{t_i}}} &= \frac{\beta}{\sqrt{R_{t_i}}}(\mu - R_{t_i})\delta_{t_i} +  \sigma\delta B_i\\
                                                          &=  \beta\mu\frac{\delta_{t_i}}{\sqrt{R_{t_i}}} - \beta\sqrt{R_{t_i}}\delta_{t_i} + \sigma\delta B_i
        \end{align*}
        Es decir se tiene que: 
        \begin{equation}\label{SII.CIR.8}
            \frac{ R_{t_{i+1}} - R_{t_i}}{\sqrt{R_{t_i}}} = \beta\mu\frac{\delta_{t_i}}{\sqrt{R_{t_i}}} - \beta\sqrt{R_{t_i}}\delta_{t_i} + \sigma\delta B_i
        \end{equation}
        
        Sea ahora: 
        \begin{align*}
            y_{i} &= \frac{R_{t_{i + 1}} - R_{t_i}}{\sqrt{R_{t_i}}}\\
            \beta_1 &= \beta\mu\\
            \beta_2 &= -\beta\\
            z_{1i} &= \frac{\delta_{t_i}}{\sqrt{R_{t_i}}}\\
            z_{2i} &=  \delta_{t_i}\sqrt{R_{t_i}}\\
            \varepsilon_{i} &= \sigma\delta B_i
        \end{align*}
        Entonces será posible escribir a (\ref{SII.CIR.8}) como: 
        \[y_i = \beta_1z_{1i} + \beta_2z_{2i} + \varepsilon_{i} \ \ \ i\in\{1,\hdots,n-1\}\]
        O en notación matricial como: 
        \begin{equation}\label{SII.CIR.9}
            Y = Z\bar{\beta} + \varepsilon
        \end{equation}
        Donde:
        \begin{align*}
        Y &= \begin{bmatrix}
            y_1\\ 
            y_2\\ 
            \vdots\\
            y_{n-1} 
            \end{bmatrix} &Z = \begin{bmatrix}
            \frac{\delta_{t_1}}{\sqrt{R_{t_1}}} & \delta_{t_1}\sqrt{R_{t_1}}\\ 
            \frac{\delta_{t_2}}{\sqrt{R_{t_2}}} & \delta_{t_2}\sqrt{R_{t_2}}\\ 
            \vdots & \vdots\\ 
            \frac{\delta_{t_{n-1}}}{\sqrt{R_{t_{n-1}}}} & \delta_{t_{n-1}}\sqrt{R_{t_{n-1}}}
            \end{bmatrix}\\
        \bar{\beta} &= \begin{bmatrix}
            \beta_1\\ 
            \beta_2
       \end{bmatrix} &\varepsilon = \sigma\begin{bmatrix}
            \delta_1N_{1}(0,1)\\ 
            \delta_2N_{2}(0,1)\\ 
            \vdots\\ 
            \delta_{n-1}N_{n-1}(0,1)
            \end{bmatrix}
        \end{align*}
    Para la simplificación del módelo anterior, usualmente se asume que las observaciones \(R_{t_i}\) se distribuyen de manera uniforme en el tiempo, es decir \(\delta = \delta_{t_i} = \delta_{t_j} \ \ \forall{i \neq j}\), lo cual es un supuesto bastante razonable ya que los historicos de tasas regularmente se publican en lapsos homogeneos, e.j. por días o semanas. Dado este supuesto las matrices anteriores cobran la siguiente forma:
        \begin{align*}
        Y &= \begin{bmatrix}
            y_1\\ 
            y_2\\ 
            \vdots\\
            y_{n-1} 
            \end{bmatrix} &Z = \begin{bmatrix}
            \frac{\delta}{\sqrt{R_{t_1}}} & \delta\sqrt{R_{t_1}}\\ 
            \frac{\delta}{\sqrt{R_{t_2}}} & \delta\sqrt{R_{t_2}}\\ 
            \vdots & \vdots\\ 
            \frac{\delta}{\sqrt{R_{t_{n-1}}}} & \delta\sqrt{R_{t_{n-1}}}
            \end{bmatrix}\\
        \bar{\beta} &= \begin{bmatrix}
            \beta_1\\ 
            \beta_2
       \end{bmatrix} &\varepsilon = \sigma\delta\begin{bmatrix}
            N_{1}(0,1)\\ 
            N_{2}(0,1)\\ 
            \vdots\\ 
            N_{n-1}(0,1)
            \end{bmatrix}
        \end{align*}
    Las matrices anteriores, en conjunto con la ecuación (\ref{SII.CIR.9}), definen un modelo de regresión lineal heterocedástico, por lo cual es posible utilizar la técnica de los mínimos cuadrados ordinarios para estimar los parámetros de la regresión, así:
    \[\hat{\bar{\beta}} = \begin{bmatrix}
            \hat{\beta_1}\\ 
            \hat{\beta_2}
       \end{bmatrix} = \left(Z^{t}Z\right)Z^{t}Y\] 
    De este modo tenemos que: 
    \begin{equation*}
        \hat{\beta} = -\hat{\beta_2}
    \end{equation*}
    \begin{equation}\label{SII.CIR.10}
        \hat{\mu} = \frac{\hat{\beta_1}}{\hat{\beta}} 
    \end{equation}
    Por último \(\sigma\) se estimará utilizando la varianza residual del modelo, ya que:
    \[\Hat{\sigma^2\delta} = \frac{1}{n}\left \|Y -Z\hat{\bar{\beta}}  \right \|^2\]
    lo cual implica que: 
    \begin{equation}\label{SII.CIR.11}
        \hat{\sigma}=\frac{1}{\sqrt{\delta n}}\left\|Y -Z\hat{\bar{\beta}}  \right \|^2
    \end{equation}
    Con (\ref{SII.CIR.10}) y (\ref{SII.CIR.11}), hemos encontrado una semilla inicial para el método que se implementará a continuación para realizar el cálculo de los estimadores de máxima verosimilitud.
    \subsubsection{Máxima Verosimilitud para el modelo CIR} 
    Puede probarse que si \(2\alpha = \frac{2\beta}{\mu} > \sigma^2\), entonces la densidad de transición de \(R_t\) dado \(R_s\) es:
    \begin{equation}\label{SII.CIR.12}
     p(R_t|R_s,\theta) = ce^{-(u + v)}\left(\frac{v}{u}\right)^{\frac{q}{2}}I_{q}(2\sqrt{uv})    
    \end{equation}
    donde:
    \begin{align*}
        \delta_s &= t - s\\
        c &= \frac{\beta}{\sigma^2\left(1 - e^{-\beta\delta_s}\right)}\\
        u  &= cR_se^{-\beta \delta_s}\\
        v  &= cR_t\\
        q  &= \frac{2\beta \mu}{\sigma^2} - 1\\
        \theta &= (\beta,\mu = \frac{\alpha}{\beta},\sigma)
    \end{align*}
    E \(I_{q}\) es la función de Bessel modificada de orden \(q\), la cual puede ser definida como: 
    \[I_{q}= \sum_{j = 0}^{\infty}\left(\frac{x}{2}\right)^{2j + q}\frac{1}{k!\Gamma(j +q +1)}\]
    Esto último nos sirve para lo siguiente, dado nuestro conjunto de observaciones \(\{R_{t_1},\hdots,R_{t_{n}}\}\), la función de versimilutud descrita por la densidad conjunta de nuestras observaciones se podra escribir de la siguiente manera:
    \[L(\theta) = p\left(R_{t_1}\right)\Pi_{i=1}^{n-1}p\left(R_{t_{i+1}}|R_{t_i},\theta \right)\]
    donde \( p\left(R_{t_1}\right)\) es la densidad inicial del proceso, la cual por simplicidad supondremos no depende de \(\theta\). Como es usual en este tipo de problemas será más fácil trabajar con la función de log verosimilitud, en lugar de con la función de verosimilitud directamente, ya que el lograritomo convertira productos en sumas, operaciones que consumen menor tiempo en un entorno computacional, así: 
    \[ln(\theta) = ln\left(p\left(R_{t_1}\right)\right) + \sum_{i = 1}^{n}ln\left(p\left(R_{t_{i+1}}|R_{t_i},\theta \right)\right)\]
     Sí suponemos como previamente mencionamos, que las observaciones \(R_{t_i}\) están uniformemente distribuidads en el tiempo, i.e. \(\delta = \delta_{t_i} = \delta_{t_j}\), por (\ref{SII.CIR.12}) se obtiene la siguiente igualdad: 
    \begin{equation}\label{SII.CIR.13}
                 ln(\theta) = ln\left(p\left(R_{t_1}\right)\right) + ln\left(c\right) +    \sum_{i=1}^{n}\left[-u_{t_i} - v_{t_{i+1}} + \frac{qv_{t_{i+1}}}{2u_{t_i}}  ln\left(I_{q}(2\sqrt{u_{t_i}v_{t_{i+1}}})\right)\right] 
    \end{equation}
    \(u_{t_i} = cR_{t-i}e^{-\beta \delta}\), \(v_{t_{i+1}} = cR_{t_{i+1}}\), \(c = \frac{\beta}{\sigma^2\left(1 - e^{-\beta\delta}\right)}\) y \(q\) se define igual a como lo hicimos anteriormente. Ahora el objetivo es intentar maximizar la función de log-verosimitud, ya que nuestra estimación estara dada por:
    \[\hat{\theta} = \max_{\theta}ln(L(\theta))\]
    Estó ultimo se puede hacer en varios lenguajes de programación, pero se debe tener en cuenta el siguiente hecho, ya que puede generar problemas numéricos, \(((I_{q}(2\sqrt{u_{t_i}v_{t_{i+1}}}\) tiende rapidamente hacia infinito cuando \(u_{t_i}v_{t_{i+1}} \to \infty\), lo cual por la definicion de \(u_{t_i}\) y \(v_{t_{i+1}}\), ocurre cuando \(\sigma^2 \to 0\), es decir cuando la tasa tiene una volatilidad cercana a cero como en los CETES, por esta razón es recomendable reescalar exponencialmente la función de bessel, esto quiere decir que en vez de usar \(I_{q}(x)\) en (\ref{SII.CIR.13}), se utilice: 
    \[I_{q}(x)e^{-x}\]
    Esto reduce los problemas de diveregencia, ya que cuando el término \(I_{q}(x)\), explota para \(x\) suficientemente grandes, 
    el factor \(e^{-x}\) tiende a cero y reduce este fenómeno, por lo cual (\ref{SII.CIR.13}) se convierte en: 
    \begin{equation*}
         ln(\theta) = ln\left(p\left(R_{t_1}\right)\right) + ln\left(c\right) +    \sum_{i=1}^{n}\left[-u_{t_i} - v_{t_{i+1}} + \frac{qv_{t_{i+1}}}{2u_{t_i}}  ln\left(I_{q}(2\sqrt{u_{t_i}v_{t_{i+1}}})\right) + \sqrt{2u_{t_i}v_{t_{i+1}}}\right] 
    \end{equation*}
    Esta última es la función a maximizar. 
    
        \subsection{Modelo de Hull-White}
        Sea \(\{B_{t}\}_{t\geq0}\), un movimiento browniano. En el modelo de Hull-White, la tasa de interés \(R_t\) se supondrá un proceso adaptado a la filtración
        \(\{\mathcal{F}_{t}\}_{t\geq0}\) , a la cual el browniano esta adaptado, que sigue la dinámica de mercado descrita por la ecuación diferencial estocástica 
        \begin{equation}\label{SII.HW.1}
            dR_t = (\theta_t - \beta R_t)dt + \sigma dB_t \ \ \ \beta,\sigma >0 \ t \geq 0 
        \end{equation}
        \(\alpha_t > 0\) una función determinística, i.e, que depende solo del tiempo y no tiene un componente aleatorio, talque \(\int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt < \infty\), con condicion inicial \(R_0 = r_0\), ya que el rendimiento en el tiempo cero está dado. La solución a la ecuación anterior se puede encontrar de la siguiente manera:   
        \begin{solucion}
        \begin{align*}
            dR_t &= (\theta_t - \beta R_t)dt + \sigma dB_t\\
            dR_t +\beta R_t dt &= \theta_t dt + \sigma dB_t\\ 
            e^{\beta t}dR_t +\beta e^{\beta t}R_t dt &= \theta_t e^{\beta t}dt + \sigma e^{\beta t}dB_t
        \end{align*} 
        En la ultima igualdad se ha multiplicado por la constante \(e^{\beta t}\), observe que por (\ref{SII.1}) se satisface 
        la siguiente igualdad: 
        \begin{align*}
            d\left(e^{\beta t}R_t \right)&=e^{\beta t}dR_t +\beta e^{\beta t}R_t dt = \theta_t e^{\beta t}dt + \sigma e^{\beta t}dB_t\\
            d\left(e^{\beta t}R_t \right)&=\theta_t e^{\beta t}dt + \sigma e^{\beta t}dB_t
        \end{align*}
        La expresión anterior describe a un proceso  de Itô, y dada la condición inicial \(R_0 = r_0\) obtenemos la solución a dicha expresión como: 
        \begin{align*}
            e^{\beta s}R_s  &=  e^{\beta*(0)}r_0 + \int_{0}^{s}\theta_{t}e^{\beta t}dt + \sigma\int_{0}^{s}e^{\beta t}dB_t \\
            e^{\beta s}R_s  &= r_0 + \int_{0}^{s}\theta_{t}e^{\beta t}dt  + \sigma\int_{0}^{s}e^{\beta t}dB_t
        \end{align*}
         multiplicando ambos lados de la ecuación anterior por \(e^{\beta s}\), se tiene que la solución a (\ref{SII.HW.1}) queda dada por:
         \begin{equation}\label{SII.HW.2}
             R_s  = r_0e^{-\beta s} + \int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt + \sigma e^{-\beta s}\int_{0}^{s}e^{\beta t}dB_t
         \end{equation}
        \end{solucion}
         Ahora se obtendra la varianza y la esperanza del proceso \(R_s\) :
    \begin{solucion}
    Una vez más el teorema (\ref{secc2.3_IntItodetermin}) implica que la variable aleatoria \(\int_{0}^{s}e^{\beta t}dB_t\), se distribuye como
    \(N\left(0,\int_{0}^{s}e^{2\beta t}dt = \frac{1}{2\beta} \left(e^{2\beta s} - 1 \right) \right)\), así por (\ref{SII.HW.2}) se tiene que: 
    \[R_s\sim N\left(  r_0e^{-\beta s} + \int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt,\frac{\sigma^2 e^{-2\beta s}}{2\beta}
    \left(e^{2\beta s} - 1 \right)\right)\]
    Simplificando un poco los términos anteriores obtenemos: 
    \begin{equation*}
        R_s\sim N\left(  r_0e^{-\beta s} + \int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt,\frac{\sigma^2}{2\beta}
    \left(1 - e^{-2\beta s} \right)\right)
    \end{equation*}
    es decir se tiene que:
    \begin{align*}
        \mathbb{E}(R_s) &=  r_0e^{-\beta s} + \int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt\\
        \mathbb{V}(R_s) &= \frac{\sigma^2}{2\beta}\left(1 - e^{-2\beta s} \right)
    \end{align*}
    \end{solucion}
    
    \subsubsection{Estimadores de máxima verosimilitud para el modelo Hull-White}
    Es posible demostrar que la función \(\theta_t\) está totalmente determinada por la siguiente expresión: 
    \begin{equation}\label{SII.HW.3}
        \theta_t = \frac{\partial f^{M}(0,t)}{\partial t} + \beta f^{M}(0,t) + \frac{\sigma^2}{2\beta}\left(1 - e^{-2\beta t} \right)
    \end{equation}
    Donde \(f^{M}(0,t)\) es la tasa forward con tenor continuo en el tiempo 0 a la madurez t, la cual puede ser expreada como:
    \[
       f^{M}(0,t) = -\frac{\partial lnP^{M}(0,t)}{\partial t}
    \]
    Y \(P^{M}(0,t)\) es el precio de mercado de un bono cupón cero con Madurez t. Por otra parte, retomando (\ref{SII.HW.2}) tenemos que: 
    \[
       R_s  = r_0e^{-\beta s} + \int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt + \sigma e^{-\beta s}\int_{0}^{s}e^{\beta t}dB_t
    \]
    Resolvamos la integral de en medio con la información adquirida: 
    \begin{align*}
        \int_{0}^{s}\theta_{t}e^{\beta(t-s)}dt &=  \int_{0}^{s}\frac{\partial}{\partial t}\left(e^{\beta(t-s)}f^{M}(0,t)\right)dt
        +\frac{\sigma^2}{2\beta}e^{\beta(t-s)}\\
        &= f^{M}(0,s) - e^{-\beta s}f^{M}(0,0) + e^{-\beta s}\frac{\sigma^2}{2\beta}\left[\int_{0}^{s}\left(e^{\beta t}- e^{-\beta t} \right) \right]\\
        &= f^{M}(0,s) - e^{-\beta s}(0) + \frac{\sigma^2}{2\beta^2}(1 - e^{-\beta s})^{2}\\
        &= f^{M}(0,s) + \frac{\sigma^2}{2\beta^2}(1 - e^{-\beta s})^{2}\\
        &= \alpha_{s}
    \end{align*}
    Observe que: 
    \begin{equation}\label{SII.HW.4}
     \alpha_0 = \int_{0}^{0}\theta_{t}e^{\beta(t-s)}dt = 0
    \end{equation}
    Entonces por (\ref{SII.HW.3}) y por la ecuacion anterior, obtenemos la siguiente igualdad: 
    \begin{equation*}
        R_s  = r_0e^{-\beta s} + \alpha_{s} + \sigma e^{-\beta s}\int_{0}^{s}e^{\beta t}dB_t
    \end{equation*}
    Ahora observe lo siguiente: 
    \begin{equation}\label{SII.HW.5}
        R_s - \alpha_{s} = r_0e^{-\beta s} + \sigma e^{-\beta s}\int_{0}^{s}e^{\beta t}dB_t
    \end{equation}
    Sea 
    \begin{equation}\label{SII.HW.6}
        X_s = R_s - \alpha_{s} \ \ \ s \geq 0
    \end{equation}
    entonces \(X_s\) es la solución a la ecuación diferencial estocástica: 
    \[
         dS_t = -\beta S_tdt + \sigma dB_t \ \ \ s_0 = r_0
    \]
    Ya que, está es la ecuación de un modelo de Vasicek sin regresion a la media, y en la sección anterior se vio que la solución a esta ecuación está dada por: 
    \[
       S_s =r_0e^{-\beta s} + \sigma\int_{0}^{s}e^{\beta(t-s)}dB_t
    \]
    Condición que por (\ref{SII.HW.4}), (\ref{SII.HW.5}) y (\ref{SII.HW.6}), el proceso \(X_s\) satisface, por tanto es posible estimar
    los parámetros \(\beta, \sigma\), utilizando la estimación de máxima verosimilitud del modelo de Vacicek (\ref{SII.VA.4}), al modelo descrito por la ecuación: 
    \[
       dX_t = -\beta X_tdt + \sigma dB_t \ \ \ X_0 = r_0
    \]
    Así por (\ref{SII.VA.3}) y (\ref{SII.VA.4})  se tiene como estimaciones de máxima verosimilitud para \(\beta\) y \(\sigma\), lo siguiente: 
    \begin{align*}
        \hat{\beta} &= -\frac{ln(\hat{a})}{\delta}\\
        \hat{\sigma} &= \frac{2\hat{\beta}\hat{V^2}}{1 - e^{-2\hat{\beta}\delta}} 
    \end{align*}
    donde \(\delta\), es el tiempo que transcure entre las observaciones del proceso, usualmente \(\delta = 1\) día, y la definicion 
    de \(\hat{a}\) y \(\hat{V^2}\) se puede encontrar en la sección de estimacion de máxima verosimilitud para el modelo de Vasicek, incluida en este PDF.
    
        \subsection{Ecuación de Black-Scholes-Merton}

        A continuación resolveremos la ecuación diferencial de Black \& Scholes, la cuál se deduce del modelo de Black \& Scholes.\\
Sea $V_t = V(S_t, t)$ el valor de una opción \textit{Call Europeo} sobre el subyacente $S$ en el tiempo $t$. Entonces
\begin{equation}
    \label{eq:bs}
    \frac{\partial V}{\partial t} + \frac{1}{2} \sigma^2 S^2 \frac{\partial^2 V}{\partial S^2} + r S \frac{\partial V}{\partial S} - rV = 0
\end{equation}
con $V(0, t) = 0$, $V(S,t) \sim S$ cuando $S \to \infty$. Además si $T$ es la madurez del derivado $$V(S,T) = \max(S-K, 0)$$ 
Notemos que $\frac{\partial V}{\partial t}$, $\frac{\partial V}{\partial S}$ y $\frac{\partial V^2}{\partial S^2}$ están abreviadas por conveniencia para simplificar la notación, éstas representan a los procesos $\frac{\partial V}{\partial t} (S, t)$, $\frac{\partial V}{\partial S}(S, t)$ y $\frac{\partial^2 V}{\partial S^2}(S, t)$ respectivamente.\\
Es importante recalcar que cualquier derivado cuyo precio depende solo del valor actual de $S$ y $t$ debe satisfacer la ecuación de Black \& Scholes.
\begin{solucion}
    Haremos uso de $\tau$ para la variable de tiempo (en lugar de la $t$ más natural) y así evitar un conflicto de notación en los cambios de variables que tendremos que hacer.
    Sean $t = T - \frac{\tau}{(1/2)\sigma^2}$, $S = Ke^x$ y pongamos 
    $$ V(S,t) = Kv(x,\tau)$$
    Recordemos que $\sigma$ es la volatilidad, $r$ es la tasa de interés de un bono libre de riesgo, y $K$ es el \textit{Strike}.\\
    En las variables definidas inicialmente, la elección de $t$ invierte el sentido del tiempo, cambiando el problema de parabólico hacia atrás a parabólico hacia delante. La elección de $S$ es una transformación bien conocida basada en la experiencia con la ecuación equidimensional de Euler en ecuaciones diferenciales. Además, las variables se han escalado cuidadosamente para hacer que la ecuación transformada se exprese en cantidades adimensionales.\\
    % Aquí iria lo del consejo
    \noindent
    Siguiendo el consejo, escribamos ahora $$ \tau = \frac{\sigma^2}{2} \cdot (T - t)$$ y $$ x = \ln{\frac{S}{K}}.$$
    Por otra parte, las primeras derivadas de $V$ son
    \begin{equation}
        \label{eq:dVt}
        \frac{\partial V}{\partial t} = K \cdot \frac{\partial v}{\partial \tau} \cdot \frac{\partial \tau}{\partial t} = K \cdot \frac{\partial v}{\partial \tau} \cdot \frac{-\sigma^2}{2}
    \end{equation}
    y
    \begin{equation}
        \label{eq:dVS}
        \frac{\partial V}{\partial S} = K \cdot \frac{\partial v}{\partial x} \cdot \frac{\partial x}{\partial S} = K \cdot \frac{\partial v}{\partial x} \cdot \frac{1}{S}.
    \end{equation}
    Y la segunda derivada es
    \begin{align*}
        \frac{\partial ^2 V}{\partial S^2} &= \frac{\partial}{\partial S} \left( \frac{\partial V}{\partial S}\right)\\
        &= \frac{\partial}{\partial S} \left( K \frac{\partial v}{\partial x} \frac{1}{S} \right)\\
        &= K \frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + K \frac{\partial}{\partial S} \left( \frac{\partial v}{\partial x}\right) \cdot \frac{1}{S}\\
        &= K \frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + K \frac{\partial}{\partial x} \left( \frac{\partial v}{\partial x}\right) \cdot \frac{dx}{dS} \cdot \frac{1}{S}\\
        &= K \frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + K \frac{\partial^2 v}{\partial x^2} \cdot \frac{1}{S^2}.
    \end{align*}
    Es decir
    \begin{equation}
        \label{eq:ddVS}
        \frac{\partial ^2 V}{\partial S^2} = K \frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + K \frac{\partial^2 v}{\partial x^2} \cdot \frac{1}{S^2}
    \end{equation}
    Ahora, la condición terminal es
    $$ V(S,T) = \max (S-K, 0) = \max (Ke^x - K, 0) $$
    pero $V(S,T) = Kv(x, 0)$, así $v(x, 0) = \max(e^x - 1, 0)$.\\
    Ahora sustituyendo todas las derivadas en la ecuación de Black \& Scholes, es decir, en \eqref{eq:bs}, obtenemos:
    \begin{equation}
        \label{eq:bs1}
        K\frac{\partial v}{\partial \tau} \cdot \frac{-\sigma^2}{2} + \frac{\sigma^2}{2} S^2 \left(K\frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + K\frac{\partial^2 v}{\partial x^2} \cdot \frac{1}{S^2} \right) + rS \left(K\frac{\partial v}{\partial x} \cdot \frac{1}{S} \right) - rKv = 0.
    \end{equation}
    Esta expresión puede simplificarse de la siguiente forma:
    \begin{equation*}
        K\frac{\partial v}{\partial \tau} \cdot \frac{\sigma^2}{2} = \frac{\sigma^2}{2} S^2 \left(K\frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + K\frac{\partial^2 v}{\partial x^2} \cdot \frac{1}{S^2} \right) + rS \left(K\frac{\partial v}{\partial x} \cdot \frac{1}{S} \right) - rKv
    \end{equation*}
    Multiplicando ambos lados por $\dfrac{1}{K} \cdot \dfrac{2}{\sigma^2}$
    \begin{align*}
        \frac{\partial v}{\partial \tau} &= S^2 \left(\frac{\partial v}{\partial x} \cdot \frac{-1}{S^2} + \frac{\partial^2 v}{\partial x^2} \cdot \frac{1}{S^2} \right) + \frac{2}{\sigma^2} \cdot rS \left(\frac{\partial v}{\partial x} \cdot \frac{1}{S} \right) - \frac{2}{\sigma^2} \cdot rv\\
        &= \left(\frac{-\partial v}{\partial x} + \frac{\partial^2 v}{\partial x^2} \right) + \frac{2}{\sigma^2} \cdot r \left(\frac{\partial v}{\partial x}\right) - \frac{2}{\sigma^2} \cdot rv\\
        &= \frac{\partial^2 v}{\partial x^2} + \left(\frac{\partial v}{\partial x}\right) \left( \lambda - 1 \right) - \lambda \cdot v 
    \end{align*}
    Por lo tanto \eqref{eq:bs1} puede escribirse de la siguiente forma
    \begin{equation}
        \label{eq:bs2}
        \frac{\partial v}{\partial \tau} = \frac{\partial^2 v}{\partial x^2} + \left(\frac{\partial v}{\partial x}\right) (\lambda - 1) - \lambda \cdot v
    \end{equation}
    Donde $\lambda = \dfrac{2}{\sigma^2} \cdot r$.
    Notemos que ahora \eqref{eq:bs2} tiene coeficientes constantes.
    También es importante recalcar que solo hay un parámetro adimensional $\alpha$ que mide la tasa de interés libre de riesgo como un múltiplo de la volatilidad y un tiempo 'reescalado' hasta el vencimiento $\sigma T$, no las 4 cantidades dimensionadas originales $K$, $T$, $\sigma ^2$ y $r$.\\
    Además \eqref{eq:bs2} esta definida en el intervalo $ -\infty < x < \infty$, pues para cualquier valor de $x$ se tiene que $ 0 < S < \infty$ por $ S = Ke^x $.\\
    Hasta este punto ya sería posible resolver la ecuación de B\&S de forma directa, pero simplificaremos aún más cambiando la escala de la variable dependiente una vez más, por
    \begin{equation}
        \label{eq:v}
        v = e^{\alpha x + \beta \tau} u(x,\tau)
    \end{equation}
    donde $\alpha$ y $\beta$ aún no están determinadas. Ahora, derivando \eqref{eq:v} respecto a $\tau$ se tiene
    \begin{equation}
        \label{eq:dvt}
        v_\tau = \beta e^{\alpha x + \beta \tau} u + e^{\alpha x + \beta \tau} u_\tau
    \end{equation}
    {Nota:} El subíndice $\tau$ se refiere a la derivada parcial respecto a $\tau$.\\
    Y de igual forma
    \begin{equation}
        \label{eq:dvx}
        v_x = \alpha e^{\alpha x + \beta \tau} u + e^{\alpha x + \beta \tau} u_x
    \end{equation}
    Y finalmente
    \begin{equation}
        \label{eq:ddvx}
        v_{xx} = \alpha^2 e^{\alpha x + \beta \tau} u + 2 \alpha e^{\alpha x + \beta \tau} u_x + e^{\alpha x + \beta \tau} u_{xx}.
    \end{equation}
    Sustituyendo estas derivadas en \eqref{eq:bs2} y dividiendo entre el factor común $e^{\alpha x + \beta \tau}$ obtenemos
    \begin{equation*}
        \beta u + u_\tau = \alpha^2 u + 2 \alpha u_x + u_{xx} + (\lambda - 1)(\alpha u + u_x) - \lambda u.
    \end{equation*}
    Juntando términos semejantes
    \begin{equation*}
        u_\tau = u_{xx} + [2\alpha + (\lambda - 1)]u_x + [\alpha^2 + (\lambda - 1)\alpha - \lambda - \beta]u.
    \end{equation*}
    Tomemos $\alpha = -\frac{\lambda - 1}{2}$ de tal forma que el coeficiente de $u_x$ sea 0, y de igual manera tomemos $ \beta = \alpha^2 + (\lambda - 1)\alpha - \lambda = - \frac{(\lambda + 1)^2}{4}$ tal que el coeficiente de $u$ también sea 0. Con esta elección, la ecuación se reduce a
    \begin{equation}
        \label{eq:c1}
        u_\tau = u_{xx}
    \end{equation}
    Necesitamos transformar la condición inicial también y para esto hacemos lo siguiente
    \begin{align*}
        u_0 (x) = u(x,0) &= e^{-(-\frac{\lambda - 1}{2})x -(-\frac{(\lambda + 1)^2}{4})\cdot 0} v (x,0)\\
        &= e^{\left(\frac{\lambda - 1}{2}\right)x} \max (e^x -1, 0)\\ &= \max\left(e^{\left(\frac{\lambda + 1}{2}\right)x} - e^{\left(\frac{\lambda -1 }{2}\right)x}, 0\right).
    \end{align*}
    Por lo que
    \begin{equation}
        \label{eq:c2}
        u(x,0) = u_0(x) = \max\left(e^{\left(\frac{\lambda + 1}{2}\right)x} - e^{\left(\frac{\lambda -1 }{2}\right)x}, 0\right) = g(x)
    \end{equation}
    Notemos que como $ \lambda > 0 $ entonces $ \left(\frac{\lambda + 1}{2}\right) x > \left(\frac{\lambda - 1}{2}\right) x$ por tanto $ u(x,0) > 0 $ para toda $ x > 0$, y por otra parte $ u(x,0) = 0 $ cuando $ x \leq 0 $.\\
    Antes de continuar es necesario introducir una ecuación diferencial muy famosa que será la piedra angular para resolver la ecuación de B\&S. Se trata de la \textbf{\textit{ecuación de calor}} o \textbf{\textit{ecuación de difusión}}, en $\mathbb{R}$ esta dada por
    \begin{equation*}
    \begin{cases}
        u_\tau = u_{xx}& \ \ \mbox{para}\ \ x \in \mathbb{R}, \tau > 0 \\
        u(x,0) = g(x).
    \end{cases}
    \end{equation*}
    Haciendo uso de la transformada de Fourier, se sabe que la solución de la ecuación de difusión es
    \begin{equation*}
        u(x,\tau) = \frac{1}{2\sqrt{\pi \tau}}\int_{\mathbb{R}} g(y) e^{-\frac{(x-y)^2}{4\tau}} dy \ \ \ \mbox{para} \ \ \ -\infty < x < \infty, \ \tau > 0 
    \end{equation*}
     Retomando... dado el resultado anterior es posible aplicar la fórmula de la solución a la \textbf{\textit{ecuación de difusión}}, pues justo acabamos de obtener por \eqref{eq:c1} y \eqref{eq:c2} que
    \begin{equation*}
    \begin{cases}
        u_\tau = u_{xx}\\
        u(x,0) = g(x).
    \end{cases}
    \end{equation*}    
    Así
    \begin{equation}
        \label{eq:hf}
        u(x,\tau) = \frac{1}{2\sqrt{\pi \tau}}\int_{-\infty}^\infty u_0(s) e^{-\frac{(x-s)^2}{4\tau}} ds.
    \end{equation}
    Sin embargo, hagamos primero el cambio de variable 
    $$ z = \frac{s-x}{\sqrt{2 \tau}} $$
    $$ dz = \frac{1}{\sqrt{2 \tau}} ds. $$
    Así
    \begin{equation}
        \label{eq:I}
        u(x,\tau) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} u_0 \left( z\sqrt{2\tau} + x \right) e^{-\frac{z^2}{2}} dz.
    \end{equation}
    Esta integral solo la resolveremos donde $u_0 > 0$, es decir, cuando $z > -\frac{x}{\sqrt{2\tau}}$. Sobre ese dominio se tiene que
    $$u_0 = e^{\frac{\lambda + 1}{2} ( x + z\sqrt{2 \tau})} - e^{\frac{\lambda - 1}{2} ( x + z\sqrt{2 \tau})} $$
    Por lo que \eqref{eq:I} queda de la siguiente forma
    \begin{equation*}
        u(x, \tau) = \frac{1}{\sqrt{2 \pi}} \int_{-x/\sqrt{2 \tau}}^{\infty} e^{\frac{\lambda + 1}{2} ( x + z\sqrt{2 \tau})} e^{-\frac{z^2}{2}} dz - \frac{1}{\sqrt{2 \pi}} \int_{-x/\sqrt{2 \tau}}^{\infty} e^{\frac{\lambda - 1}{2} ( x + z\sqrt{2 \tau})} e^{-\frac{z^2}{2}} dz
    \end{equation*}
    Llamemos a estas integrales $I_1$ e $I_2$ respectivamente.\\
    Resolvamos primero $I_1$. Para esto fijémonos en el exponente de la exponencial
    $$ \frac{\lambda + 1}{2} ( x + z\sqrt{2 \tau}) - \frac{z^2}{2} $$
    Completando el cuadrado
    \begin{align*}
        \frac{\lambda + 1}{2} ( x + z\sqrt{2 \tau}) - \frac{z^2}{2} &= \left(\frac{-1}{2} \right) \left( z^2 - \sqrt{2\tau} (\lambda + 1) z \right) + \left(\frac{\lambda +1 }{2} \right) x\\
        &= \left(\frac{-1}{2} \right) \left( z^2 - \sqrt{2\tau} (\lambda + 1) z + \tau \frac{(\lambda + 1)^2}{2} \right) + \left(\frac{\lambda +1 }{2} \right) x + \tau \frac{(\lambda +1)^2}{4}\\
        &= \left(\frac{-1}{2}\right) \left(z - \sqrt{\tau/2} (\lambda + 1) \right)^2 + \left(\frac{\lambda +1 }{2} \right) x + \tau \frac{(\lambda +1)^2}{4}
    \end{align*}
    Por lo tanto
    \begin{equation*}
        I_1 = \frac{e^{\left(\frac{\lambda +1 }{2} \right) x + \tau \frac{(\lambda +1)^2}{4}}}{\sqrt{2\pi}} \int_{-x/\sqrt{2\tau}}^\infty e^{ -\frac{1}{2} \left(z - \sqrt{\tau/2} (\lambda + 1) \right)^2 } dz
    \end{equation*}
    Ahora, haciendo un nuevo cambio de variable
    $$ y = z - \sqrt{\tau/2} (\lambda + 1) $$
    $$ dy = dz $$
    Además
        $$ -x/\sqrt{2\tau} &< z < \infty$$
        $$ -x/\sqrt{2\tau} - \sqrt{\tau/2} (\lambda + 1) &< z - \sqrt{\tau/2} (\lambda + 1) < \infty$$
        $$ -x/\sqrt{2\tau} - \sqrt{\tau/2} (\lambda + 1) &< y < \infty $$
    Así
    \begin{equation}
        \label{eq:I1}
        I_1 = \frac{e^{\left(\frac{\lambda + 1}{2} \right) x + \tau \frac{(\lambda + 1)^2}{4}}}{\sqrt{2\pi}} \int_{-x/\sqrt{2\tau} - \sqrt{\tau/2} (\lambda + 1) }^\infty e^{\left( - \frac{y^2}{2} \right)} dy
    \end{equation}
    La integral se puede representar en términos de la función de distribución acumulativa de una variable aleatoria normal estándar, usualmente denotada por $\Phi$, esto es
    $$ \Phi(d) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^d e^{-\frac{y^2}{2}} dy  = \frac{1}{\sqrt{2 \pi}} \int_{-d}^\infty e^{-\frac{y^2}{2}} dy$$
    Por lo tanto
    \begin{equation*}
        I_1 = e^{\left(\frac{\lambda + 1}{2} \right) x + \tau \frac{(\lambda + 1)^2}{4}} \Phi(d_1)
    \end{equation*}
    donde
    \begin{equation*}
        \label{eq:d1}
        d_1 = x/\sqrt{2\tau} + \sqrt{\tau/2} (\lambda + 1)
    \end{equation*}
    El cálculo de $I_2$ es idéntico al que se realizó para $I_1$, pero en este caso con $(\lambda - 1)$ en lugar de $(\lambda + 1)$, por lo que ya no es complicado llegar a que
    \begin{equation*}
        I_2 = e^{\left(\frac{\lambda - 1}{2} \right) x + \tau \frac{(\lambda - 1)^2}{4}} \Phi(d_2)
    \end{equation*}
    donde
    \begin{equation}
        \label{eq:d2}
        d_2 = x/\sqrt{2\tau} + \sqrt{\tau/2} (\lambda - 1)
    \end{equation}
    La solución del problema del valor inicial de la ecuación de calor transformada es
    \begin{equation*}
        u(x, \tau) = e^{\left(\frac{\lambda + 1}{2} \right) x + \tau \frac{(\lambda + 1)^2}{4}} \Phi(d_1) - e^{\left(\frac{\lambda - 1}{2} \right) x + \tau \frac{(\lambda - 1)^2}{4}} \Phi(d_2)
    \end{equation*}
    Lo único que nos queda por hacer es 'deshacer' los cambios de variable que realizamos a lo largo de este proceso.\\
    Comenzando por $u$
    \begin{align*}
        v(x, \tau) &= e^{-\frac{(\lambda - 1)}{2} x - \frac{(\lambda + 1)^2}{4} \tau} u(x, \tau)\\
        &= e^{-\frac{(\lambda - 1)}{2} x - \frac{(\lambda + 1)^2}{4} \tau} \left(e^{\left(\frac{\lambda + 1}{2} \right) x + \tau \frac{(\lambda + 1)^2}{4}} \Phi(d_1) - e^{\left(\frac{\lambda - 1}{2} \right) x + \tau \frac{(\lambda - 1)^2}{4}} \Phi(d_2) \right)\\
        &= e^{(\frac{\lambda + 1}{2})x - (\frac{\lambda - 1}{2})x} \Phi(d_1) - e^{\frac{(\lambda - 1)^2}{4}\tau - \frac{(\lambda + 1)^2}{4}\tau} \Phi(d_2)\\
        &= e^x \Phi(d_1) - e^{-\lambda \tau} \Phi(d_2)
    \end{align*}
    Sustituyendo $x = \ln{S/K}$, $\tau = \left(\dfrac{1}{2}\right) \sigma^2 (T-t)$ y $ \lambda = \dfrac{2}{\sigma^2} \cdot r$
    \begin{align*}
        &= e^{\ln{S/K}} \Phi(d_1) - e^{- (2r/\sigma^2)(1/2)\sigma^2(T-t)} \Phi(d_2)\\
        &= \left(\frac{S}{K}\right) \Phi(d_1) - e^{-r(T-t)} \Phi(d_2)
    \end{align*}
    Y finalmente recordemos que $V(S,t) = K v(x, \tau)$, por lo que
    \begin{align*}
        V(S,t) &= K \left[ \left(\frac{S}{K}\right) \Phi(d_1) - e^{-r(T-t)} \Phi(d_2) \right]\\
        &= S \Phi(d_1) - K e^{-r(T-t)} \Phi(d_2)
    \end{align*}
    Es decir
    \begin{equation*}
        V(S,t) = S \Phi\left( \frac{\ln{(S/K)} + (r + \sigma^2/2)(T-t)}{\sigma\sqrt{T-t}} \right) - K e^{-r(T-t)}\Phi\left( \frac{\ln{(S/K)} + (r - \sigma^2/2)(T-t)}{\sigma\sqrt{T-t}} \right)
    \end{equation*}
    Este resultado es la solución más famosa de la ecuación de Black \& Scholes, la cuál representa el precio de una opción Call sobre el subyacente $S$ en el tiempo $t$.\\
    Aunque es más fácil encontrarla en su versión abreviada 
    \begin{equation*}
        C(S,t) = S \Phi(d_1) - K e^{-r(T-t)}\Phi(d_2)
    \end{equation*}
    donde
    \begin{equation*}
        d_1 = \frac{\ln{(S/K)} + (r + \sigma^2/2)(T-t)}{\sigma\sqrt{T-t}}
    \end{equation*}
    \begin{equation*}
        d_2 = \frac{\ln{(S/K)} + (r - \sigma^2/2)(T-t)}{\sigma\sqrt{T-t}}
    \end{equation*}
    Dado el resultado anterior y utilizando la paridad Put-Call, se puede deducir de manera muy sencilla el precio para una opción Put de la siguiente manera
    \begin{align*}
        C(S,t) - P(S,t) &= S_t - Ke^{-r(T-t)}\\
        P(S,t) &= C(S,t) - S_t + Ke^{-r(T-t)}\\
        &= S \Phi(d_1) - K e^{-r(T-t)}\Phi(d_2) - S_t + Ke^{-r(T-t)}\\
        &= S_t(\Phi(d_1) - 1) - Ke^{-r(T-t)}(\Phi(d_2) - 1)\\
    \end{align*}
    Por la simetría de la distribución normal estándar se tiene que  $\Phi(x) = 1 - \Phi(-x)$ para cualquier $x \in \mathbb{R}$, así
    \begin{align*}
        &= S_t(-\Phi(-d_1)) - Ke^{-r(T-t)}(-\Phi(-d_2))\\
        &= Ke^{-r(T-t)}\Phi(-d_2) - S_t\Phi(-d_1)
    \end{align*}
    Por lo tanto el precio de una opción Put sobre el subyacente $S$ en el tiempo $t$ es
    \begin{equation*}
        P(S,t) = Ke^{-r(T-t)}\Phi(-d_2) - S_t\Phi(-d_1)
    \end{equation*}
    donde $d_1$ y $d_2$ son de la misma forma que se mencionó previamente para el caso del Call.\\
    Cabe mencionar que la fórmula del precio de la opción Put de igual manera es solución de la ecuación diferencial de Black \& Scholes.
\end{solucion}

\subsection*{Movimiento Browniano Geométrico}
Bajo el modelo de Black-Scholes-Merton, el precio de una acción que no paga dividendos sigue el comportamiento de una Movimiento Browniano Geométrico
\begin{equation}
    \label{eq:dS}
    dS_t = \mu S_t dt + \sigma S_t dB_t
\end{equation}
donde $B_t$ es un movimiento Browniano, $\mu$ es la tasa de rendimiento esperada de la acción, $\sigma$ es la volatilidad del precio de las acciones. Esta ecuación es el modelo más utilizado del comportamiento del precio de las acciones y en un mundo neutral al riesgo, $\mu$ es igual a la tasa libre de riesgo $r$.\\
Para resolver la ecuación diferencial estocástica $\eqref{eq:dS}$, hacemos lo siguiente.\\
Definamos $f(t,x) = \ln{x}$. Así
\begin{equation*}
    \frac{\partial f}{\partial x} = \frac{1}{x},
    \ \ \ \ \ \ \ \ \frac{\partial^2 f}{\partial x^2} = -\frac{1}{x^2},
    \ \ \ \ \ \ \ \ \frac{\partial f}{\partial t} = 0.
\end{equation*}
Ahora, usando el lema de Itô en su forma diferencial obtenemos que
\begin{align*}
    d\ln{(S_t)} = df(t, S_t) &= \frac{\partial f}{\partial t} (t, S_t) dt + \frac{\partial f}{\partial x} (t, S_t) dS_t + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} (t, S_t) (dS_t)^2\\
    &= 0 \cdot dt + \left( \frac{1}{S_t} \right) dS_t + \left( \frac{1}{2} \right)\left( - \frac{1}{S_t^2} \right) (dS_t)^2
\end{align*}
por $\eqref{eq:dS}$ obtenemos que
\begin{align*}
    &= \left( \frac{1}{S_t} \right) dS_t + \left( \frac{1}{2} \right)\left( - \frac{1}{S_t^2} \right) ( \mu S_t dt + \sigma S_t dB_t )^2 \\
    &= \left( \frac{1}{S_t} \right) dS_t + \left( \frac{1}{2} \right)\left( - \frac{1}{S_t^2} \right) ( \mu^2 S_t^2 (dt)^2 + 2\mu S_t \sigma S_t dt dB_t + \sigma^2 S_t^2 (dB_t)^2)
\end{align*}
como $dtdt = 0$, $ dtdB_t = 0$ y $dB_tdB_t = dt$, entonces
\begin{align*}
    &= \left( \frac{1}{S_t} \right) dS_t + \left( \frac{1}{2} \right)\left( - \frac{1}{S_t^2} \right) (\sigma^2 S_t^2 dt)\\
    &= \left( \frac{1}{S_t} \right) (\mu S_t dt + \sigma S_t dB_t) + -\frac{\sigma^2}{2} S_t dt\\
    &= \mu dt + \sigma dB_t - \frac{\sigma^2}{2} S_t dt\\
    &= \left(\mu - \frac{\sigma^2}{2}\right) dt + \sigma dB_t.
\end{align*}
Por lo tanto
\begin{equation}
    \label{eq:lnSt}
    d\ln{(S_t)} = \left(\mu - \frac{\sigma^2}{2}\right) dt + \sigma dB_t
\end{equation}
Finalmente, si integramos de 0 a $t$ obtenemos
\begin{align*}
    \int_0^t d\ln{(S_t)} = \int_0^t \left(\mu - \frac{\sigma^2}{2}\right) dt + \sigma \int_0^t dB_t\\
    \ln{(S_t)} - \ln{(S_0)} = \left(\mu - \frac{\sigma^2}{2}\right) \cdot t + \sigma (B_t - B_0)
\end{align*}
Como $\{B_t\}_t$ es movimiento Browniano, entonces
\begin{align*}
    \ln{\left(\frac{S_t}{S_0}\right)} = \left(\mu - \frac{\sigma^2}{2}\right) t + \sigma B_t\\
    \frac{S_t}{S_0} = e^{\left(\mu - \frac{\sigma^2}{2}\right) t + \sigma B_t}
\end{align*}
Por lo tanto
\begin{equation}
    \label{eq:St}
    S_t = S_0 e^{\left(\mu - \frac{\sigma^2}{2}\right) t + \sigma B_t}
\end{equation}
Nótese que despreciando el movimiento browniano $B_t$, entonces $S_t$ cambia geométricamente con un factor común $e^{(\mu - \frac{\sigma^2}{2})}$. Es por eso que a $S_t$ se llama un movimiento browniano geométrico.
\subsection*{Esperanza y Varianza de $S_t$}
Calculemos ahora la esperanza de $S_t$ de la siguiente forma
\begin{align*}
    \mathbb{E}(S_t) &= \mathbb{E}\left( S_0 e^{\left(\mu - \frac{\sigma^2}{2}\right) t + \sigma B_t} \right)\\
    &= S_0 e^{\left(\mu - \frac{\sigma^2}{2}\right) t} \mathbb{E}\left( e^{\sigma B_t} \right)\\
    &= S_0 e^{\left(\mu - \frac{\sigma^2}{2}\right) t} \varphi_{B_t}(\sigma)
\end{align*}
donde $\varphi_{B_t}$ es la función generadora de momentos de $B_t \sim N(0,t)$, así
\begin{align*}
    &= S_0 e^{\left(\mu - \frac{\sigma^2}{2}\right) t} e^{\frac{1}{2} \sigma^2 t}\\
    &= S_0 e^{\mu t}
\end{align*}
Por lo tanto
\begin{equation}
    \label{eq:ES}
    \mathbb{E}(S_t) = S_0 e^{\mu t}
\end{equation}
Por otra parte, para calcular la varianza de $S_t$ primero calculamos $\mathbb{E}(S_t^2)$
\begin{align*}
    \mathbb{E}(S_t^2) &= \mathbb{E}\left( S_0^2 e^{(2\mu - \sigma^2) t + 2\sigma W_t} \right)\\
    &= S_0^2 e^{(2\mu - \sigma^2) t} \mathbb{E}\left( e^{2\sigma W_t} \right)\\
    &= S_0^2 e^{(2\mu - \sigma^2) t} \varphi_{W_t}(2\sigma)\\
    &= S_0^2 e^{(2\mu - \sigma^2) t} e^{2 \sigma^2 t}\\
    &= S_0^2 e^{(2\mu + \sigma^2) t}
\end{align*}
Por lo tanto
\begin{equation}
    \label{eq:ES2}
    \mathbb{E}(S_t^2) = S_0^2 e^{(2\mu + \sigma^2) t}
\end{equation}
De esta forma, por $\eqref{eq:ES}$ y $\eqref{eq:ES2}$, la varianza de $S_t$ es
\begin{align*}
    \mathbb{V}(S_t) &= \mathbb{E}(S_t^2) - \mathbb{E}(S_t)^2\\
    &= S_0^2 e^{(2\mu + \sigma^2) t} - S_0^2 e^{2\mu t}\\
    &= S_0^2 e^{2 \mu t} \left(e^{\sigma^2 t} - 1 \right) 
\end{align*}
Es decir
\begin{equation*}
    \label{eq:VS}
    \mathbb{V}(S_t) = S_0^2 e^{2 \mu t} \left(e^{\sigma^2 t} - 1 \right)
\end{equation*}
\subsubsection{Estimación de Máxima Verosimilitud}
Consideremos el Movimiento Browniano Geométrico anterior, es decir, el proceso
\begin{equation*}
    dS_t = \mu S_t dt + \sigma S_t dW_t
\end{equation*}
Es importante mencionar que se supondrá que tenemos $m$ simulaciones de trayectorias independientes de este proceso, y que deseamos estimar el vector de parámetros $\theta = (\mu, \sigma)$.\\
Consideremos ahora el proceso $\eqref{eq:lnSt}$ pero tomando el cambio de variable $Y(t) = \ln{(S_t)}$
\begin{equation*}
    dY(t) = \left( \mu - \frac{\sigma^2}{2} \right) dt + \sigma dB_t
\end{equation*}
Para facilitar la construcción asumiremos que el tiempo total $T$ se divide en $n$ incrementos iguales de $\Delta t$, entonces $T = n\Delta t$.\\
Definamos ahora 
$$Y_i = Y(i \Delta t) \ \ \ \ \ \mbox{y} \ \ \ \ \ Z_i = Y_i - Y_{i-1}$$
No es difícil verificar que tenemos $n$ variables aleatorias $Z_i \sim N((\mu - \frac{\sigma^2}{2}) \Delta t, \sigma^2 \Delta t)$ independientes.\\
Por lo tanto, si tenemos $m$ trayectorias simuladas con $n$ incrementos iguales $\Delta t$, con el mismo valor inicial $S_0$ y denotamos las observaciones como $\{ S_{(ij)} \}$, donde $i = 1, 2, ..., n$ denota los valores de la $j$-ésima trayectoria (con $j = 1, 2, ..., m$) al tiempo $i\Delta t$. Entonces los Estimadores de Máxima Verosimilitud de $\mu$ y $\sigma$ están dados por
\begin{align*}
    \hat{\sigma} &= \sqrt{\frac{1}{ m n \Delta t} \left[ \sum_{ij} (Z_{ij}^2) - \frac{1}{m n} \left( \sum_{ij} Z_{ij} \right)^2\right]} \\
    \hat{\mu} &= \frac{1}{m n \Delta t} \sum_{ij} Z_{ij} + \frac{\hat{\sigma}^2}{2}
\end{align*}

        
        \subsection{CIR++}
        %%July 
        El modelo \(CIR++\) es una extensión del modelo \(CIR\). Pero en este caso, el proceso  es definido como en  (\ref{SII.CIR.1}), donde su vector de parámetros es \( \alpha= (k,\theta,\sigma) \).La tasa dinámica estará dada por 
        \begin{equation}\label{SII.CIR++.0}
        dX_{t}=k\left ( \theta-X_{t} \right )dt+\sigma \sqrt{X_{t}}dB_{t}. \ \ \ X_0 = x_0    
        \end{equation}
        \begin{equation}\label{SII.CIR++.(-1)}
            R_{t}=X_{t}+\varphi (t),
        \end{equation}
        Donde \(x_{0}\) , \(k\) , \(\theta \) y \( \sigma \) son constantes positivas tal que \(2k> \sigma \)  asegurando así que el proceso \(X\) permanezca positivo.%%A continuación se presentan algunas fórmulas analíticas implicadas por dicha extensión, estás son \(P(t,T)=A(t,T)e^{\left ( -B(t,T) \right )R_{t}}\) y 

        %%\begin{align}\label{SIR.CIR++.1}
        %%    \mathbf{ZBC}\left ( t,T,S,X \right )=P(t,S)X^{2}\left ( 2\overline{r}[\rho +\Psi +B(T,S)];\frac{4k\theta }{\sigma ^{2}}, \frac{2\rho ^{2}R_{t}e^{\left \{ h(T-t) \right \}}}{\rho+ \Psi +B(T,S) } \right )\\ 
        %%-XP(t,T)X^{2}\left ( 2\overline{r}[\rho +\Psi];\frac{4k\theta }{\sigma ^{2}}, \frac{2\rho ^{2}R_{t}e^{\left \{ h(T-t) \right \}}}{\rho+ \Psi } \right )
        %%\end{align}
        %%Teniendo por notación 
        %%\begin{align*}
        %%   \rho= \rho(T-t):= \frac{2h}{\sigma ^{2}\left ( e^{\left [ h(T-t)-1 \right ]} \right )} \\
        %%    \Psi =\frac{k+h}{\sigma ^{2}}\\
        %%    \overline{r}= \overline{r}(S-T):= \frac{A(T,S/X ))}{B(T,S)} 
        %%\end{align*}%%
            Suponiendo un ajuste exacto de la estructura de términos iniciales de los factores de descuento, tenemos que \( \varphi(t)= \varphi^{CIR}(t;\alpha)\) donde 
                \begin{align*}
                    \varphi^{CIR}(t;\alpha)=f^{M}(0,t)-f^{CIR}(0,t;\alpha),
        \end{align*}
        \begin{equation*}
             f^{CIR}(0,t:\alpha)=\frac{2k\theta\left ( e^{\left\{th \right \}}-1 \right )}{2h+(k+h)\left ( e^{\left\{th \right \}}-1 \right )}+x_{0}\frac{4h^2 e^{ \left\{th \right \}}} {\left [ 2h+(k+h)(e^{ \left\{th \right \}}-1 ) \right ]^{2}}
        \end{equation*}
       y
       \begin{equation*}
            f^{M}(0,t)=\frac{\partial \ln P^{M}(0,t)}{\partial t}
       \end{equation*}
       Aquí \(P^{M}(0,t)\) es el precio de mercado de un bono cupón cero con madurez en \(t\) y  \(h=\sqrt{k^{2}+2\sigma^{2}}\). Por otro lado, sabemos que el precio en el tiempo \(t\), de un bono cupón cero con madurez \(T\), puede calcularse como 
       
        \begin{equation*}
        P(t,T)=\overline{A}(t,T)e^{-B(t,T)R_{t}}
         \end{equation*}
        
        donde
        \begin{equation*}
        \overline{A}(t,T)=\frac{P^{M}(0,T)A(0,t)e^{\left\{-B\left ( 0,t \right )x_{0} \right\}}}{P^{M}(0,t)A(0,T)e^{\left \{-B\left ( 0,T \right )x_{0}\right \}}}A(t,T)e^{B\left ( t,T \right )\varphi^{CIR}(t;\alpha)}
           \end{equation*}
        y \(A(t,T)\), \(B(t,T)\) están definidas como 
        \begin{align*}
            A(t,T)&=\left [ \frac{2h exp^{{(k+h)(T-t)\frac{1}{2}}}}{2h+(k+h)(e^{{(T-t)h}}-1)} \right ]^{\frac{2k\theta}{\sigma^{2}}}\\
            B(t,T)&=\frac{2\left ( e^{{(T-t)h}}-1 \right )}{2h+(k+h)\left ( e^{{(T-t)h}}-1 \right ) )},\\
            h&=\sqrt{k^{2}+2\sigma^{2}}.
        \end{align*}
        
        Así la tasa spot en el momento \(t\), al vencimiento \(T\), bajo este modelo estará dada por: 
        \begin{align*}
            R(t,T)= \frac{\ln{\frac{P^{M}(0,t)A(0,T)exp{-B\left ( 0,T \right )x_{0}}}{A(t,T)P^{M}(0,T)A(0,t)exp{-B\left ( 0,T \right )x_{0}}}}}{T-t}-\frac{B\left ( t,T \right) \varphi^{CIR}(t;\alpha)-B\left ( t,T \right )R_{t}}{T-t}
        \end{align*}
        
        lo cual es compatible con la definición de \(R_{t}\).
        El precio en el momento \(t\) de una opción de compra europea con vencimiento \(T > t\) y el precio strike \(K\) en un bono cupón \(0\) con vencimiento \(\tau>T\) es
        \begin{align*}
            \mathbf{ZBC}\left ( t,T,\tau,\mathbf{K} \right )=\frac{P^{M}(0,\tau)A(0,t)e^{{-B\left ( 0,t \right )}x_{0}}}{P^{M}(0,t)A(0,\tau)e^{{-B\left ( 0,\tau \right )}x_{0}}}\\
            \\
           * \mathbf{\Psi^{CIR}}\left ( t,T,\tau,\mathbf{K} \frac{P^{M}(0,T)A(0,\tau)e^{{-B\left ( 0,\tau \right )}x_{0}}}{P^{M}(0,\tau)A(0,T)e^{{-B\left ( 0,T \right )}x_{0}}},r(t)-\varphi ^{CIR}(t;\alpha);\alpha)\right )
        \end{align*}
        
        Donde \(\mathbf{\Psi ^{CIR}}\left ( t,T,\tau,X;\alpha \right )\) es el precio de la opción CIR definida como 


        \begin{align*}
    \mathbf{ZBC}\left ( t,T,S,X \right )=P(t,S)X^{2}\left ( 2\overline{r}[\rho +\Psi +B(T,S)];\frac{4k\theta }{\sigma ^{2}}, \frac{2\rho ^{2}R_{t}e^{\left \{ h(T-t) \right \}}}{\rho+ \Psi +B(T,S) } \right )\\ 
    -XP(t,T)X^{2}\left ( 2\overline{r}[\rho +\Psi];\frac{4k\theta }{\sigma ^{2}}, \frac{2\rho ^{2}R_{t}e^{\left \{ h(T-t) \right \}}}{\rho+ \Psi } \right )
             \end{align*}      
    Teniendo por notación 
    \begin{align*}
          \rho:= \frac{2h}{\sigma ^{2}\left ( e^{\left [ h(T-t)-1 \right ]} \right )} \\
        \Psi =\frac{k+h}{\sigma ^{2}}\\
        \overline{r}= \frac{A(T,S/X ))}{B(T,S)} 
\end{align*}
        
        
        simplificando esta formula obtenemos:
\begin{align*}
        \mathbf{ZBC}\left ( t,T,S,X \right )=P(t,S)\chi^{2}\left ( 2\overline{r}[\rho +\Psi +B(T,S)];\frac{4k\theta }{\sigma ^{2}}, \frac{2\rho ^{2}R_{t}e{^ \left \{ h(T-t) \right \}}}{\rho+ \Psi +B(T,S) } \right )\\ 
        -KP(t,T)\chi^{2}\left ( 2\overline{r}[\rho +\Psi];\frac{4k\theta }{\sigma ^{2}}, \frac{2\rho ^{2}R_{t}e^{\left \{ h(T-t) \right \}}}{\rho+ \Psi } \right )
\end{align*}
        con \(R_{t}=x\).\ Al simplificar más esta fórmula, obtenemos
        
        \begin{multline}\label{SII.CIR++.3}
            \mathbf{ZBC}\left ( t,T,\tau,\mathbf{K} \right )=\\
             P(t,\tau)\chi^{2}\left ( 2\widehat{r}[\rho+\Psi + B(T,\tau) ]; \frac{4k\theta}{\sigma^{2}},\frac{2\rho^{2}[r(t)-\varphi ^{CIR}(t;\alpha)]e^{{h(T-t)}}}{\rho+\Psi + B(T,\tau)} \right )\\
             -KP(t,T)\mathit{X}^{2}\left ( 2\widehat{r}[\rho +\Psi ];\frac{4k\theta}{\sigma^{2}},\frac{2\rho^{2}[r(t)-\varphi ^{CIR}(t;\alpha)]e^{{h(T-t)}}}{\rho+\Psi + B(T,\tau)} \right )
        \end{multline}
        con
        \\
        \begin{equation*}
            \widehat{r}=\frac{1}{B(T,t)}\left [ \ln \frac{A(T,\tau)}{K} -\ln\frac{P^{M}(0,T)A(0,\tau)e^{{-B\left ( 0,\tau \right )}x_{0}}}{P^{M}(0,\tau)A(0,T)e^{{-B\left ( 0,T \right )}x_{0}}} \right]
        \end{equation*}
           El precio de la opción put se obtiene a través de la paridad put-call.\\
        A partir de esta formula podemos  fijar con precios de caps y floors ya que pueden ser vistos como portafolios de opciones de bonos cero.
        Comencemos con una cópula valuada en el tiempo t. La cópula reestablecida en el tiempo T , que paga en el tiempo \(T+\tau\) y strike en X y la cantidad nominal N.
        \begin{equation*}
            \mathbf{Cpl}(t,T,T+\tau,N,X)=N(1+X\tau)\mathbf{ZBP}\left ( t,T,T+\tau,\frac{1}{1+X\tau} \right )
        \end{equation*}
        
            
        En términos más generales, en lo que respecta a un Cap, denotamos por \(\tau =\left \{ t_{0},t_{1},...,t_{n} \right \}\) al conjunto de los tiempos de pago de cap aumentados con la primera fecha \(t_{0}\), y por \(\tau_{i}\) la fracción de año de \(t_{i-1}\)  a \(t_{i}, i=1,...,n\) . Aplicando \(\sigma _{f}\left ( t,T \right )=\frac{\partial B(t,T)}{\partial T}\sigma(t,R_{t})\)
        entonces obtenemos el precio en el tiempo \(t<t_{0}\) del cap con tasa cap (strike) \(X\), valor nominal \(N\) en los tiempos \(\tau\) dado por 
        \begin{equation*}
            \mathbf{Cap}(t,\tau,N,X)= N \sum_{i=1}^{n}\left ( 1+X\tau_{i} \right )\mathbf{ZBP}\left(t,t_{i-1},t_{i},\frac{1}{1+X \tau_{i}}\right)
        \end{equation*}
        mientras que el precio para el floor es 
        \begin{equation*}
        \mathbf{Flr}(t,\tau,N,X)= N \sum_{i=1}^{n}\left ( 1+X\tau_{i} \right )\mathbf{ZBC}\left(t,t_{i-1},t_{i},\frac{1}{1+X \tau_{i}}\right)
        \end{equation*}
          
       %% Los swaps europeos pueden tener un precio explícito por medio de la descomposición de  Jamshidian's (1989). De hecho, considere una opción swap con tasa X,con vencimiento
        %%\(T\) y valor nominal N, donde se le da al titular el %%derecho de ingresar en el momento
        %%\(t_{0}=T\)  una tasa swap de interés con tiempos \(\tau={t_{1},...,t_{n}}, \)  \(t_{1}>T\)
        %%donde el pagador a la tasa fija X y arrendamiento. Nosotros
    %%    denotaremos por \(\tau_{i}\) a la fracción del año entre \(t_{i-1}\) a \(t_{i}\),\(i=1, ...,n\) y el conjunto \(c_{i}:=X\tau_{i}\) de \(i=1,..,n-1\) y \(c_{n}:=1+X\tau_i{i}\). Denotado por \(r*\) al valor de la tasa spot en el tiempo \(T\) para 
      %%  \begin{equation*}
    %%        \sum_{i=i}^{n}c_{i}\mathbf{\overline{A}}(T,t_{i}))e^{-B(T,t_{i})r^{*}}=1
    %%55    \end{equation*}
    %%    y para simplificar denotaremos \(X_{i}:=\mathbf{\overline{A}}(T,t_{i}))e^{-B(T,t_{i})r^{*}}\) y el precio del swap al tiempo \(T\) dado por 
        
      %%  \begin{equation*}
            %%\mathbf{PS}(t,T,\tau,N,X)= N %%\sum_{i=1}^{n}c_{i}\mathbf{ZBP}(t,T,t_{i},X_{i})
    %%    \end{equation*}
    %%    \begin{equation*}
      %%          \mathbf{PS}(t,T,\tau,N,X)= N \sum_{i=1}^{n}c_{i}\mathbf{ZBC}(t,T,t_{i},X_{i})
     % %  \end{equation*}
        %%%%Escribe aquí 
        
        
        La esperanza y la varianza de \(R_t\) son relativamente sencillas de calcular dado que el proceso \(X_t\) por (\ref{SII.CIR++.(-1)}), sigue un modelo CIR simple con \(\alpha = \theta k, \beta = k, \sigma = \sigma\),  y por lo realizado en la sección anterior se tiene
       \begin{align*}
          \mathbb{E}(X_s) &= x_0e^{-ks} +\frac{\theta k}{k}\left(1 - e^{-k s} \right)\\
                          &= x_0e^{-ks} + \theta\left(1 - e^{-k s} \right)
       \end{align*}
       Y por (\ref{SII.CIR++.0}) sabemos que \(X_s=R_s - \varphi(s)\), entonces: 
       \begin{equation}\label{SII.CIR++.10}
           \mathbb{E}(R_s) = x_0e^{-ks} + \theta\left(1 - e^{-k s} \right) + \mathbb{E}(\varphi(s)) \\
       \end{equation}
       Por otra parte la varianza de \(X_s\) estará dada por: 
       \begin{align*}
           \mathbb{V}(X_s) &= \frac{\sigma^2}{k}x_0\left(e^{-k s} - e^{-2k s} \right) + \frac{\theta k \sigma^2}{2k^2}
             \left(1 - e^{-2k s}\right)\\
                          &= \frac{\sigma^2}{k}x_0\left(e^{-k s} - e^{-2k s} \right) + \frac{\theta \sigma^2}{2k}
             \left(1 - e^{-2k s}\right)
       \end{align*}
       Así: 
       \begin{equation}\label{SII.CIR++.11}
           \mathbb{V}(R_s) &= \frac{\sigma^2}{k}x_0\left(e^{-k s} - e^{-2k s} \right) + \frac{\theta \sigma^2}{2k}
             \left(1 - e^{-2k s}\right) - \mathbb{V}(\varphi(s)) + 2Cov(\varphi(s),R_s)
       \end{equation}
       Si suponemos ahora que \(\varphi(s)\), es una función deterministica, es decir, sin componente aleatorio y que solo depende del tiempo \(t\), entonces podemos escribir a (\ref{SII.CIR++.10}) y (\ref{SII.CIR++.11}), como:
       \begin{align*}
            \mathbb{E}(R_s) &= x_0e^{-ks} + \theta\left(1 - e^{-k s} \right) + \varphi(s)\\
            \mathbb{V}(R_s) &= \frac{\sigma^2}{k}x_0\left(e^{-k s} - e^{-2k s} \right) + \frac{\theta \sigma^2}{2k}
             \left(1 - e^{-2k s}\right)
       \end{align*}
       
       
       
        \subsubsection{Estimácion por máxima verosimilitud para el modelo CIR++}
        Los parametros \(k, \theta\) y \(\sigma\) del modelo  CIR++ pueden ser estimados usando el método de estimación por máxima verosimilitud. Aunado al hecho de que los incrementos de la tasa modelada siguen una distribución ji-cuadrada, (Brigo y Mercurio 2007), dada por la siguiente expresion:
       \begin{equation*}
             X_{t}=\frac{\sigma^{2}(1-e^{-k(t-u)})}{4k}\chi^{2}_{d} \left ( \frac{4ke^{-k(t-u)}}{\sigma ^{2}(1-e^{-k(t-u)})}X_u \right )
        \end{equation*}
donde 
\begin{equation}\label{SII.CIR++.4}
d= \frac{4\theta k}{\sigma ^{2}}
\end{equation}
    Esto implica que dado \(R_{u}\), \(R_{t}\) están distribuida como \(\frac{\sigma^{2}(1-e^{-k(t-u)})}{4k}\) veces una ji-cuadrada con \(d\) grados de libertad, con parámetro de locación: 
    \begin{equation}\label{SII.CIR++.5}
       \lambda= \left ( \frac{4ke^{-k(t-u)}}{\sigma ^{2}(1-e^{-k(t-u)})} \right )R_{u}
    \end{equation}
De este modo la función de verosimilitud, puede ser expresada utilizando las probabilidades de transición del proceso, así con n observaciones se tiene:   
\begin{equation*}
    \boldsymbol{L}\left ( k,\theta,\sigma \right )=\prod_{t=1}^{n-1}\mathbb{P}\left ( R_{
t+\Delta t}| R_{t};k, \theta, \sigma \right )
\end{equation*}
Y para el modelo CIR++, dadas las observaciones anteriores, se tiene que: 
\begin{equation*}
\mathbb{P}\left ( R_{
t+\Delta t}| R_{t} \right )=(2c)\left(g\left ( 2R_{t+\Delta t}|2cR_{t} \right )\right)
\end{equation*}
donde: 
\begin{equation*}
    c=\frac{2k}{\sigma^{2}\left ( 1-e^{-k \Delta t} \right )}
\end{equation*}
    y \(g(.) \) es la función de distribución de una v.a. ji-cuadrada  no centrada con grados de libertad expuestos en (\ref{SII.CIR++.4}) y un parametro \( \lambda \) definido en (\ref{SII.CIR++.5}).
    Esto nos dice que el modelo CIR++ no es gaussiano y siguiendo la idea  que se distribuye como una Ji-Cuadrada necesitamos el uso de una optimización numerica para encontrar el estimador de maxima verosimilitud.
\section{Simulación}
\subsection{Tasa CETES con modelo Vasicek}
Para la simulación del modelo de Vasicek se uso la discretización propuesta en el método de Euler, y los estimadores de máxima verosimilitud obtenidos en la sección anterior, dada una semilla inicial, i.e, una tasa observada, denotada por, 
\(\hat{R_{t_1}} = r_{0}\), se podran obtener \(n-1\) simulaciones de la siguiente manera, \(\{\hat{R_{t_2}},\hdots,\hat{R_{t_n}}\}\), todas ellas separadas por un lapso de tiempo \(\delta\) constante , el algoritmo aplicado para la simulación del modelo se muestra en la siguiente igualdad: 
\[
    R_{t_{i+1}} = R_{t_i}e^{-\alpha \delta} + \mu\left(1 -  e^{-\alpha \delta}\right) + \sigma \sqrt{\frac{1 -  e^{-2\alpha \delta}}{2\alpha}}Z_{i}
\]
con \(i = 1,\hdots,n-1\), y con \(Z_i\) variables aleatorias independientes \(N(0,1)\). 
Nosotros decidimos para la simulación tomar del periodo que inicia el 20/04/2007, y culmina el miercoles de la semana que inicia el 20/04/20, se decidio tomar los miercoles por ser el dia en el que mas continuidad existia en la emisión de las tasas, debido a que los lunes y viernes suele haber puentes que en ocasiones se recorren al jueves o martes, pero casí nunca tocan el miércoles. Por otra parte el análisis se inicio en 2007, ya que dada la crisis financiera de 2009 se detecto un comportamiento parecido, en las tasas de CETES, al que se esta viendo en estos momentos, el cual tenia inicio a principios de dicho año. Por convención de mercado se tomaron años de 250 días y 50 semanas, así nuestra delta quedo dada por \(\delta = \frac{1}{50}\) 
Se construyo una variedad de funciones para el desarrollo de la simulación, aunque las dos principales se muestran a continuación, junto con algunos resultados importantes: 

\begin{itemize}
    \item VAS.SIM(r,date,tenor = True,deltat = 1/50,N = 50,seed = 46,nsims = 50000,conf = c(.005,.995),n.tr = 20)
            \begin{enumerate}
                \item r: El vector de tasas observadas. 
                \item date: Vector de fechas de las tasas observadas
                \item tenor: Booleano, True entonces se contruye el modelo para las tasas con capitalización anual simple, False se construye el modelo para las tasas con capitalización continua. 
                \item nsims: Número de simulaciones, el máximo probado hasta el momento son 50000.
                \item n.tr: Número de trayectorias que se seleccionaran aleatoriamente de las nsims simulaciones para ser graficadas, 
                se recomienda no imprimir más de 20 o la gráfica se vera amontonada.
            \end{enumerate}
    Los demás parametros deben dejarse en sus valores por default para obtener la simulación anual. Ejemplos del uso de está función son los siguientes: 
    \begin{center}
       \includegraphics[scale = 0.28]{vcon.png} 
    \end{center}
    La imágen anterior muestra los movimientos de la tasa de CETES 360 equivalente con capitalizacion continua, durante el periodo de tiempo analizado, una trayectoria del proceso simulado, y un intervalo de confianza del \(99.5\%\), la media y el número de trayectorias que se hayan solicitado graficar respectivamente, La imagen no es muy nitida pero en el script adjuntado a este trabajo puede visualizarse mucho mejor. %%:DDDDDDDD cy
    La siguiente imágen muestra exactamente lo mismo pero para la tasa de CETES 360 con capitalizacicon anual efectiva: 
    \begin{center}
       \includegraphics[scale = 0.28]{vsim.png} 
    \end{center}
    Dado que las diferencias ambas tasas son pequeñas, se aprecia que los intervalos de confianza son muy  similares 
    
    \item VAS.SIM.M(r,date,tenor = True,deltat = 1/50,N = 50,nsims = 50000,conf = .995,n.tr = 8,color = "p")
            \begin{enumerate}
                \item r: El vector de tasas observadas. 
                \item date: Vector de fechas de las tasas observadas
                \item tenor: Booleano, True entonces se contruye el modelo para las tasas con capitalización anual simple, False se construye el modelo para las tasas con capitalización continua. 
                \item nsims: Número de simulaciones, el máximo probado hasta el momento son 50000.
                \item n.tr: Número de trayectorias que se seleccionaran aleatoriamente de las nsims simulaciones para ser graficadas, 
                se recomienda no imprimir más de 8 o la gráfica se vera amontonada.
                \item conf: nivel de confianza establecido para graficar el intervalo.
                \item color: paleta de colores con la que se imprimira la gráfica, si color = "p", entonces se tomara una paleta
                de purpuras, si color = "b", una paleta de azules se ocupara en su lugar
            \end{enumerate}
   A continuación algunos ejemplos: 
    \begin{center}
       \includegraphics[scale = 0.4]{vtrc.png} 
    \end{center}
  Lo anterior es para la tasa continua, y lo siguiente para la tasa anual efectiva:
    \begin{center}
       \includegraphics[scale = 0.4]{vtrs.png} 
    \end{center}
  En los ejemplos anteriores se pueden apreciar las dos paletas de colores. De igual modo estás graficas pueden observarse mejor en el script adjunto
\end{itemize}

\subsection{Tasa CETES con modelo CIR}
Para la simulación del modelo CIR se uso la discretización propuesta en el método de Euler, y los estimadores de máxima verosimilitud y mínimos cuadrados ordinarios, obtenidos en la sección anterior, dada una semilla inicial, i.e, una tasa observada, denotada por, 
\(\hat{R_{t_1}} = r_{0}\), se podran obtener \(n-1\) simulaciones de la siguiente manera, \(\{\hat{R_{t_2}},\hdots,\hat{R_{t_n}}\}\), todas ellas separadas por un lapso de tiempo \(\delta\) constante , el algoritmo aplicado para la simulación del modelo se muestra en la siguiente igualdad: 
\[
    \hat{R_{t_{i+1}}} = \hat{R_{t_i}} + \alpha\left( \mu - \hat{R_{t_i}}\right) + \sigma \sqrt{\hat{R_{t_i}}\delta} Z_{i}
\]
con \(i = 1,\hdots,n-1\), y con \(Z_i\) variables aleatorias independientes \(N(0,1)\). 
Nosotros decidimos para la simulación tomar del periodo que inicia el 20/04/2007, y culmina el miércoles de la semana que inicia el 20/04/20, se decidió tomar los miércoles por ser el dia en el que mas continuidad existia en la emisión de las tasas, debido a que los lunes y viernes suele haber puentes que en ocasiones se recorren al jueves o martes, pero casí nunca tocan el miércoles. Por otra parte el análisis se inicio en 2007, ya que dada la crisis financiera de 2009 se detecto un comportamiento parecido, en las tasas de CETES, al que se esta viendo en estos momentos, el cual tenia inicio a principios de dicho año. Por convención de mercado se tomaron años de 250 días y 50 semanas, así nuestra delta quedo dada por \(\delta = \frac{1}{50}\) 
Se construyo una variedad de funciones para el desarrollo de la simulación, aunque las dos principales se muestran a continuación, junto con algunos resultados importantes: 

\begin{itemize}
    \item CIR.SIM(r,date,tenor = True,deltat = 1/50,N = 50,seed = 46,nsims = 50000,conf = c(.005,.995),n.tr=20)
            \begin{enumerate}
                \item r: El vector de tasas observadas. 
                \item date: Vector de fechas de las tasas observadas
                \item tenor: Booleano, True entonces se contruye el modelo para las tasas con capitalización anual simple, False se construye el modelo para las tasas con capitalización continua. 
                \item nsims: Número de simulaciones, el máximo probado hasta el momento son 50000.
                \item n.tr: Número de trayectorias que se seleccionaran aleatoriamente de las nsims simulaciones para ser graficadas, 
                se recomienda no imprimir más de 20 o la gráfica se vera amontonada.
            \end{enumerate}
    Los demás parametros deben dejarse en sus valores por default para obtener la simulación anual. Ejemplos del uso de está función son los siguientes: 
    \begin{center}
       \includegraphics[scale = 0.28]{Ccon.png} 
    \end{center}
    La imágen anterior muestra los movimientos de la tasa de CETES 360 equivalente con capitalizacion continua, durante el periodo de tiempo analizado, una trayectoria del proceso simulado, y un intervalo de confianza del \(99.5\%\), la media y el número de trayectorias que se hayan solicitado graficar respectivamente, La imagen no es muy nitida pero en el script adjuntado a este trabajo puede visualizarse mucho mejor. %%:DDDDDDDD cy
    La siguiente imágen muestra exactamente lo mismo pero para la tasa de CETES 360 con capitalizacicon anual efectiva: 
    \begin{center}
       \includegraphics[scale = 0.28]{Csim.png} 
    \end{center}
    Dado que las diferencias ambas tasas son pequeñas, se aprecia que los intervalos de confianza son muy  similares 
    
    \item CIR.SIM.M(r,date,tenor = True,deltat = 1/50,N = 50,nsims = 50000,conf = .995,n.tr = 8,color = "g")
            \begin{enumerate}
                \item r: El vector de tasas observadas. 
                \item date: Vector de fechas de las tasas observadas
                \item tenor: Booleano, True entonces se contruye el modelo para las tasas con capitalización anual simple, False se construye el modelo para las tasas con capitalización continua. 
                \item nsims: Número de simulaciones, el máximo probado hasta el momento son 50000.
                \item n.tr: Número de trayectorias que se seleccionaran aleatoriamente de las nsims simulaciones para ser graficadas, 
                se recomienda no imprimir más de 8 o la gráfica se vera amontonada.
                \item conf: nivel de confianza establecido para graficar el intervalo.
                \item color: paleta de colores con la que se imprimira la gráfica, si color = "g", entonces se tomara una paleta
                de verdes, si color = "b", una paleta de azules se ocupara en su lugar
            \end{enumerate}
   A continuación algunos ejemplos: 
    \begin{center}
       \includegraphics[scale = 0.4]{Ctrc.png} 
    \end{center}
  Lo anterior es para la tasa continua, y lo siguiente para la tasa anual efectiva:
    \begin{center}
       \includegraphics[scale = 0.4]{Ctrs.png} 
    \end{center}
  En los ejemplos anteriores se pueden apreciar las dos paletas de colores. De igual modo estás graficas pueden observarse mejor en el script adjunto
\end{itemize}
Se aprecia que los intervalos de confianza generados por el modelo CIR tienden a ser mas estrechos, una de las ventajas del uso de este modelo, la tasa modelada sera mayor a cero c.s. 

\section{Ejercicios teóricos adicionales}
\subsection{Tiempos de paro.} Sea \(T\) un tiempo de paro respecto a una filtracion \(\left \{ \mathcal{F}_{t} \right \}_{t\geq 0}\) y sea \( \mathcal{F}_{T}\) la \(\sigma\)-álgebra detenida en \(T\): 
\begin{itemize}
    \item \(\mathcal{F}_{T}\) es una \(\sigma\)-álgebra.
    \begin{enumerate}
        \item \(\Omega \in \mathcal{F}_{T}\)
             \begin{solucion}
             Sabemos que \(\Omega \in \mathcal{F}_{\infty}\) ya que \(\mathcal{F}_{\infty}\) es una \(\sigma\)-álgebra, por otro lado al ser \(T\) un tiempo de paro se tiene para toda \(t\geq 0\)que:
             \[\Omega\cap\{T \leq t\} = \{T \leq t\}\in \mathcal{F}_t\]
             de lo cual se puede concluir que: 
             \[\Omega \in \mathcal{F}_t \ \ \ \forall t \geq 0\]
             \end{solucion}
         \item Si \(A \in \mathcal{F}_{T}\) entonces \(A^c \in \mathcal{F}_{T}\)
             \begin{solucion}
             Al ser \(\mathcal{F}_{\infty}\) una \(\sigma\)-álgebra, se deberia de tener que si \(A \in \mathcal{F}_{\infty}\) entonces \(A \in \mathcal{F}_{\infty}\), por lo que solo resta probar que: 
             \[A^c\cap\{T \leq t\} \in \mathcal{F}_t \ \ \ \forall t \geq 0\]
             Pero al estar \(A \in \mathcal{F}_T\) y ser \(T\) un tiempo de paro sabemos que para cada \(t \geq 0 \) se debe de tener que: 
             \[A\cap\{T \leq t\} \in \mathcal{F}_{t}\ \ \ \ \{T \leq t\} \in \mathcal{F}_{t}...(1)\]
             y notemos que para toda \(t \geq 0\) se satisface la siguiente igualdad:
             \[A^c\cap\{T \leq t\} = \{T \leq t\}/ \left \{ A\cap\{T \leq t\} \right \} \in \mathcal{F}_{t}\]
             esto último por (1) y por ser \(\mathcal{F}_{t}\) una \(\sigma\)-álgebra para cada \(t \geq 0\)
             por tanto se tiene que: 
             \[A^c\cap\{T \leq t\} \in \mathcal{F}_{t} \ \ \ \forall t \geq 0\]
             Lo cual implica que: 
             \[A^c \in \mathcal{F}_{T}\]
             \end{solucion}
         \item Si \(\{A_n\}_{n \in \mathbb{N}} \subseteq  \mathcal{F}_{T}\) entonces \(\bigcup_{n \in \mathbb{N}}A_n \in \mathcal{F}_{T}\)
            \begin{solucion}
            Una vez más notemos que al ser \(\mathcal{F}_{\infty}\) una \(\sigma\)-álgebra, se deberia de tener que si \(\{A_n\}_{n \in \mathbb{N}} \subseteq  \mathcal{F}_{\infty}\) entonces \(\bigcup_{n \in \mathbb{N}}A_n \in \mathcal{F}_{\infty}\), por lo que solo resta probar que: 
             \[\left \{ \bigcup_{n \in \mathbb{N}}A_n \right \}\cap\{T \leq t\} \in \mathcal{F}_t \ \ \ \forall t \geq 0\]
             Lo cual es bastante sencillo ya que para cada \(t \geq 0\) y para cada \(n \in \mathbb{N}\) se debería tener que:
             \[A_n\cap\{T \leq t\} \in \mathcal{F}_{t}\]
             Pero al ser \(\mathcal{F}_{t}\) una \(\sigma\)-álgebra para cada \(t \geq 0\), esto último implica que:
             \[\left \{ \bigcup_{n \in \mathbb{N}}A_n \right \}\cap\{T \leq t\} = \left \{ \bigcup_{n \in \mathbb{N}} A_n\cap\{T \leq t\}\right \}  \cap\{T \leq t\} \in \mathcal{F}_{t}\]
             Esto último implica que: 
             \[\bigcup_{n \in \mathbb{N}}A_n \in \mathcal{F}_{T}\]
            \end{solucion}
    \end{enumerate}
    \item \(T\) es \(\mathcal{F}_{T}\)-medible.
           \begin{solucion}
           Esto es trivial ya que al ser \(T\) un tiempo de paro sabemos que para cada \(t \geq 0\) se tiene: 
           \[\{T \leq t\}\in \mathcal{F}_t \subseteq \mathcal{F}_{\infty}...(1)\] 
           Por último solo falta notar que: 
           \[\{T \leq a\}\cap\{T \leq t\}=\{T \leq min(a,t)\} \in \mathcal{F}_{min(a,t)} \subseteq \mathcal{F}_{t}\ \ \  \forall t,a \geq 0 ... (2)\]
          por lo cual se puede concluir de (1) y (2) que:
          \[\{T \leq t\} \in \mathcal{F}_T \ \ \  \forall t \geq 0\]
          Lo cual es suficiente para decir que \(T\) es \(\mathcal{F}_{T}\)-medible. 
           \end{solucion}
    \item Sí \(T\) es tiempo de paro finito c.s y \(\left \{ \mathcal{F}_{t} \right \}_{t\geq 0}\) es la filtración natural generada por un MB \(\left \{ B_{t} \right \}_{t\geq 0}\) entonces la variable aleatoria \(B_{T}\) es \(\mathcal{F}_{T}\)-medible.
    \begin{solucion}
      Definamos la variable aleatoria \(\mathrm{1}_{\left\{T < \infty\right\}}B_T\) bajo la siguiente regla de correspondencia: 
      \[
      \left\{\begin{matrix}
      B_{T(\omega)}(\omega)  & si \ \ T(\omega) < \infty\\ 
      0 & si \ \ T(\omega) = \infty
      \end{matrix}\right.
      \]
    Se probara que dicha variable aleatoria es \(\mathcal{F}_{T}\)-medible. Observese que se cumple la siguiente cadena de igualdades: 
    \[\mathrm{1}_{\left\{T < \infty\right\}}B_T = \lim_{n \to \infty}\left\{ \sum_{i = 0}^{\infty}\mathrm{1}_{\left\{ i2^{-n} \leq T < (i+1)2^{-n}\right\}}B_{i2^{-n}} \right\} = \lim_{n \to \infty}\left\{ \sum_{i = 0}^{\infty} \mathrm{1}_{\left\{T < (i+1)2^{-n} \right\}}\mathrm{1}_{\left\{ i2^{-n} \leq T \right\}}B_{i2^{-n}} \right\} ...(9.1)\]
    Por ser \(T\) un tiempo de paro se tendrá por el inciso anterior que \(T\) es  \(\mathcal{F}_{T}\)-medible y, por ende la variable aleatoria \(\mathrm{1}_{\left\{T < (i+1)2^{-n} \right\}}\) también lo será. Por otra parte notese que para cada \(s \geq 0\), \(B_s\mathrm{1_{\left\{s\leq T \right\}}}\) también sera \(\mathcal{F}_{T}\)-medible, ya que si \(A \in \mathbb{B(R)}\) tq \(0 \notin A\), se tendra que:
    
    \[\left\{ B_s\mathrm{1_{\left\{s\leq T \right\}}} \in A \right\}\bigcap \left\{T \leq t \right\} = \left\{\begin{matrix}
      \varnothing & if \ \ t < s\\ 
      \left\{ B_s \in A \right\}\bigcap \left\{s \leq T \leq t \right\}  & if \ \ t \geq s
      \end{matrix}\right. \]
    Lo cual es \(\mathcal{F}_{t}\)-medible en ambos casos (note que \(\left\{s \leq T \leq t \right\} = \left\{T \leq t \right\}\bigcap\left\{T < s \right\}^c\)).
    Por otro lado si \(0 \in A\) entonces \(0 \notin A^c\) y por lo anteriormente demostrado se tendra que \( \left\{ B_s\mathrm{1_{\left\{s\leq T \right\}}} \in A^c \right\} \in  \mathcal{F}_{T}\) y, por ser \(\mathcal{F}_{T}\) una \(\sigma\)-álgebra se tendra que \(\left\{ B_s\mathrm{1_{\left\{s\leq T \right\}}} \in A \right\} = \left\{ B_s\mathrm{1_{\left\{s\leq T \right\}}} \in A^c \right\}^c \in \mathcal{F}_{T} \), y por (1) \(\mathrm{1}_{\left\{T < \infty\right\}}B_T\) es suma, multiplicación y limite de funciones \(\mathcal{F}_{T}\)-medibles por lo cual debe de ser una función \(\mathcal{F}_{T}\)-medible. 
    \end{solucion}
\end{itemize}



\subsection{Procesos Gausianos} Considere \((X,Y)\) normal bivariada con vector de medias \(\mu = (0,0)\) y, con coeficiente de correlación \(\rho\) tal que \(\left | \rho \right |\), demuestra que \(Z = \frac{X}{Y}\) se distribuye como una Cauchy y encuentra sus parámetros. 
\begin{solucion}
Tomemos las siguientes variables aleatorias, \(Z = \frac{X}{Y}\) y \(U = Y\), obtendre la densidad conjunta de estas varaibles aleatorias para posteriormente obtener la densidad marginal de \(Z\). Para esto definimos: 
\[g(x,y) =  \left ( \frac{x}{y},y  \right )\]
notemos que el rango de \(g\) puede escribirse de la siguiente forma: 
\[Ran(g):=\left \{ (x,y) \in \mathbb{R^2} \left |  \right. y \neq 0 \right \}\]
por otro lado la función anterior es invertible y su inversa está dada por la siguiente regla de correspondencia:
\[g^{-1}(z,u) =  \left ( zu,u  \right )\]
Dicha inversa tiene por matriz jacobiana a: 
\[J_{g^{-1}}(z,u) = \begin{bmatrix}
u & z\\ 
0 & 1
\end{bmatrix}\]
Cuyo determinante en valor absoluto es: 
\[\left | J_{g^{-1}}(z,u) \right |  = \left | u \right |\]
Ahora con los elementos previamente calculados sera facil aplicar la formula de transformación de Jacobi para así obtener la distribución conjunta del vector aleatorio \((Z,U)\): 
\[f_{(Z,U)}(z,u)= f_{(X,Y)}(g^{-1}(z,u))\left | J_{g^{-1}}(z,u) \right |\mathbb{I}_{Ran(g)}(z,u)\]
notemos además que al ser \(Ran(g)\) distinto de \(\mathbb{R^2}\) solamente por un punto, y al ser \((Z,U)\) un vector aleatorio con densidad conjunta, entonces podemos omitir esta indicadora y tomar la igualdad anterior sobre todo \(\mathbb{R^{2}}\) 
sustituyendo obtenemos: 
\[f_{(Z,U)}(z,u) = \frac{\left | u \right |e^{-\frac{u^{2}\left ( \frac{z^2}{\sigma_{x}^2} - \frac{2\rho z}{\sigma_{y}\sigma_{x}} + \frac{1}{\sigma_{y}^2} \right ) }{2(1-\rho^2)}}}{2\pi\sigma_{x}\sigma_{y}\sqrt{1-\rho^2}} \]
Notemos ahora que: 
\begin{align*}
     u^{2}\left ( \frac{z^2}{\sigma_{x}^2} - \frac{2\rho z}{\sigma_{y}\sigma_{x}} + \frac{1}{\sigma_{y}^2} \right )  &= u^{2}\left ( \frac{z^2}{\sigma_{x}^2} - \frac{2\rho z}{\sigma_{y}\sigma_{x}} + \frac{\rho^2}{\sigma_{y}^2} + \frac{1 - \rho^2}{\sigma_{y}^2} \right ) \\
       &= u^{2}\left (\left (\frac{z}{\sigma_{x}} - \frac{\rho}{\sigma_{y}} \right )^2 +   \frac{1 - \rho^2}{\sigma_{y}^2}\right )         \\
       &= u^{2}\left (\frac{1}{\sigma_{x}^2} \left (z - \frac{\rho\sigma_{x}}{\sigma_{y}} \right )^2 +   \frac{1 - \rho^2}{\sigma_{y}^2}\right )   
\end{align*}
Sea \(a =\frac{1}{\sigma_{x}^2} \left (z - \frac{\rho\sigma_{x}}{\sigma_{y}} \right )^2 +   \frac{1 - \rho^2}{\sigma_{y}^2} \) y \(b = 2\pi\sigma_{x}\sigma_{y}\sqrt{1-\rho^2}\), entonces se satisface la siguiente igualdad:
\[f_{(Z,U)}(z,u) = \frac{\left | u \right |e^{-\frac{u^{2}a}{2(1-\rho^2)}}}{b}\]
De esta forma se debe satisfacer la siguiente igualdad: 
\begin{align*}
    f_{Z}(z) &= \int_{-\infty}^{\infty} f_{(Z,U)}(z,u)du \\
             &= \int_{-\infty}^{\infty}\frac{\left | u \right |e^{-\frac{u^{2}a}{2(1-\rho^2)}}}{b}du   \\
             &=\frac{2}{b}\int_{0}^{\infty}ue^{-\frac{u^{2}a}{2(1-\rho^2)}}du = (1)
\end{align*}
Sea \(v = \frac{u^{2}a}{2(1-\rho^2)}\) y por ende \(dv = \frac{au}{1 - \rho^2}du\)
\begin{align*}
        (1)    &= \frac{2}{b}\frac{1-\rho^2}{a}\int_{0}^{\infty}e^{-v}dv\\
               &=\frac{2}{b}\frac{1-\rho^2}{a}
\end{align*}
ya que \(\int_{0}^{\infty}e^{-v}dv = 1\) por ser la integral de la densidad de una exponencial con parametro 1 sobre todo su rango. Por lo cual tenemos la siguiente igualdad: 
\begin{align*}
  f_{Z}(z)  &=  \frac{2}{b}\frac{1-\rho^2}{a} \\
            &= \frac{2}{2\pi\sigma_{x}\sigma_{y}\sqrt{1-\rho^2}}\frac{1-\rho^2}{\frac{1}{\sigma_{x}^2} \left (z - \frac{\rho\sigma_{x}}{\sigma_{y}} \right )^2 +   \frac{1 - \rho^2}{\sigma_{y}^2}}  \\
            &=\frac{\sqrt{1-\rho^2}}{\pi\sigma_{x}\sigma_{y}\left ( \frac{1}{\sigma_{x}^2} \left (z - \frac{\rho\sigma_{x}}{\sigma_{y}} \right )^2 +   \frac{1 - \rho^2}{\sigma_{y}^2} \right )}\\
            &= \frac{1}{\pi\frac{\sigma_{x}}{\sigma_{y}}\sqrt{1-\rho^2}\left (  \left (z - \frac{\rho\sigma_{x}}{\sigma_{y}} \right )^2\frac{\sigma_{y}^2}{\sigma_{x}^2(1-\rho^2)} +  1 \right )}
\end{align*}
Sea \(\alpha  = \frac{\rho\sigma_{x}}{\sigma_{y}}\) y \(\beta = \frac{\sigma_{x}}{\sigma_{y}}\sqrt{1-\rho^2}\), entonces: 
\[f_{Z}(z) =  \frac{1}{\pi\beta\left ( \frac{\left ( z - \alpha \right )^2}{\beta^2} + 1\right )}\]
Por lo que se concluye que \(Z \sim Cauchy(\alpha,\beta)\)
\end{solucion}
\subsection{Proceso Ornstein - Uhlenbeck}. Sea \(\left \{ B_{t} \right \}_{t\geq 0}\) un movimiento browniano y \(\alpha > 0\), considere el proceso estocástico: \(X_{t}:= e^{\frac{\alpha t}{2}}B_{e^{\alpha t}} \) 
\begin{itemize}
    \item Calcula \(\mathbf{E}(X_{t})\) y \(cov(X_{t},X_{s})\) para \(s,t \geq 0 \) 
    \begin{solucion}
    Sea \(t \geq 0\) 
    \begin{align*}
        \mathbf{E}(X_{t}) &= \mathbf{E}(e^{\frac{\alpha t}{2}}B_{e^{\alpha t}}) \\ 
                          &=e^{\frac{\alpha t}{2}}\mathbf{E}(B_{e^{\alpha t}}) = (1).
    \end{align*}
    Y como \(\left \{ B_{t} \right \}_{t\geq 0}\) es un movimiento browniano entonces \(B_{e^{\alpha t}} \sim N(0,e^{\alpha t})\), por lo que:
    \[(1) = 0 \]
    Por lo cual para \(t \geq 0 \) 
    \[\mathbf{E}(X_{t}) = 0 \]  
    Por otro lado para \(s,t \geq 0 \): 
    \begin{align*}
        cov(X_{t},X_{s}) &= cov(e^{\frac{\alpha t}{2}}B_{e^{\alpha t}},e^{\frac{\alpha s}{2}}B_{e^{\alpha s}})   \\
                         &= e^{\frac{\alpha t}{2}}e^{\frac{\alpha s}{2}}cov(B_{e^{\alpha t}},B_{e^{\alpha s}})                      \\
                         &=e^{\frac{\alpha(t + s)}{2}}cov(B_{e^{\alpha t}},B_{e^{\alpha s}}) = (2) 
    \end{align*}
    Y una vez más, utilizando el hecho de que \(\left \{ B_{t} \right \}_{t\geq 0}\) es un movimiento browniano se tiene que \(cov(B_{e^{\alpha t}},B_{e^{\alpha s}}) =e^{\alpha t}\wedge e^{\alpha s} \).
    Así: 
    \begin{align*}
         (2) &= e^{\frac{\alpha(t + s)}{2}}e^{\alpha t}\wedge e^{\alpha s} \\ 
             &= e^{\frac{\alpha(t + s)}{2} + \alpha t\wedge\alpha s }
    \end{align*}
    De lo que se concluye para \(s,t \geq 0 \) que:
    \[cov(X_{t},X_{s}) =  e^{\frac{\alpha(t + s)}{2} + \alpha t\wedge\alpha s }\] 
    \end{solucion}
  \item Encuentre la densidad conjunta del vector \((X_{t_{1}},...,X_{t_{n}})\) con \(0 \leq t_{0} < t_{1} < ... < t_{n}\), ¿Es un proceso estocástico gaussiano centrado?
  \begin{solucion}
  Sea \(u \in \mathbb{R^n}\) talque \(u = (u_1,...,u_n)\) y \(U = (X_{t_{1}},...,X_{t_{n}})\) entonces: 
  \begin{align*}
       \varphi_{U}(u)    &=  \mathbf{E}\left ( e^{i\left \langle u,U \right \rangle} \right )         \\
                         &=  \mathbf{E}\left ( e^{i\sum_{j=1}^{n}u_jX_{t_j}} \right )          \\
                         &=  \mathbf{E}\left ( e^{i\sum_{j=1}^{n}u_je^{\frac{\alpha t_j}{2}}B_{e^{\alpha t_j}}} \right ) = (3)         
  \end{align*}
  Sea \(w_j = u_je^{\frac{\alpha t_j}{2}}\) y \(B = (B_{e^{\alpha t_1}},...,B_{e^{\alpha t_n}})\), se sabe que \(B\) es un vector gaussiano con vector de medias \(\mu = 0\) y matriz de covarianzas \((Q)_{ij} = e^{\alpha t_i}\wedge e^{\alpha t_j} = e^{\alpha t_i \wedge\alpha t_j }\), por ser \(\left \{ B_{t} \right \}_{t\geq 0}\) un movimiento browniano, de lo cual se desprende que:
  \begin{align*} 
      (3) &=  \mathbf{E}\left ( e^{i\left \langle w,B \right \rangle} \right )                           \\
          &=   \varphi_{B}(w)                           \\
          &=   e^{-\frac{1}{2}\left \langle w,Qw \right \rangle}...(4)
  \end{align*}
  Notemos ahora que para \(i \in \left \{1,...,n \right \} \): 
  \begin{align*}
      (Qw)_{i1}  &=\sum_{k=1}^{n}Q_{ik}w_k     \\
                 &=\sum_{k=1}^{n}e^{\alpha t_i \wedge\alpha t_k }w_k   \\
                 &=\sum_{k=1}^{n}e^{\alpha t_i \wedge\alpha t_k} u_ke^{\frac{\alpha t_k}{2}}                            \\
                 &=\sum_{k=1}^{n}u_ke^{\alpha t_i \wedge\alpha t_k + \frac{\alpha t_k}{2}}
  \end{align*}
 De lo cual se tiene que:
 \begin{align*}
    \left \langle w,Qw \right \rangle &= \sum_{j=1}^{n}w_j(Qw)_{j1}   \\
                                      &= \sum_{j=1}^{n}u_je^{\frac{\alpha t_j}{2}}\sum_{k=1}^{n}u_ke^{\alpha t_j \wedge\alpha t_k + \frac{\alpha t_k}{2}}   \\
                                      &=\sum_{j=1}^{n}u_j\sum_{k=1}^{n}u_ke^{\alpha t_j \wedge\alpha t_k + \frac{\alpha (t_k + t_j)}{2}}    \\
                                      &=\sum_{j=1}^{n}u_j\sum_{k=1}^{n}u_kcov(X_{t_{k}},X_{t_{j}})= (5) 
 \end{align*}
 Sea \(Q^{'} \in M_{nxn}\) tal que \((Q^{'})_{ij} = cov(X_{t_{i}},X_{t_{j}})\)
  \end{solucion}
  \begin{align*}
                (5) &=   \sum_{j=1}^{n}u_j\sum_{k=1}^{n}u_k(Q^{'})_{jk}   \\
                    &= \left \langle u,Q^{'}u \right \rangle 
  \end{align*}
  Por lo cual se puede concluir que: 
  \[\left \langle w,Qw \right \rangle = \left \langle u,Q^{'}u \right \rangle \]
  Y por \((4)\) se tiene que: 
  \[ \varphi_{U}(u) = e^{-\frac{1}{2}\left \langle w,Qw \right \rangle} = e^{-\frac{1}{2}\left \langle u,Q^{'}u \right \rangle} \]
  De lo anterior se concluye que \(U \sim N(\mathbf{0},Q^{'})\) cuya densidad esta dada por: 
  \[f_{U}(u) = \frac{e^{-\frac{1}{2}\left \langle u,(Q^{'})^{-1}u \right \rangle}}{(2\pi)^{\frac{n}{2}}\sqrt{detQ^{'}}}\]
\end{itemize}

\subsection{Esperanza condicional Browniana} Sea \(\{B_t\}_{t\geq 0}\) un  movimiento browniano en \(\mathbb{R}\). Encuentra la distribución condicional de \((B_s,B_t)\) con 
\(0\leq s \leq t \leq 1\) condicionado en \(\{B_1 = 0\}\) y después cálcule \(\mathbb{E}(B_sB_t|B_1 = 0)\).
\begin{solucion}
Para obtener la densidad del vector \((B_s,B_t)\) condicional al evento \(\{B_1 = 0\}\) se utilizara la siguiente fórmula de probabilidad elemental dado que el evento
\(\{B_1 = 0\}\) tiene medida 0.

\begin{equation}\label{3.1}
    \mathbb{P}((B_s,B_t) \in A | \{B_1 = 0\}) =\frac{\int_{(x,y)\in A}
                       f_{(B_s,B_t,B_1)}(x,y,0)dxdy}{\int_{(x,y)\in\mathbb{R}^2}
                       f_{(B_s,B_t,B_1)}(x,y,0)dxdy}
\end{equation}
como \(\{B_t\}_{t\geq 0}\) es un movimiento browniano, entonces \(B_t\) es un proceso gaussiano centrado por lo que el vector \((B_s,B_t,B_1)\)
\begin{enumerate}
    \item Es un vector gaussiano centrado
    \item Tiene por matriz de covarianzas al minimo entre los tiempos 
\end{enumerate}
Estó último se puede ver como que \((B_s,B_t,B_1)\sim N(\mu,Q)\) donde: 
\begin{align*}
    \mu &= (0,0,0)\\
     Q  &= \begin{pmatrix}
s\wedge s & s\wedge t & s\wedge 1\\ 
t\wedge s & t\wedge t & t\wedge 1\\ 
1\wedge s & 1\wedge t & 1\wedge 1
\end{pmatrix}
\end{align*}

debido a que \(0\leq s \leq t \leq 1\) entonces la matriz \(Q\) puede ser escrita como sigue: 
\[
Q  = \begin{pmatrix}
s & s & s\\ 
s & t & t\\ 
s & t & 1
\end{pmatrix}
\]

Por otro lado sabemos que la densidad conjunta de un vector gaussiano trivariado y centrado estará dada por la siguiente
expresión (de acuerdo al corolario 2.1.10): 

\begin{equation}\label{3.2}
    f_{(B_s,B_t,B_1)}(x,y,0) = \frac{e^{-\frac{1}{2}\left<z,Q^{-1}z \right>}}{\sqrt{det(Q)}(2\pi)^{\frac{3}{2}}}
\end{equation}
donde:
\[z = (x,y,0)\]
Por lo cual procederemos a calcular \(detQ\) y \(Q^{-1}\), notemos primeramente que: 

\begin{align*}
    detQ &= \begin{vmatrix}
s & s & s\\ 
s & t & t\\ 
s & t & 1
\end{vmatrix}  \\
         &=  \begin{vmatrix}
s & s & s\\ 
0 & t-s & t-s\\ 
0 & t-s & 1-s
\end{vmatrix} \\
         &= \begin{vmatrix}
s & s & s\\ 
0 & t-s & t-s\\ 
0 & 0 & 1-t
\end{vmatrix}\\
         &= s(t-s)(1-t) 
\end{align*}
Las primeras dos igualdades se dan porque las matrices anteriores son equivalentes por filas. la última se da debido a que el 
determinate de una matriz triangular inferior o superior es el producto de los elementos de su diagonal. Así: 
\begin{equation}\label{3.3}
    det(Q) =  s(t-s)(1-t) 
\end{equation}

Por otro lado después de realizar
algunos cálculos se obtiene la matriz inversa de \(Q\), la cual está dada por la siguiente expresión: 
\[
    Q^{-1} = \begin{pmatrix}
\frac{t}{s(t-s)} &-\frac{1}{t-s}  &0 \\ 
-\frac{1}{t-s} &\frac{1-s}{(1-t)(t-s)}  &-\frac{1}{1-t} \\ 
0 &-\frac{1}{1-t}  &\frac{1}{1-t} 
\end{pmatrix}
\]
Por último notemos que si \(z =  (x,y,0)\)entonces se tendra que: 
\begin{align*}
     Q^{-1}z   &= \begin{pmatrix}
\frac{t}{s(t-s)} &-\frac{1}{t-s}  &0 \\ 
-\frac{1}{t-s} &\frac{1-s}{(1-t)(t-s)}  &-\frac{1}{1-t} \\ 
0 &-\frac{1}{1-t}  &\frac{1}{1-t} 
\end{pmatrix}\begin{pmatrix}
x\\ 
y\\ 
0
\end{pmatrix}\\
              &= \begin{pmatrix}
\frac{t}{s(t-s)}x - \frac{1}{t-s}y\\ 
-\frac{1}{(t-s)}x + \frac{1-s}{(t-s)(1-t)}y\\ 
- \frac{1}{1-t}y
\end{pmatrix}
\end{align*}
Por lo que: 
\begin{align*}
    \left<z,Q^{-1}z\right> &= x\left(\frac{t}{s(t-s)}x - \frac{1}{t-s}y\right) + y\left(-\frac{1}{(t-s)}x + \frac{1-s}{(t-s)(1-t)}y\right)\\ 
                           &= \frac{t}{s(t-s)}x^2 - \frac{2}{t-s}xy   + \frac{1-s}{(t-s)(1-t)}y^2\\
                           &= \frac{1}{t-s}\left[\frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 \right]
\end{align*}
Hemos obtenido por ende la siguiente ecuación:
\begin{equation}\label{3.4}
     \left<z,Q^{-1}z\right> = \frac{1}{t-s}\left[\frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 \right]
\end{equation}

Por (\ref{3.2}),(\ref{3.3}) y (\ref{3.4}) se tiene lo siguiente: 

\begin{equation}\label{3.5}
    f_{(B_s,B_t,B_1)}(x,y,0) = \frac{e^{-\frac{1}{2(t-s)}\left[\frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 \right]}}{\sqrt{s(t-s)(1-t)}(2\pi)^{\frac{3}{2}}}
\end{equation}
\begin{align*}
     \frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 &= \frac{t}{s}x^2 - 2xy  + \frac{s}{t}y^2 - \frac{s}{t}y^2 + \frac{1-s}{1-t}y^2\\
                                                 &= \frac{t}{s}x^2 - 2xy  + \frac{s}{t}y^2 +  \frac{t-s}{t(1-t)}y^2\\
                                                 &= \frac{t}{s}\left(x -  \frac{s}{t}y\right)^2 + \frac{t-s}{t(1-t)}y^2
\end{align*}
Lo que da paso a la ecuacion:  
\begin{equation}\label{3.6}
     \frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 =\frac{t}{s}\left(x -  \frac{s}{t}y\right)^2 + \frac{t-s}{t(1-t)}y^2
\end{equation}

De este modo (\ref{3.5}) puede ser escrita como: 
\begin{equation}\label{3.7}
    f_{(B_s,B_t,B_1)}(x,y,0) = \frac{e^{-\frac{1}{2(t-s)}\left[\frac{t}{s}\left(x -  \frac{s}{t}y\right)^2 + \frac{t-s}{t(1-t)}y^2 \right]}}{\sqrt{s(t-s)(1-t)}(2\pi)^{\frac{3}{2}}}
    =\frac{e^{-\frac{t}{2s(t-s)}\left(x -  \frac{s}{t}y\right)^2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{s(t-s)(1-t)}(2\pi)^{\frac{3}{2}}}
\end{equation}
Ahora por la fórmula (\ref{3.1}) tenemos que: 
\begin{equation}\label{3.8}
 F_{B_sB_t|B_1}(r,z|0) = \mathbb{P}((B_s,B_t) \in (-\infty,r]\times(-\infty,z] | \{B_1 = 0\}) =\frac{\int_{-\infty}^{r}\int_{-\infty}^{z}
                       f_{(B_s,B_t,B_1)}(x,y,0)dydx}{\int_{(x,y)\in\mathbb{R}^2}
                       f_{(B_s,B_t,B_1)}(x,y,0)dydx}  
\end{equation}
Procedamos al calculo de la constante que se encuentra como divisor en el cociente anterior, usaremos la expreción (\ref{3.7}) para simplificar los cálculos
 \begin{equation*}
    \int_{(x,y)\in\mathbb{R}^2}f_{(B_s,B_t,B_1)}(x,y,0)dydx &= \int_{-\infty}^{\infty}\left\{\int_{-\infty}^{\infty}\frac{e^{-\frac{t}{2s(t-s)}\left(x -  \frac{s}{t}y\right)^2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{s(t-s)(1-t)}(2\pi)^{\frac{3}{2}}}dx\right\}dy = * 
 \end{equation*}
Calculando primero la integral respecto a \(x\) obtenemos: 
\begin{align*}
    * &= \int_{-\infty}^{\infty}\frac{e^{-\frac{t}{2s(t-s)}\left(x -  \frac{s}{t}y\right)^2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{s(t-s)(1-t)}(2\pi)^{\frac{3}{2}}}dx  \\
      &= \frac{e^{-\frac{y^2}{2t(1-t)}}}{(2\pi)\sqrt{t(1-t)}}\int_{-\infty}^{\infty}\frac{e^{-\frac{1}{2\frac{s(t-s)}{t}}\left(x -  \frac{s}{t}y\right)^2}}{\sqrt{2\pi}\sqrt{\frac{s(t-s)}{t}}}dx...(a)  \\
      &=\frac{e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{2\pi}\sqrt{t(1-t)}}
\end{align*}
En la linea (a) se agrego un \(1 = \frac{\sqrt{t}}{\sqrt{t}}\) y, se uso el hecho de que el integrando respecto a x, es la función densidad de probabilidades
de una \(N\left(\frac{s}{t}y,\frac{s(t-s)}{t}\right)\)
Continuando con el cálculo anterior: 
\begin{equation*}\label{3.9}
    *= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}\frac{e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{t(1-t)}}dy = \frac{1}{\sqrt{2\pi}}
\end{equation*}
En la ecuación anterior se uso el hecho de que el integrando es la función densidad de probabilidad de una \(N(0,t(1-t))\)
Combinando (\ref{3.8}) y (\ref{3.9}) obtenemos: 
\begin{align*}
     F_{B_sB_t|B_1}(r,z|0) &= \frac{\int_{-\infty}^{r}\int_{-\infty}^{z}
                       f_{(B_s,B_t,B_1)}(x,y,0)dydx}{\frac{1}{\sqrt{2\pi}}}\\
                           &=\int_{-\infty}^{r}\int_{-\infty}^{z}\sqrt{2\pi}f_{(B_s,B_t,B_1)}(x,y,0)dydx
\end{align*}
Notese qué esta última igualdad implica que: 
\begin{align*}
    f_{B_sB_t|B_1}(x,y|0) &= \sqrt{2\pi}f_{(B_s,B_t,B_1)}(x,y,0)\\
                          &=  \sqrt{2\pi}\frac{e^{-\frac{1}{2(t-s)}\left[\frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 \right]}}{\sqrt{s(t-s)(1-t)}(2\pi)^{\frac{3}{2}}}\\
                          &= \frac{e^{-\frac{1}{2(t-s)}\left[\frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 \right]}}{\sqrt{s(t-s)(1-t)}(2\pi)}
\end{align*}
Por lo cual hemos encontrado la densidad buscada: 
\begin{equation}\label{3.10}
    f_{B_sB_t|B_1}(x,y|0) = \frac{e^{-\frac{1}{2(t-s)}\left[\frac{t}{s}x^2 - 2xy   + \frac{1-s}{1-t}y^2 \right]}}{\sqrt{s(t-s)(1-t)}(2\pi)}
\end{equation}
y por la igualdad (\ref{3.6}) se obtiene también: 
\begin{equation}\label{3.11}
    f_{B_sB_t|B_1}(x,y|0) = \frac{e^{-\frac{t}{2s(t-s)}\left(x -  \frac{s}{t}y\right)^2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{s(t-s)(1-t)}(2\pi)}
\end{equation}
Por último se realizara el cálculo de la esperanza solicitada, note que por (\ref{3.11}) se tiene que: 

\begin{align*}
    \mathbb{E}(B_sB_t|B_1 = 0) &=\int_{\mathbb{R}^2}xyf_{B_sB_t|B_1}(x,y|0)dxdy \\ 
                               &=\int_{-\infty}^{\infty}\left\{\int_{-\infty}^{\infty}\frac{xye^{-\frac{t}{2s(t-s)}\left(x -  \frac{s}{t}y\right)^2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{s(t-s)(1-t)}(2\pi)}dx\right\}dy=*
\end{align*}
Calculando primeramente la integral respecto a x obtenemos: 
\begin{align*}
    \int_{-\infty}^{\infty}\frac{xye^{-\frac{t}{2s(t-s)}\left(x -  \frac{s}{t}y\right)^2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{s(t-s)(1-t)}(2\pi)}dx 
    &=\frac{ye^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{t(1-t)}}\int_{-\infty}^{\infty}\frac{e^{-\frac{1}{2\frac{s(t-s)}{t}}\left(x -  \frac{s}{t}y\right)^2}}{\sqrt{2\pi}\sqrt{\frac{s(t-s)}{t}}}dx...(a)\\
    &=\frac{ye^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{t(1-t)}}\left(\frac{sy}{t}\right)\\
    &=\left(\frac{s}{t}\right)\frac{y^{2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{t(1-t)}}
\end{align*}
En la linea (a) se agrego un \(1 = \frac{\sqrt{t}}{\sqrt{t}}\) y, se uso el hecho de que el integrando respecto a x, es la función densidad de probabilidades
de una \(N\left(\frac{s}{t}y,\frac{s(t-s)}{t}\right)\)
Continuando cn el cálculo anterior: 
\begin{align*}
    *&= \int_{-\infty}^{\infty} \left(\frac{s}{t}\right)\frac{y^{2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{t(1-t)}} dy \\
     &=  \left(\frac{s}{t}\right)\int_{-\infty}^{\infty}\frac{y^{2}e^{-\frac{y^2}{2t(1-t)}}}{\sqrt{2\pi}\sqrt{t(1-t)}} dy...(b)\\
     &=  \left(\frac{s}{t}\right)t(1-t) = s(1-t) 
\end{align*}

En la linea (b) se uso el hecho de que el integrando es la función densidad de probabilidad de una \(N(0,t(1-t))\). 
¨Por tanto
\[
 \mathbb{E}(B_sB_t|B_1 = 0) = s(1-t)
\]
\end{solucion}

\subsection{Movimiento Browniano en \(\mathbb{R}^n\)} Sea \(B_t = (B_{t}^{1},\hdots,B_{t}^{n})\) un MB en \(\mathbb{R}^n\), sea \(x\in\mathbb{R}^n\) talque 
\(\left \| x \right \|_2 = 1\). Demuestre que \(\{X_t\}_{t\geq0}:= \{\left \langle x,B_t \right \rangle\}_{t\geq0}\) es un 
MB.
\begin{enumerate}
    \item \(X_0 \ \myeq \ 0\) 
    \begin{solucion}
     \(X_0 = \left \langle x,B_0 \right \rangle \ \myeq \ \left \langle x,0 \right \rangle = 0\) 
    \end{solucion}
    \item Sean \(0\leq t_0 < t_1 < \hdots < t_p\) y sean \(r \neq  k\), \(r,k \in \{1,\hdots,p\}\) p.d 
    \[X_{t_r} - X_{t_{r-1}} \mathbb{\perp} X_{t_k} - X_{t_{k-1}} \]
    \begin{solucion}
       Notese que se satisfacen las siguientes igualdades: 
       \begin{equation*}
        X_{t_{r}} - X_{t_{r-1}}  = \sum_{j=1}^{n}x_j\left(B_{t_r}^{j} - B_{t_{r-1}}^{j} \right)
       \end{equation*}
       \begin{equation*}
         X_{t_{k}} - X_{t_{k-1}}  = \sum_{i=1}^{n}x_i\left(B_{t_k}^{i} - B_{t_{k-1}}^{i} \right)   
       \end{equation*}
     Cuando \(i\neq j\) entonces \(B_{t_r}^{j} - B_{t_{r-1}}^{j} \perp B_{t_k}^{i} - B_{t_{k-1}}^{i}\), por ser procesos independientes y, cuando \(i = j\)
      \(B_{t_r}^{j} - B_t_{r-1}^{j} \perp B_{t_k}^{i} - B_{t_{k-1}}^{i}\) porque \(\{B_{t}^{j}\}_{t\geq0}\) es un M.B. 
      Así 
      \[\sum_{j=1}^{n}x_j\left(B_{t_r}^{j} - B_{t_{r-1}}^{j} \right) \perp \sum_{i=1}^{n}x_i\left(B_{t_k}^{i} - B_{t_{k-1}}^{i} \right) \]
      Es decir: 
       \[X_{t_r} - X_{t_{r-1}} \mathbb{\perp} X_{t_k} - X_{t_{k-1}} \]
    \end{solucion}
    \item Sea \(s \leq t\) p.d. \(X_t - X_s \sim N(0,t-s)\)
   
    \begin{solucion}
     Note que
    \[
    X_t - X_s = \left \langle x,B_t - B_s \right \rangle = \sum_{k=1}^{n}x_k\left(B_{t}^{k} - B_{s}^{k}\right)
    \]
    Sea \(Y_k = B_{t}^{k} - B_{s}^{k}\), para \(k = 1,\hdots,n\) observemos que \(Y_1 \perp Y_2 \hdots  \perp Y_k\) y \(Y_k\sim N(0,t-s)\),
    de lo cual se sigue: 
    \begin{equation}\label{1.3.1}
      \sum_{k=1}^{n}x_kY_k \sim N(0,x_{1}^{2}(t-s) + \hdots x_{r}^{2}(t-s))
    \end{equation}
    y como \(\left \| x \right \|_2 = 1\) entonces \(\sum_{k=1}^{n}x_{k}^{2} = 1\) y, por esto y por (\ref{1.3.1}) se tiene que:
    \[X_t - X_s \sim N(0,t-s)\]
    Lo que implica también que:
    \[X_t - X_s \sim X_{t+h} - X_{s+h}, \ \ \forall 0\leq s\leq t, \ \ t\geq -s \]
    \end{solucion}
    \item Trayectorias continuas: 
    \begin{solucion}
    Sabemos que \(t \rightarrow \alpha_kB_{t}^{k}(\omega)\) es continua por ser MB luego  \(t \rightarrow\sum_{k=1}^{n}\alpha_kB_{t}^{k}(\omega)\)
    lo es por ser suma de continuas y, en particular: 
    \(t \rightarrow\sum_{k=1}^{n}x_kB_{t}^{k}(\omega) = \left \langle x,B_t \right \rangle\) es continua.
    \end{solucion}
\end{enumerate}
\subsection{Aplicaciones de la desigualdad de Doob para el Browniano}
Sea \(B_t\) un movimiento browniano y defina \(S_t=sup_{s\leq t}B_s\) y \(S^*_t=sup_{s\leq t}\).
Resuelva lo siguiente:

\begin{enumerate}
\item Sea $\alpha > 0$ demuestre que 
        \[M_t= e^{\alpha B_t - \alpha^2 t/2}\]
     es una martingala con trayectorias continuas

\begin{itemize}

\item 
\[\mathbb{E}[\left | M_t \right |]=\mathbb{E}\left [ \left | e^{\alpha B_t - \alpha^2 t/2} \right | \right ]= \mathbb{E}\left [ e^{\alpha B_t - \alpha^2 t/2} \right ]\]
\[=\frac{\mathbb{E}\left [ e^{\alpha B_t} \right ]}{e^{\alpha^2 t/2}} = 1 <\infty\]

\item \(M_t\) es \(\mathbb{F}_t\)-medible
 \item 
 \[\mathbb{E}[ M_t | F_s ]=\mathbb{E} [ e^{\alpha B_t - \alpha^2 t/2} | F_s] \]
\[=\mathbb{E} [ e^{\alpha (B_t-B_s)- \alpha^2 t/2 + \alpha B_s} | F_s] = e^{\alpha B_s- \alpha^2 t/2 } \mathbb{E} [ e^{\alpha (B_t-B_s)} | F_s] \]  
\[= e^{\alpha B_s- \alpha^2 t/2 } e^{\alpha^2 (t- s)/2 }= e^{\alpha B_s- \alpha^2 s/2 } \]  

\end{itemize}
\item   Al aplicarle la desigualdad maximal de Doob, deduzca que   
    \[\mathbb{P}(S_t\geq at) \leq e^{-\alpha at+\alpha^2t/2}\]
Sean los eventos
\[\left \{ Sup_{s\leq t}B_s\geq at \right \}= \left \{ Sup_{ s \leq t}\;e^{\alpha B_s}\geq e^{\alpha at} \right \}\]
Entonces 
\[\mathbb{P}(S_t\geq at)=\mathbb{P}( Sup_{ s \leq t}B_s\geq at )=\mathbb{P}( Sup_{s \leq t}\;e^{\alpha B_s}\geq e^{\alpha at})\]
\[\leq \frac{\mathbb{E}[e^{\alpha B_t}]}{ e^{\alpha at} }=e^{(\frac{1}{2}\alpha^2t-\alpha at)}\]
 \[\therefore \mathbb{P}(S_t\geq at) \leq e^{-\alpha at+\alpha^2t/2}\]

\item Minimice sobre $\alpha$ y deduzca que    \[\mathbb{P}(S_t\geq at) \leq e^{-\alpha^2t/2}\]
Minimizaremos \(e^{-\alpha at+\alpha^2t/2}\)
\[\frac{\partial }{\partial \alpha } (e^{-\alpha at+\alpha^2t/2}) = (-at+\alpha t) e^{-\alpha at+\alpha^2t/2}\]
Igulando a cero
\begin{align*} (-at+\alpha t) e^{-\alpha at+\alpha^2t/2} &= 0\\ -at+\alpha t &= 0\\ \alpha &= t \end{align*}
Sustituyendo el valor de $\alpha$
 \[\therefore \mathbb{P}(S_t\geq at) \leq e^{-\alpha^2t/2}\]
 
\end{enumerate}
\subsection{Puente Browniano} Definimos $X_t := B_t-tB_1$ para $t \in [0,1]$
\begin{itemize}
\item Calcula $\mathbb{E}(X_t)$ y $Cov(X_s,X_t)$ con $1 \leq s, t \geq 0$.
  \begin{solucion}
  
Por definición, este proceso comienza en 0 y termina de nuevo allí.\\ 
    Recordemos que un MB tiene esperanza igual 0, así
 \begin{equation*}
     \mathbb{E}[X_t]= \mathbb{E}[B_t-tB_1]= 0-t*0=0
 \end{equation*}
 Para la $Cov(X_s,X_t)$, tomamos $\mathbb{E}[B_tB_s]= t \wedge s$, si $(0 \leq s, t \leq 1)$ entonces:
 \begin{align*}
     \mathbb{E}[B_tB_s]&= \mathbb{E}[(B_t-tB_1)(B_s-sB_1)]\\
     &= \mathbb{E}[B_tB_s]-\mathbb{E}[tB_1B_s]-\mathbb{E}[sB_1B_t]+\mathbb{E}[stB_1^2]\\
     &= t \wedge s -t(1 \wedge s)-s(t \wedge 1)+st\\
     &= t \wedge s -2st+st\\
     &= t \wedge s-st
 \end{align*}
 Un MB tiene covarianza $s\wedge t$ y en este caso $s\wedge 1=s.$
  \end{solucion}
\end{itemize}

\subsection{Curtosis normal} La $\textsl{curtosis}$ de una variable aleatoria es definida para ser la razón de su cuarto momento central al cuadrado de su varianza. Para una variable aleatoria normal, la curtosis es 3. Este ejercicio verifica este hecho. $\par$

Sea X una variable aleatoria normal con media $\mu$, así que $X-\mu$ tiene media cero. Sea $\sigma^{2}$ la varianza de $X$, la que es también varianza de $X-\mu$. Previamente, se calculó que la función que genera los momentos de $X-\mu$ que está dada por $\varphi(u)=\mathbb{E}e^{u(X-\mu)}=e^{\frac{1}{2}u^{2}\sigma^{2}}$, donde $u$ es una variable real. Diferenciando esta función con respecto a $u$, se obtiene
\begin{equation*}
    \varphi'(u)=\mathbb{E}\left[ (X-\mu)e^{u(X-\mu)} \right]=\sigma^{2}ue^{\frac{1}{2}\sigma^{2}u^{2}}
\end{equation*}
\hspace{0.3cm} y en particular, $\varphi'(0)=\mathbb{E}(X-\mu)=0$. Diferenciando nuevamente, se obtiene que
\begin{equation*}
    \varphi''(u)=\mathbb{E}\left[ (X-\mu)^{2}e^{u(X-\mu)} \right]=(\sigma^{2}+\sigma^{4}u^{2})e^{\frac{1}{2}\sigma^{2}u^{2}}
\end{equation*}
\hspace{0.3cm} y en particular, $\varphi''(0)=\mathbb{E}[(X-\mu)^{2}]=\sigma^{2}$. Diferenciar dos veces más y obtener la formula de curtosis normal $\mathbb{E}[(X-\mu)^{4}]=3\sigma^{4}$.

\begin{solucion}

\begin{align*}
    \varphi^{(3)}(u)&=2\sigma^{4}ue^{\frac{1}{2}\sigma^{2}u^{2}}+(\sigma^{2}+\sigma^{4}u^{2})\sigma^{2}ue^{\frac{1}{2}\sigma^{2}u^{2}} \\
    &=e^{\frac{1}{2}\sigma^{2}u^{2}}(3\sigma^{4}u+\sigma^{6}u^{3}) \\
    \varphi^{(4)}(u)&=\sigma^{2}ue^{\frac{1}{2}\sigma^{2}u^{2}}(3\sigma^{4}u+\sigma^{6}u^{3})+e^{\frac{1}{2}\sigma^{2}u^{2}}(3\sigma^{4}+3\sigma^{6}u^{2})\\
    &=e^{\frac{1}{2}\sigma^{2}u^{2}}(6\sigma^{6}u^{2}+\sigma^{8}u^{4}+3\sigma^{4}).
\end{align*}
\hspace{0.3cm} Finalmente, se tiene que
\begin{equation*}
    \mathbb{E}[(X-\mu)^{4}]=\varphi^{(4)}(0)=3\sigma^{4}
\end{equation*}
\end{solucion}
\subsection{Otras variaciones del Movimiento Browniano}. Un teorema afirma que si \(T\) es un número positivo y se elige una partición $\Pi$ con puntos $0=t_{0}<t_{1}<t_{2}<...<t_{n}=T$, entonces cuando el número $n$ de puntos de la partición se aproxima al infinito y la longitud del subintervalo más largo $||\Pi||$ aproxima a cero, la variación cuadrática muestral.
\begin{equation*}
    \sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{2}
\end{equation*}
\hspace{0.3cm} aproxima a $T$, para casi todas las trayectorias del movimiento Browniano B. Anteriormente, se ha mostrado que $\sum\nolimits_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})(t_{j+1}-t_{j})$ y $\sum\nolimits_{j=0}^{n-1}(t_{j+1}-t_{j})^{2}$ tienen límite cero. Estos hechos se resumen por las reglas de multiplicación.
\begin{align*}
    dB_tdB_t&=dt \\
    dB_tdt&=0 \\
    dtdt&=0
\end{align*}
\hspace{0.3cm} (i) Muestre que a medida que el número $m$ de puntos de partición se aproxima al infinito y la longitud del subintervalo más largo se aproxima a cero, la primera variación de la muestra es
\begin{equation*}
    \sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|
\end{equation*}
\hspace{0.3cm} aproxima a $\infty$ para casi todas las trayectorias del Movimiento Browniano.(Hint:
\begin{equation*}
        \sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{2}
    \leq \max_{0\leq k\leq n-1}|B_{t_{k+1}}-B_{t_{t_{k}}}|\sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|.)
\end{equation*}
\hspace{0.3cm} (ii) Muestre que a medida que el número $n$ de puntos de partición se aproxima al infinito y la longitud del subintervalo más largo se aproxima a cero, la variación cúbica de muestra
\begin{equation*}
    \sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|^{3}
\end{equation*}
\hspace{0.3cm} aproxima a cero para cada todas las trayectorias del Movimiento Browniano B.

\begin{solucion}
(i) Asuma que existe $A \in \mathcal{F}$, tal que $\mathbb{P}(A)>0$ y para cada $\omega \in A$,
\begin{equation*}
    \limsup_{n}\sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|(\omega)<\infty
\end{equation*}
\hspace{0.3cm} Entonces, para cada $\omega \in A$,
\begin{align*}
    \sum_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{2}(\omega) &\leq \max_{0\leq k\leq n-1} |B_{t_{k+1}}-B_{t_{k}}|(\omega)\sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|(\omega)\\
    &\leq \max_{0\leq k\leq n-1} |B_{t_{k+1}}-B_{t_{k}}|(\omega)\limsup_{n}\sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|(\omega)\\
    &\rightarrow 0
\end{align*}
\hspace{0.3cm} ya que por continuidad uniforme de funciones continuas en un intervalo cerrado
\begin{equation*}
    \lim_{n\rightarrow \infty} \max_{0\leq k\leq n-1}|B_{t_{k+1}}-B_{t_{k}}|(\omega)=0
\end{equation*}
\hspace{0.3cm} Lo cual es una contradicción con $\sum\nolimits_{j=0}^{n-1}(B_{t_{j+1}}-B_{t_{j}})^{2}=T$ casi seguramente.
\end{solucion}

\begin{solucion}
 (ii) Por un argumento similar al que se utilizó en (i).
\begin{equation*}
    \sum_{j=0}^{n-1}|B_{t_{j+1}}-B_{t_{j}}|^{3}
\end{equation*}
\hspace{0.3cm} aproxima a cero para casi todas las trayectorias del Movimiento Browniano W.
\end{solucion}


\subsection{Fórmula de Black-Scholes-Merton} Sean la tasa de interés $r$ y la volatilidad $\sigma>0$ constantes,y
\begin{equation*}
    S_t=S_0e^{(r-\frac{1}{2}\sigma^{2})t+\sigma B_t}
\end{equation*}

\hspace{0.3cm} un Movimiento Browniano Geométrico con tasa de rendimiento media $r$, donde el precio inicial de la acción $S_0$ es positivo. Sea $K$ una constante positiva. Muestra que, para $T>0$,
\begin{equation*}
    \mathbb{E}\left[ e^{-rT}(S_T-K)^{+}\right]=S_0N(d_{+}(T,S_0))-Ke^{-rT}N(d_{-}(T,S_0)),
\end{equation*}
\hspace{0.3cm} donde
\begin{equation*}
    d_{\pm}(T,S_0)=\frac{1}{\sigma\sqrt{T}}\left[ log\frac{S_0}{K}+\left( r \pm \frac{\sigma^{2}}{2}\right)T \right]
\end{equation*}
\hspace{0.3cm} y $N$ es la función de distribución normal estándar acumulativa
\begin{equation*}
    N(y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e^{-\frac{1}{2}z^{2}}dz=\frac{1}{\sqrt{2\pi}}\int_{-y}^{\infty}e^{-\frac{1}{2}z^{2}}dz.
\end{equation*}
\begin{solucion}
 \begin{equation*}
     \mathbb{E}[e^{-rT}(S_{T}-K)^{+}]=
 \end{equation*}
 \begin{align*}
     &=e^{-rT}\int_{\frac{1}{\sigma}\left[ln\frac{K}{S_{0}-(r-\frac{1}{2}\sigma^{2})T}\right]}^{\infty}\left(S_{0}e^{(r-\frac{1}{2}\sigma^2)T+\sigma x}-K\right)\frac{e^{-\frac{x^{2}}{2T}}}{\sqrt{2\pi T}}dx\\
     &=e^{-rT}\int_{\frac{1}{\sigma\sqrt{T}}\left[ln\frac{K}{S_{0}-(r-\frac{1}{2}\sigma^{2})T}\right]}^{\infty}\left(S_{0}e^{(r-\frac{1}{2}\sigma^2)T+\sigma \sqrt{T}y}-K\right)\frac{e^{-\frac{y^{2}}{2T}}}{\sqrt{2\pi}}dy\\
     &=S_{0}e^{\frac{1}{2}\sigma^2T}\int_{\frac{1}{\sigma\sqrt{T}}\left[ln\frac{K}{S_{0}-(r-\frac{1}{2}\sigma^{2})T}\right]}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}+\sigma\sqrt{T}y}dy-Ke^{-rT}\int_{\frac{1}{\sigma\sqrt{T}}\left[ln\frac{K}{S_{0}-(r-\frac{1}{2}\sigma^{2})T}\right]}^{\infty}\frac{1}{\sqrt{2\pi}e^{-\frac{y^2}{2}}}dy \\
     &=S_{0}\int_{\frac{1}{\sigma\sqrt{T}}\left[ln\frac{K}{S_{0}-(r-\frac{1}{2}\sigma^{2})T}\right]-\sigma\sqrt{T}}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{\xi^{2}}{2}}d\xi-Ke^{-rT}N\left( \frac{1}{\sigma\sqrt{T}}\left(ln\frac{S_{0}}{K}+(r-\frac{1}{2}\sigma^{2})T \right)\right)\\
     &=Ke^{-rT}N(d_{+}(T,S_{0}))-Ke^{-rT}N(d_{-}(T,S_{0}))
 \end{align*}
\end{solucion}

\subsection{Propiedad de Markov del Browniano Geométrico}

Sea $B_t$ un movimiento Browniano y sea $\mathcal{F}_t$, $t\geq0$ una filtración asociada. $\par$

(i) Para $\mu \in \mathbb{R}$, considere el Movimiento Browniano con deriva $\mu$.
\begin{equation*}
    X_t=\mu t+B_t
\end{equation*}
\hspace{0.3cm} Muestre que para cualquier función Borel-medible $f(y)$ y para cualesquiera $0\leq s<t$, la función
\begin{equation*}
    g(x)=\frac{1}{\sqrt{2\pi(t-s)}}\int_{-\infty}^{\infty}f(y)exp\left\{ -\frac{-(y-x-\mu(t-s))^{2}}{2(t-s)}\right\}dy
\end{equation*}
\hspace{0.3cm} satisface $\mathbb{E}[f(X_t)|\mathcal{F}_s]=g(X_s)$, y por lo tanto $X$ tiene la propiedad de Markov. Además, $g(x)$ se puede rescribir como $\int\nolimits_{-\infty}^{\infty}f(y)p(\tau,x,y)dy$, donde $\tau=t-s$ y
\begin{equation*}
    p(\tau,x,y)=\frac{1}{\sqrt{2\pi\tau}}exp\left\{ -\frac{(y-x-\mu\tau)^{2}}{2\tau}\right\}
\end{equation*}
\hspace{0.3cm} es la densidad de transición para el movimiento Browniano con deriva $\mu$.$\par$

(ii) Para $v \in \mathbb{R}$ y $\sigma>0$, considere el Movimiento Browniano Geométrico
\begin{align*}
    S_t&=S_0e^{\sigma B_t+vt} \\
    \tau&=t-s
\end{align*}

\begin{equation*}
    p(\tau,x,y)=\frac{1}{\sigma y\sqrt{2\pi\tau}}exp\left\{ -\frac{(log\frac{x}{y}-v\tau)^{2}}{2 \sigma^{2} \tau}\right\}.
\end{equation*}
\hspace{0.3cm} Muestre que para cualquier función $f(y)$ Borel-medible y para cualesquiera $0\leq s<t$ la función
\begin{equation*}
    g(x)=\int_{0}^{\infty}f(y)p(\tau,x,y)dy
\end{equation*}
\hspace{0.3cm}satisface $\mathbb{E}[f(S_t)|\mathcal{F}_s]=g(S_s)$ y por lo tanto, $S$ tiene la propiedad de Markov y $p(\tau,x,y)$ es su densidad de transición.

\begin{solucion}
 (i) \begin{align*}
     \mathbb{E}[f(X_{t})|\mathcal{F}_{t}]&= \mathbb{E}[f(B_{t}+\mu t)|\mathcal{F}_{s}]\\
     &=\mathbb{E}[f(B_{t}+(B_{s}-B_{s})+\mu t))|\mathcal{F}_{s}]\\
     &=\mathbb{E}[f(B_{t}-B_{s}+(B_{s}+\mu t))|\mathcal{F}_{s}]\\
     &=\mathbb{E}[f(B_{t-s}+(B_{s}+\mu t))]\\
     &=\int_{-\infty}^{\infty}f(x+B_{s}+\mu t)\frac{e^{-\frac{x^{2}}{2(t-s)}}}{\sqrt{2\pi (t-s)}}dx\\
     &= \int_{-\infty}^{\infty}f(y)\frac{e^{-\frac{(y-B_{s}-\mu s-\mu(t-s))^{2}}{2(t-s)}}}{\sqrt{2\pi(t-s)}}dy\\
     &=g(X_{s})
 \end{align*}
 \hspace{0.3cm} De esta manera
 \begin{equation*}
     E[f(X_{t})|\mathcal{F}_s]&=\int_{-\infty}^{\infty}f(y)p(t-s,X_{s},y)dy
 \end{equation*}
 \hspace{0.3cm} con:
 \begin{equation*}
     p(\tau,x,y)&=\frac{1}{\sqrt{2\pi\tau}}e^{-\frac{(y-x-\mu\tau)^2}{2\tau}}
 \end{equation*}
\end{solucion}
\begin{solucion}
 (ii) \begin{equation*}
    \mathbb{E}[f(S_{t})|\mathcal{F}_{s}]=\mathbb{E}[f(S_{0}e^{\sigma X_{t}})|\mathcal{F}_{s}]
\end{equation*}
\hspace{0.3cm} Por lo que se probó anteriormente en (i) y con $\mu=\frac{v}{\sigma}$, se tiene que
\begin{equation*}
     \mathbb{E}[f(S_{t})|\mathcal{F}_{s}]=\int_{-\infty}^{\infty}f(S_{0}e^{\sigma y})\frac{1}{\sqrt{2\pi(t-s)}}e^{-\frac{(y-X_{s}-\mu(t-s))^{2}}{2(t-s)}}dy
\end{equation*}
\hspace{0.3cm} Por practicidad, considere $S_{0}e^{\sigma y}=z$
\begin{align*}
    &= \int_{0}^{\infty}f(z)\frac{1}{\sqrt{2\pi(t-s)}}e^{-\frac{(\frac{1}{\sigma}ln\frac{z}{S_{0}}-\frac{1}{\sigma}ln\frac{S_s}{S_0}-\mu(t-s))^2}{2}}\frac{dz}{\sigma z}\\
    &= \int_{0}^{\infty}f(z)\frac{e^{-\frac{(ln\frac{z}{S_s}-v(t-s))^2}{2\sigma^2(t-s)}}}{\sigma z\sqrt{2\pi(t-s)}}dz\\
    &=\int_{0}^{\infty}f(z)p(t-s,S_{s},z)dz\\
    &=g(S_{s})
\end{align*}
\end{solucion}

\subsection{Convergencia de la distribución de los precios de las acciones en una secuencia de modelos binomiales} Este problema presenta la convergencia de la distribución de los precios de las acciones en una secuencia de modelos binomiales con la distribución del movimiento Browniano Geométrico. Sea $\sigma>0$ y $r\geq0$ dados. Para cada entero positivo $n$, se considera el modelo binomial tomando $n$ pasos por unidad de tiempo. En este modelo, la tasa de interes por periodo es $\frac{r}{n}$, el factor de subida es $u_{n}=e^{\sigma/\sqrt{n}}$, y el factor de baja es $d_{n}=e^{-\sigma/\sqrt{n}}$. Por otro lado, las probabilidades neutrales al riesgo están dadas por:
\begin{equation*}
    \Tilde{p}_{n}=\frac{\frac{r}{n}+1-e^{-\sigma/\sqrt{n}}}{e^{\sigma/\sqrt{n}}-e^{-\sigma/\sqrt{n}}}, \hspace{1cm} \Tilde{q}_{n}\frac{e^{\sigma/\sqrt{n}}-\frac{r}{n}-1}{e^{\sigma/\sqrt{n}}-e^{-\sigma/\sqrt{n}}}
\end{equation*}
\hspace{0.4cm} Sea $t$ un número racional positivo arbitrario, y para cada entero positivo $n$ para el cual $nt$ es un entero, se define
\begin{equation*}
    M_{nt,n}=\sum_{k=1}^{nt}X_{k,n},
\end{equation*}
\hspace{0.4cm} donde $X_{1,n},...,X_{n,n}$ son variables aleatorias independientes e idénticamente distribuidas con
\begin{equation*}
    \Tilde{\mathbb{P}}\{ X_{k,n}=1\}=\Tilde{p}_{n}, \hspace{1cm}\Tilde{\mathbb{P}}\{ X_{k,n}=-1\}=\Tilde{q}_{n}, \hspace{1cm} k=1,...,n
\end{equation*}
\hspace{0.4cm} El precio de la acción en el momento $t$ en este modelo binomial, que es el resultado de $nt$ pasos desde el momento inicial, está dado por
\begin{align*}
    S_{n}(t)&=S_0u_{n}^{\frac{1}{2}(nt+M_{nt,n})}d_{n}^{\frac{1}{2}(nt-M_{nt,n})}\\
    &=S_0exp\left\{ \frac{\sigma}{2\sqrt{n}}(nt+M_{nt,n})\right\}exp\left\{ -\frac{\sigma}{2\sqrt{n}}(nt-M_{nt,n})\right\}\\
    &=S_0exp\left\{ \frac{\sigma}{\sqrt{n}}M_{nt,n}\right\}
\end{align*}
\hspace{0.4cm} Este problema, muestra que así como $n\rightarrow\infty$, la distribución de la secuencia de variables aleatorias $\frac{\sigma}{\sqrt{n}}M_{nt,n}$ que aparece en el exponente anterior converge a la distribución normal con media $(r-\frac{1}{2}\sigma^2)t$ y varianza $\sigma^2t$- Por lo tanto, la distribución limitante de $S_{n}(t)$ es la misma distribución que del Movimiento Browniano Geométrico $S_0\cdot exp\{\sigma B_t+(r-\frac{1}{2}\sigma)t\}$ al tiempo t.$\par$

(i) Muestre que la función generadora de momentos $\varphi_{n}(u)$ de $\frac{1}{\sqrt{n}}M_{nt,n}$ está dada por

\begin{equation*}
    \varphi_{n}(u)=\left[ e^{\frac{u}{\sqrt{n}}}\left(\frac{\frac{r}{n}+1-e^{-\sigma/\sqrt{n}}}{e^{\sigma/\sqrt{n}-e^{\sigma/\sqrt{n}}}} \right) -e^{\frac{u}{\sqrt{n}}}\left( \frac{\frac{r}{n}+1-e^{-\sigma/\sqrt{n}}}{e^{\sigma/\sqrt{n}-e^{\sigma/\sqrt{n}}}} \right) \right]^{nt}
\end{equation*}
\hspace{0.4cm} (ii) Se quiere calcular
\begin{equation*}
    \lim_{n\rightarrow\infty}\varphi_{n}(u)=\lim_{n\downarrow0}\varphi_{\frac{1}{x^2}}(u),
\end{equation*}
\hspace{0.4cm} donde se ha cambiado la variable $x=\frac{1}{\sqrt{n}}.$ Para hacer esto, se
calcula $log_{\varphi_{\frac{1}{x^2}}}(u)$ y tomando el límite cuando $x\downarrow0$. Muestre que
\begin{equation*}
    log\varphi_{\frac{1}{x^2}}(u)=\frac{t}{x^2}log\left[ \frac{(rx^{2}+1)sinh \hspace{0.1cm}ux+sinh (\sigma-u)x}{sinh\hspace{0.1cm}\sigma x}\right]
\end{equation*}
\hspace{0.4cm} (la definiciones son $sinh \hspace{0.1cm}z=\frac{e^{z}-e^{-z}}{2}$, $cosh\hspace{0.1cm}z=\frac{e^{z}+e^{-z}}{2}$), y use la fórmula
\begin{equation*}
    sinh(A-B)=sinh A cosh B- cosh A sinh B
\end{equation*}
\hspace{0.4cm} para reescribir como
\begin{equation*}
    log\varphi_{\frac{1}{x^2}}(u)=\frac{t}{x^2}log\left[ cosh\hspace{0.1cm}ux+\frac{(rx^2+1-cosh\hspace{0.1cm}\sigma x)sinh\hspace{0.1cm}ux}{sinh \hspace{0.1cm}\sigma x} \right]
\end{equation*}
\hspace{0.4cm}(iii) Use expansiones de serie de Taylor,
\begin{equation*}
    cosh\hspace{0.1cm}z=1+\frac{1}{2}z^2+O(z^4), sinh \hspace{0.1cm}z=z+O(z^3)
\end{equation*}
\hspace{0.4cm} para mostrar que
\begin{equation*}
    cosh\hspace{0.1cm}ux+\frac{(rx^2+1-cosh\hspace{0.1cm}\sigma x)sinh\hspace{0.1cm}ux}{sinh \hspace{0.1cm}\sigma x}=1+\frac{1}{2}u^{2}x^{2}+\frac{rux^{2}}{\sigma}+\frac{1}{2}ux^{2}\sigma+O(x^{4})
\end{equation*}
\hspace{0.4cm} La notación $O(x^{j})$ es usada para representar los términos de orden $x^{j}$.$\par$

(iv) Use la expansión de la serie de Taylor $log(1+x)=x+O(x^2)$ para calcular $lim_{x\downarrow0}log\varphi_{\frac{1}{x^2}}(u)$. Además explique cómo sabe que la distribución limitante $\frac{\sigma}{\sqrt{n}}M_{nt,n}$ es normal con media $(r-\frac{1}{2}\sigma^2)t$ y varianza $\sigma^2t$.

\begin{solucion}
 (i)
 \begin{align*}
    \varphi_{n}(u)&=\Tilde{\mathbb{E}}\left[e^{u\frac{1}{\sqrt{n}}M_{nt,n}} \right]\\
    &=\left( \Tilde{\mathbb{E}}\left[e^{u\frac{1}{\sqrt{n}}X_{1,n}} \right]\right)^{nt}\\
    &=(e^{\frac{u}{\sqrt{n}}}\Tilde{p}_{n}+e^{-\frac{u}{\sqrt{n}}}\Tilde{q}_{n})^{nt}\\
    &=\left[ e^{\frac{u}{\sqrt{n}}}\left(\frac{\frac{r}{n}+1-e^{-\sigma/\sqrt{n}}}{e^{\sigma/\sqrt{n}-e^{\sigma/\sqrt{n}}}} \right) -e^{\frac{u}{\sqrt{n}}}\left( \frac{\frac{r}{n}+1-e^{-\sigma/\sqrt{n}}}{e^{\sigma/\sqrt{n}-e^{\sigma/\sqrt{n}}}} \right) \right]^{nt}
 \end{align*}
\end{solucion}
\begin{solucion}
 (ii)
 \begin{equation*}
     \varphi_{\frac{1}{x^2}}=\left[ e^{ux}\left(\frac{rx^2+1-e^{-\sigma x}}{e^{\sigma x}-e^{-\sigma x}} \right)-e^{-ux}\left(\frac{rx^2+1-e^{\sigma x}}{e^{\sigma x}-e^{-\sigma x}} \right)\right]^{\frac{t}{x^2}}
 \end{equation*}
 \hspace{0.4cm} Por lo tanto, se obtiene que
 \begin{align*}
     log\varphi_{\frac{1}{x^2}}&=\frac{t}{x^2}log\left[ \frac{(rx^2+1)(e^{ux}-e^{-ux})+e^{(\sigma-u)x}-e^{-(\sigma-u)x}}{e^{\sigma x}-e^{-\sigma x}}\right]\\
     &=\frac{t}{x^2}log\left[ \frac{(rx^2+1)sinh\hspace{0.1cm}ux+sinh(\sigma -u)x}{sinh\hspace{0.1cm}\sigma x}\right]\\
     &=\frac{t}{x^2}log\left[ \frac{(rx^2+1)sinh\hspace{0.1cm}ux+sinh\hspace{0.1cm}\sigma x\hspace{0.1cm}cosh\hspace{0.1cm}ux-cosh\hspace{0.1cm}\sigma x\hspace{0.1cm}sinh\hspace{0.1cm}ux}{sinh\hspace{0.1cm}\sigma x}\right]\\
     &=\frac{t}{x^2}log\left[ cosh\hspace{0.1cm}ux+\frac{(rx^2+1-cosh\hspace{0.1cm}\sigma x)sinh\hspace{0.1cm}ux}{sinh\hspace{0.1cm}\sigma x}\right]
 \end{align*}
\end{solucion}
\begin{solucion}
 (iii)
 \begin{align*}
     cosh\hspace{0.1cm}ux+\frac{(rx^2+1-cosh\hspace{0.1cm}\sigma x)sinh\hspace{0.1cm}ux}{sinh\hspace{0.1cm}\sigma x}&=1+\frac{u^2x^2}{2}+O(x^4)+\frac{(rx^2+1-1-\frac{\sigma^2x^2}{2}+O(x^4))(ux+O(x^3)}{\sigma x+O(x^3)}\\
     &=1+\frac{u^2x^2}{2}+\frac{(r-\frac{\sigma^2}{2})ux^3+O(x^5)}{\sigma x+O(x^3)}+O(x^4)\\
     &=1+\frac{u^2x^2}{2}+\frac{(r-\frac{\sigma^2}{2})ux^3(1+O(x^2))}{\sigma x(1+O(x^2))}+O(x^4)\\
     &=1+\frac{u^2x^2}{2}+\frac{rux^2}{\sigma}-\frac{1}{2}\sigma ux^2+O(x^4)
 \end{align*}
\end{solucion}
\begin{solucion}
 (iv)
 \begin{align*}
     log \varphi_{\frac{1}{x^2}}&=\frac{t}{x^2}log\left( 1+\frac{u^2x^2}{2}+\frac{rux^2}{\sigma}-\frac{1}{2}\sigma ux^2+O(x^4)\right)\\
     &=\frac{t}{x^2}\left( \frac{u^2x^2}{2}+\frac{ru}{\sigma}x^2-\frac{\sigma ux^2}{2}+O(x^4)\right)
 \end{align*}
 \hspace{0.4cm} Por lo tanto, obteniendo el límite cuando $\downarrow0$, se obtiene que
 \begin{equation*}
     \lim_{x\downarrow0}log\varphi_{\frac{1}{x^2}}(u)=t\left(\frac{u^2}{2}+\frac{ru}{\sigma}-\frac{\sigma u}{2}\right)
 \end{equation*}
\hspace{0.4cm} Y adicionalmente, se tiene que
\begin{equation*}
    \Tilde{\mathbb{E}}\left[ e^{u\frac{1}{\sqrt{n}}M_{nt,n}}\right]=\varphi_{n}(u)\rightarrow\frac{1}{2}tu^{2}+t(\frac{r}{\sigma}-\frac{\sigma}{2})u
\end{equation*}
\hspace{0.4cm} Por la correspondencia uno a uno entre la distribución y la función generadora de momentos, $(\frac{\sigma}{\sqrt{n}}M_{nt,n})$ converge a una variable aleatoria Gaussiana con media $t(\frac{r}{\sigma}-\frac{\sigma}{2})$ y varianza t. Por lo tanto, $(\frac{\sigma}{\sqrt{n}}M_{nt,n})_{n}$ converge a una variable aleatoria Gaussiana con media $t(r-\frac{\sigma^2}{2})$ y varianza $\sigma^2t.$

\end{solucion}
\subsection{Integral de Stratonovich} El objetivo de este ejercicio es ver cuál es el límite en \(\mathrm{L}^2\) de: 
\[\sum_{i} B_{\tau_{i}^{n}}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}} \right)\]
Donde \(\{0 = t_{0}^{n} < t_{1}^{n} < \hdots < t_{k_n}^{n} = t\}\) es una sucesión de particiones de \([0,t]\) cuya norma tiende a cero y 
\(\tau_{i}^{n}  = \frac{ t_{i}^{n} +  t_{i-1}^{n}}{2}.\) 
\begin{enumerate}
    \item Defina \(\mathcal{F}_{s,t} = \sigma\left(B_r : r\leq s \vee r \geq t\right)\). Si \(u \leq s \leq t \leq v\), pruebe que; 
    \[B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\]
    es independiente de \(\mathcal{F}_{u,v}\). Concluya que 
    \[\mathbb{E}\left[B_t - B_s \left|\right. \mathcal{F}_{u,v} \right] = \frac{t-s}{v-u}(B_v - B_u)\]
    \begin{solucion}
    Sea \(\mathhrm{H}\) el espacio generado por el movimiento browniano, i.e., \(\mathhrm{H}=\bar{span}\left\{B_r :r \geq 0\right\}\) tomemos los 
    subespacios lineales cerrados:
    \begin{align*}
        \mathhrm{H}_{1} &= \bar{span}\left\{B_r : r\leq u \vee r \geq v \right\}\\
        \mathhrm{H}_{2} &= \bar{span}\left\{B_t -B_s -(B_v -B_u)\frac{t-s}{v-u} \right\}
    \end{align*}
    Bastará probar que \(\mathhrm{H}_1 \perp \mathrm{H}_2\), ya que al ser subespacios gaussianos de un mismo espacio gaussiano, se tiene que \(\sigma(\mathhrm{H}_1) \perp \sigma(\mathrm{H}_2)\)
    Recuerde que: 
    \[\mathhrm{H}_{1} = \left\{X\in\mathhrm{L}^2 : \forall \varepsilon >0 \exists Z\in span\left\{B_r : r\leq u \vee r \geq v \right\} tq \left \|X - Z \right \|_{\mathhrm{L}^2} < \varepsilon\right\}\] 
    y \(span\left\{B_r : r\leq u \vee r \geq v \right\}\), es el conjunto de todas las combinaciones lineales finitas de los elementos del conjunto \(\left\{B_r : r\leq u \vee r \geq v \right\}\). Observe ahora lo siguiente, Si \(X\in \mathhrm{H}_{1} \) entonces para cada \(\varepsilon >0\) existe \( Z\in span\left\{B_r : r\leq u \vee r \geq v \right\}\) tal que:
    \[\left \|X - Z \right \|_{\mathhrm{L}^2} < \varepsilon\] 
    Así para cada \(n \in \mathbb{N}\) sí \(\varepsilon = \frac{1}{n}>0\) entonces, existirá \(Z_n\in span\left\{B_r : r\leq u \vee r \geq v \right\}\) tal que:
    \[\left \|X - Z_n \right \|_{\mathhrm{L}^2} < \frac{1}{n}\]
    tomando el límite cuando \(n \to \infty\) se tiene: 
    \[0 \leq \lim_{n \to \infty}\left \|X - Z_n \right \|_{\mathhrm{L}^2} \leq 0 \]
    Por lo cual, \(X\) puede ser visto como el límite en \(\mathhrm{L}^2\) de una sucición de variables aleatoria en \(span\left\{B_r : r\leq u \vee r \geq v \right\}\), 
    del mismo modo si \(Y \in \mathhrm{H}_2\) entonces, \(Y\) puede ser visto como el limite en \(\mathhrm{L}^2\) 
    de una sucesión de variables aleatorias en \(span\left\{B_t -B_s -(B_v -B_u)\frac{t-s}{v-u} \right\}\), denotemosla por \(\{Y_n\}_{n \in \mathbb{N}}\). Queriamos probar que dados \(X \in \mathhrm{H}_1,Y\in\mathhrm{H}_2\) se satisface que, \(\mathbb{E}(XY) = 0\), pero dadas las obesrcaciones anteriores esto sera equivalente a probar que: 
    \begin{align*}
        \mathbb{E}(XY) &= \left \langle X,Y \right \rangle\\
                       &= \lim_{n\to\infty}\left \langle Z_n,Y_n \right \rangle\\
                       &= \lim_{n\to\infty}\mathbb{E}(Z_nY_n)
    \end{align*}
    Por lo cual:
    \begin{equation}\label{St.0}
    \mathbb{E}(XY) &= \lim_{n\to\infty}\mathbb{E}(Z_nY_n)   
    \end{equation}
    Donde \(\{Z_n\}_{n\in\mathbb{N}}\subseteq span\left\{B_r : r\leq u \vee r \geq v \right\}\) e \(\{Y_n\}_{n\in\mathbb{N}}\subseteq span\left\{B_t -B_s -(B_v -B_u)\frac{t-s}{v-u} \right\}\).
    Observe que gracias a está ultima ifualdad bastara con probar que: 
    \[\mathbb{E}\left[B_r\left( B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\right)\right]=0 \ \ \forall r\geq 0  \ \ t.q. \ \ r\leq u \vee r\geq v\]
    Ya que esto implicara que el limete anterior es igual a cero. Se procedera por casos. Supongamos \(r \leq u\) entonces 
    \[B_r \perp B_t - B_s \ \ B_r \perp B_v - B_u\]
    Por la propiedad del movimiento browniano de los incrementos independientes, ya que \(B_r \myeq{} B_r - B_0\) y \(r \leq u \leq s \leq t \leq v\) y, por lo tanto: 
    \begin{align*}
       \mathbb{E}\left[B_r\left( B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\right)\right] &=  \mathbb{E}\left[B_r\left( B_t -B_s\right)\right] -  \frac{t-s}{v-u}\mathbb{E}\left[B_r\left(B_v -B_u\right)\right]\\
       &= 0 + \frac{t-s}{v-u}(0)\\
       &= 0
    \end{align*}
    Por otro lado sí \(r \geq v\), note que \(B_r\) puede ser escrito de la siguiente manera:
    \begin{equation}\label{St.1}
        B_r = (B_r - B_v) + (B_v - B_u) + B_u 
    \end{equation}
    Tomando: 
    \begin{align*}
        I_{r,v} &= B_r - B_v\\
        I_{v,u} &= B_v - B_u\\
        I_{u,0} &\myeq{} B_u - B_0\\
        I_{t,s} &= B_t - B_s
    \end{align*}
    Es posible expresar a la ecuación (\ref{St.1}) como: 
    \begin{equation}\label{St.2}
        B_r \myeq{}  I_{r,v} + I_{v,u} + I_{u,0}
    \end{equation}
    y tenemos además que: 
    \begin{equation}\label{St.3}
    I_{r,v} \perp I_{t,s} \ \ I_{u,0} \perp I_{t,s} \ \ I_{u,0} \perp I_{v,u} \ \ I_{r,v} \perp I_{v,u}
    \end{equation}
    \[\]
    ya que \(u \leq s \leq t \leq v \leq r\) por lo tanto: 
    \begin{align*}
        \mathbb{E}\left[B_r\left( B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\right)\right] &=  \mathbb{E}\left[\left(I_{r,v} + I_{v,u} + I_{u,0}\right)\left(  I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right]\\ 
        &= \mathbb{E}\left[\left(I_{r,v}\right)\left(I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right]\\
        &+ \mathbb{E}\left[\left( I_{v,u}\right)\left(I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right]\\
        &+ \mathbb{E}\left[\left( I_{u,0}\right)\left(I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right]\\
        &= 0 + \mathbb{E}\left[\left( I_{v,u}\right)\left(I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right] + 0\\
        &=\mathbb{E}\left[\left( I_{v,u}\right)\left(I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right]
    \end{align*}
    La igualdad en la primer linea es debido a (\ref{St.2}), y la igualdad en la tercera a (\ref{St.3}). Por lo cual: 
    \begin{align*}
        \mathbb{E}\left[B_r\left( B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\right)\right] &= \mathbb{E}\left[\left( I_{v,u}\right)\left(I_{t,s} - \frac{t-s}{v-u}I_{v,u}\right)\right]\\
        &= \mathbb{E}\left[I_{v,u}I_{t,s}\right]-\frac{t-s}{v-u}\mathbb{E}\left[I_{v,u}^2\right]\\
        &= t-s - \frac{t-s}{v-u}(v-u)\\
        &=0\\
    \end{align*}
    Así es posible afirmar que la igualdad (\ref{St.0}), es igual a 0, y por tanto que:
    \[\sigma(\mathhrm{H}_2)\perp\sigma(\mathhrm{H}_1) = \mathcal{F}_{u,v}\]
    Esto implica en particular que: 
    \[B_t -B_s -(B_v -B_u)\frac{t-s}{v-u} \perp \mathcal{F}_{u,v}\]
    El resultado deseado.
    Note ahora que: 
    \begin{align*}
        \mathbb{E}\left[B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\left. \right|\mathcal{F}_{u,v}\right]&=\mathbb{E}\left[B_t -B_s -(B_v -B_u)\frac{t-s}{v-u}\right]\\
        &=\mathbb{E}\left[B_t -B_s\right] - \frac{t-s}{v-u}\mathbb{E}\left[B_v -B_u\right]\\
        &=0
    \end{align*}
    La igualdad en la primera linea es debido a la independencia probada con anterioridad. Utilizando la igualdad anterior, la linealidad de la esperanza condicional y el hecho de que \(B_v - B_u\) es \(\mathcal{F}_{u,v}-medible\) se obtiene:
    \[\mathbb{E}\left[B_t - B_s \left|\right. \mathcal{F}_{u,v} \right] = \frac{t-s}{v-u}(B_v - B_u)\]
    
    \end{solucion}
    \item Muestre que: 
    \begin{align*}
        \mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right]
        &=\\ 
        \mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right]
    \end{align*}
    y que por lo tanto: 
    \[\mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right] \leq tmax(t_{i}^{n} -  t_{i-1}^{n} )\]
    \begin{solucion}
    Observe lo siguiente:
    \begin{align*}
        B_{\tau_{i}^n}-B_{t_{i-1}^n} - \left(B_{t_{i}^n}-B_{t_{i-1}^n}\right)\left(\frac{\tau_{i}^n - t_{i-1}^n}{t_{i}^n-t_{i-1}^n}\right)&= B_{\tau_{i}^n}-B_{t_{i-1}^n} - \left( B_{t_{i}^n}-B_{t_{i-1}^n}\right)\left(\frac{\frac{t_{i}^n+t_{i-1}^n}{2} - t_{i-1}^n}{t_{i}^n-t_{i-1}^n}\right)\\
        &= B_{\tau_{i}^n}-B_{t_{i-1}^n} - \frac{1}{2}( B_{t_{i}^n}-B_{t_{i-1}^n})\\
        &=  B_{\tau_{i}^n}-\frac{1}{2}( B_{t_{i}^n}+B_{t_{i-1}^n}) 
    \end{align*}
    Por lo cuál el término \( B_{\tau_{i}^n}-\frac{1}{2}( B_{t_{i}^n}+B_{t_{i-1}^n})\), es independiente de de la sigma álgebra \(\mathcal{F}_{t_{i-1}^n,t_{i}^n}\), y por otra parte el término \(B_{t_{i}^n}+B_{t_{i-1}^n}\), es medible respecto a dicha sigma álgebra. Por lo tanto: 
    \begin{equation}\label{St.4}
     B_{\tau_{i}^n}-\frac{1}{2}( B_{t_{i}^n}+B_{t_{i-1}^n}) \perp B_{t_{i}^n}-B_{t_{i-1}^n} \ \ \forall i   
    \end{equation}
    Ahora observemos que: 
    \begin{equation*}
        \mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right]= 
    \end{equation*}
    \[
        = \mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right.\\
    \]
    \[
        + \sum_{i\neq j}\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)
        \left.\left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right] = 
    \]
    \[
        = \mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right]\\
    \]
    \[
        + \sum_{i\neq j} \mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right.
        &\left.\left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right]
    \]
    Es decir:
    \[
       \mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right] =
    \]
    \[
    = \mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right]
    \]
        
    \begin{equation}\label{St.5}
          + \sum_{i\neq j} \mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right) \left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right]
    \end{equation}\\
    Observemos el termino: 
    \[ \mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right] \ \ \forall i\neq j\]
    
    Por (\ref{St.4}) se tiene que \(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \perp B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\) y que \(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \perp B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\), y por ser \(\{B_t\}_{t\geq0}\) un movimiento browniano, y sin perdida de generalidad supongamos\(i<j\), entonces \(t_{i-1} < \tau_i < t_{i} \leq t_{j-1}< \tau_j < t_j\), por lo cual:
    \[B_{t_{i}^{n}}- B_{t_{i-1}^{n}} \perp B_{t_{j}^{n}}- B_{t_{j-1}^{n}}, \ \ B_{t_{i}^{n}}- B_{t_{i-1}^{n}} \perp B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} = B_{\tau_{j}^{n}} - B_{t_{j-1}^{n}} - \frac{B_{t_{j}^{n}} - B_{t_{j-1}^{n}}}{2}\] 
    del mismo modo: 
     \[B_{t_{j}^{n}}- B_{t_{j-1}^{n}} \perp B_{t_{i}^{n}}- B_{t_{i-1}^{n}}, \ \ B_{t_{j}^{n}}- B_{t_{j-1}^{n}} \perp B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} = B_{\tau_{i}^{n}} - B_{t_{i-1}^{n}} - \frac{B_{t_{i}^{n}} - B_{t_{i-1}^{n}}}{2}\]
     Estas últimas igualdades por la propiedad de incrementos independientes del movimiento browniano, por lo tanto todas las variables que se multiplican en la esperanza anterior son independientes así:
     \[\mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right] = \]
     \[= \mathbb{E}\left[B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2}\right]\mathbb{E}\left[B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right]\mathbb{E}\left[B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2}\right]\mathbb{E}\left[B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right]= 0\]
     Por lo cual:
     \begin{equation}\label{St.6}
         \mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right] = 0
     \end{equation}
     Esto último para toda \(i \neq j\). Volviendo a (\ref{St.5}) se obtiene lo siguiente: 
     \[
       \mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right] =
    \]
    \[
        = \mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right] +
    \]
    \[
          \sum_{i\neq j} \mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\left(B_{\tau_{j}^{n}} - \frac{B_{t_{j}^{n}}+ B_{t_{j-1}^{n}}}{2} \right)\left(B_{t_{j}^{n}}- B_{t_{j-1}^{n}}\right)\right] = 
    \]
    \[
        \mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right] + 0
    \]
    La última igualdad debido a (\ref{St.6}). Por lo cual hemos obtenido el resultado deseado: 
    \[
    \mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right] =
    \]
    \[
      =\mathbb{E}\left[\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right] = 
    \]
    \[
      =\sum\mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right] = 
    \]
    Más aun por (\ref{St.4}) \( \left(B_{\tau_{i}^n}-\frac{1}{2}( B_{t_{i}^n}+B_{t_{i-1}^n})\right)^2 \perp \left(B_{t_{i}^n}-B_{t_{i-1}^n}\right)^2 \ \ \forall i\)
    \[
     \sum\mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\right]\mathbb{E}\left[\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)^2\right] = 
    \]
    \[
     = \sum\mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\right]\left(t_{i}^{n} - t_{i-1}^{n}\right) \leq   
    \]
    \[ 
    \leq \sum\mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\right]\max\left(t_{k}^{n} - t_{k-1}^{n}\right) =
    \]
    Realizando algunos cálculos con las covarianzas del browniano es fácil probar que \(\mathbb{E}\left[\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)^{2}\right] = \frac{1}{4}(t_{i}^n - t_{i-1}^n)\), por tanto: 
    \[
      = \sum\frac{t_{i}^n - t_{i-1}^n}{4}\max\left(t_{k}^{n} - t_{k-1}^{n}\right) = \frac{t}{4}\max\left(t_{k}^{n} - t_{k-1}^{n}\right) \leq t\max\left(t_{k}^{n} - t_{k-1}^{n}\right)
    \]
    \end{solucion}
    \item Muestre que 
    \[\sum\left(\frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2}\right)(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}) \rightarrow \frac{1}{2}B_{t}^2\]
    en \(\mathrm{L}^2\) y concluya.
    \[
     \mathbb{E}\left[\left(\sum\left(\frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2}\right)(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}) - \frac{B_{t}^2}{2}\right)\right] = 
    \]
    \[
    = \mathbb{E}\left[\left(\sum\frac{B_{t_{i}^{n}}^2 - B_{t_{i-1}^{n}}^2}{2} - \frac{B_{t}^2}{2}\right)\right] = \mathbb{E}\left[\left(\frac{B_{t}^2-B_{0}^2}{2} - \frac{B_{t}^2}{2}\right)\right] = 0
    \]
    Por lo cual: 
    \[
     \mathbb{E}\left[\left(\sum\left(\frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2}\right)(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}) - \frac{B_{t}^2}{2}\right)\right] = 0
    \]
    Y tomando el límite cuando \(n \to \infty\) obtenemos que:
    \begin{equation}\label{St.7}
       \sum\left(\frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2}\right)(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}) \to \frac{1}{2}B_{t}^2  \ en \ \mathhrm{L}^2 
    \end{equation}
    Para concluir, sean: 
    \begin{align*}
        S_n &= \sum B_{\tau_{i}^{n}}\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}} \right)\\
        Z_n &= \sum\left(\frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2}\right)(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}) 
    \end{align*}
    Note que:
    \begin{equation}\label{St.8}
    \mathbb{E}\left[(S_n - Z_n)^2\right] = \mathbb{E}\left[\left(\sum\left(B_{\tau_{i}^{n}} - \frac{B_{t_{i}^{n}}+ B_{t_{i-1}^{n}}}{2} \right)\left(B_{t_{i}^{n}}- B_{t_{i-1}^{n}}\right)\right)^2\right] \leq máx(t_{k}^{n}-t_{k-1}^{n})t
    \end{equation}
    Por último note que: 
    \begin{align*}
     \left \|S_n - \frac{B_t}{2}\right \|_{\mathhrm{L}^2} &\leq \left \|S_n - Z_n\right \|_{\mathhrm{L}^2} + \left \|Z_n - \frac{B_t}{2}\right \|_{\mathhrm{L}^2}\\
     &\leq \sqrt{máx(t_{k}^{n}-t_{k-1}^{n})t} + \left \|Z_n - \frac{B_t}{2}\right \|_{\mathhrm{L}^2}
    \end{align*}
    Y tomando el límite cuando \(n \to \infty\) de ambos lados de la desiguealdad anterior obtenemos que: 
    \[
     0 \leq \left \|S_n - \frac{B_t}{2}\right \|_{\mathhrm{L}^2} \leq 0
    \]
    Estó último por (\ref{St.7}) y por que la norma de la sucesión de particiones que fue tomada tiende a cero,
    Por lo cual: 
    \[
     \left \|S_n - \frac{B_t}{2}\right \|_{\mathhrm{L}^2} = 0
    \]
    Y esto prueba que 
    \[
    S_n \to \frac{B_t}{2} \ en \ \mathhrm{L}^2
    \]
\end{enumerate}

%%DANIELA
\subsection{Ecuación generalizada del Movimimiento Browniano Geométrico}
Sea \(S_t\) un proceso estocástivo no negativo que satisface la ecuación generalizada del Movimiento Browniano geométrico 
\begin{equation} \label{Daniela1}
    dS_t= \alpha_{t}S_t~dt+\sigma_{t}S_t~
    dB_t
\end{equation}
donde \(\alpha_{t}\) y \(\sigma_{t}\) son procesos adaptados a la filtración \(\mathcal{F}_t\), con \(t \geq 0\), asociada al Movimiento Browniano \(B_t\), con \(t \geq 0\). Mostraremos que \(S_t\) dado por
\begin{equation} \label{Daniela2}
   S_t=S_0e^{X_t} =S_0 \exp\left\{\int_0^t \sigma_{s}~dB_s+\int_0^t\left(\alpha_{s}-\frac{1}{2}\sigma^{2}_{s}\right)~ds\right\}
\end{equation}
Esta fórmula proporciona la única solución a la ecuación diferencial estocástica (\ref{Daniela1})

\begin{enumerate}[i.]
    \item Usando la ecuación (\ref{Daniela1}) y la fórmula de Itô-Doeblin, calcule \(d\log S_t\). Simplifique de tal forma que obtenga una fórmula para \(d\log S_t\), que no incluya a \(S_t\).
    \begin{proof}[Solución]
    \begin{align*}
        d\log S_t =& \frac{dS_t}{S_t}-\frac{1}{2}\frac{d\langle S\rangle_t}{S^2_t}\\
        =& \frac{2S_td~S_t-d\langle S\rangle_t}{2S^2_t}\\
        =&\frac{2S_t\left(\alpha_{t}S_tdt+\sigma_{t}S_tdB_t\right)-\sigma^{2}_{t}S^2_tdt}{2S_t^2}\\
        =& \sigma_{t}dB_t+\left(\alpha_{t}-\frac{1}{2}\sigma^{2}_{t}\right)dt.
    \end{align*}
    \end{proof}
    \item Integre la fórmula que obtuvo en el inciso anterior, y aplique la función exponencial para obtener la ecuación (\ref{Daniela2}).
    \begin{proof}[Solución]
   \begin{align*}
        \log S_t= \log S_0+\int_0^t \sigma_{s} ~dB_t+\int_0^t\left(\alpha_{s}-\frac{1}{2}\sigma^{2}_{s}\right)ds.
   \end{align*}
   Aplicando la función exponencial, tenemos que 
   \begin{align*}
      S_t= S_0 \exp\left\{\int_0^t\sigma_{s}~dB_t+\int_0^t\left(\alpha_{s}-\frac{1}{2}\sigma^{2}_{s}\right)ds\right\}.
   \end{align*}
    \end{proof}
\end{enumerate}

\subsection{Diferencial de \(S^p\)}
Sea \(S_t=S_0 \exp\left\{\sigma B_t+\left(\alpha-\frac{1}{2}\sigma^{2}\right)t\right\}\) un Movimiento Browniano geométrico, y sea \(p\) una constante positiva. Calcule \(dS^p_t\), la diferencial de \(S_t\) elevada a la potencia \(p\).
\begin{proof}[Solución]
Sin perdida de generalidad, podemos asumir que \(p \neq 1\). Además, como  \((x^p)'=px^{p-1}\), y \((x^p)^n=p(p-1)x^{p-2}\), tenemos que
\begin{align*}
    dS^p_t=& pS^{p-1}_tdS_t+\frac{1}{2}p(p-1)S_t^{p-2}d\langle S\rangle_t\\
    =& pS^{p-1}_t\left(\alpha S_tdt+\sigma S_tdB_t\right)+\frac{1}{2}p(p-1)S_t^{p-2}\sigma^2S_t^2dt\\
    =& S_t^p\left[p\alpha dt+p\sigma dB_t+\frac{1}{2}p(p-1)\sigma^2dt\right]\\
    =&pS_t^p\left[\sigma dB_t+\left(\alpha+\frac{p-1}{2}\sigma^2\right)dt\right]
\end{align*}
\end{proof}
\subsection{Algunas ecuaciones del Movimiento Browniano}
\begin{enumerate}[i.]
    \item Calcule \(dB_t^4\), y después escriba \(B_T^4\) como la suma de una integral ordinaria respecto al tiempo, y como una integral del Itô
    \begin{proof}[Solución]
    \begin{align*}
        dB_t^4=& 4B_t^3dB_t+\frac{1}{2}(4)(3)B_t^2d\langle B\rangle_t\\
        =& 4B_t^3dB_t+6B_t^2dt
    \end{align*}
    entonces,
    \begin{align*}
        B_T^4=4\int_0^T B_t^3dB_t+6\int_0^TB_t^2dt.
    \end{align*}
    \end{proof}
    \item Obtenga las esperanzas de ambos lados de la fórmula que obtuvo en el inciso anterior, use el hecho de que \(\mathbb{E}\left[B_t^2\right]=t\), y obtenga la fórmula \(\mathbb{E}\left[B_T^4\right]=3T^2\)
     \begin{proof}[Solución]
    \begin{align*}
        \mathbb{E}\left[B_T^4\right]=& 4\int_0^T\mathbb{E}\left[B_t^3dB_t\right]dt+6\int_0^T\mathbb{E}\left[B_t^2\right]dt\\
        =& 6\int_0^Tt~dt
        =3T^2
    \end{align*}
    \end{proof}
    \item Use los métodos mostrados en (I) y (II) para hallar una fórmula para \(\mathbb{E}\left[B_T^6\right]\)
     \begin{proof}[Solución]
    \begin{align*}
        dB_t^6=& 6B_t^5dB_t+\frac{1}{2}(6)(5)B_t^4dt
    \end{align*}
    entonces,
    \begin{align*}
        B_T^6=6\int_0^TB_t^5dB_t+15\int_0^TB_t^4dt.
    \end{align*}
    Por lo tanto,
    \begin{align*}
        \mathbb{E}\left[B_T^6\right]=15\int_0^T3t^2dt=15T^3
    \end{align*}
    \end{proof}
\end{enumerate}
%%Eres July? #MAYBE# #¿ERES DANI?# Quizás  #Sí eres, sólo tú responderias eso# 

%%%%%JULY%%%%%%%%
\subsection{Aplicaciones formulas de Itô - Doeblin}
Para una función \(f:\mathbb{R}\rightarrow \mathbb{R}\) de clase \(C^{2}\) y \(\left \{ B_{t} \right \}_{t\geq 0}\) un MV se cumple:
\begin{equation}\label{Rodolfonotevayas}
    f(B_{t})=f(B_{0})+\int_{0}^{t}f'(B_{t})dB_{s}+\frac{1}{2}\int_{0}^{t}f''(B_{s})ds
\end{equation}
\begin{itemize}
    \item Calcular \(\int_{0}^{t}B_{s}^{2}dB_{s}\)
    Usando la formula (\ref{Rodolfonotevayas}) nos basta con identificar la función \(f'\) a la cual está ligada nuestra integral , dicha función es \(f'(x)=x^{2}\) teniendo por función original \(f(x)=\frac{x^{3}}{3}\)  y \(f''(x)=2x\)  así sustituyendo en la formula antes mencionada tenemos 
    \begin{equation*}
         \int_{0}^{t}f'(B_{t})dB_{s} =f(B_{t})-f(B_{0})-\frac{1}{2}\int_{0}^{t}f''(B_{s})ds  
    \end{equation*}
    Donde \(f(B_{0})=0\) obteniendo
    \begin{equation*}
        \int_{0}^{t}B_{s}^{2}dB_{s}=\frac{B_{t}^{3}}{3}- \int_{0}^{t}B_{s}dB_{s}
    \end{equation*}
    \item Calcular\(\int_{0}^{t}B_{s}dB_{s}\)  donde \(f'(x)=x\) lo que implica que \(f''(x)=1\) y \(f(x)=\frac{x^{2}}{2}\) sustituyendo en la formula de \(Ito\) - Doeblin
    \begin{equation*}
        \int_{0}^{t}B_{s}dB_{s}=\frac{B_{t}^{2}}{2}- \frac{1}{2}\int_{0}^{t}ds=\frac{B_{t}^{2}}{2}- \frac{1}{2}t
    \end{equation*}
    \item Calcular \( \int_{0}^{t}e^{B_{s}}dB_{s}\) como la función exponencial tiene la propiedad particular de \(\frac{\partial e^{x}}{\partial x}=e^{x}\) obtenemos 
 '\begin{equation*}
     \int_{0}^{t}e^{B_{s}}dB_{s}=e^{B_{t}}-1-\frac{1}{2}\int_{0}^{t}e^{B_{s}}ds
 \end{equation*}
 
 \end{itemize}


\end{document}
